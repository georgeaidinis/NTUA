{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 1 Small - NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeaidinis/NTUA/blob/master/NN/Lab%201/Lab_1_Small_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cBcurO-uUmo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "#Lab 1: Επιβλεπόμενη Μάθηση - Ταξινόμηση - Μικρό Dataset (S11 - Quality Assessment of Digital Colposcopies)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKqs0XolswuQ",
        "colab_type": "text"
      },
      "source": [
        "#Section A\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Αϊδίνης Γιώργος 03116031\n",
        "\n",
        "Κολιός Παναγιώτης 03116100\n",
        "\n",
        "---\n",
        "\n",
        "Ομάδα M.B.8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-LPsGgQufbH",
        "colab_type": "text"
      },
      "source": [
        "# Section B\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypUIaKPuxyCI",
        "colab_type": "text"
      },
      "source": [
        "Αρχικά ενημερώνουμε τις βιβλιοθήκες που θα χρησιμοποιήσουμε.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZTpGgaqwT3G",
        "colab_type": "code",
        "outputId": "65525d21-b055-43a9-cb8f-eb37bf40bdd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "!pip install --upgrade pip #upgrade pip package installer\n",
        "!pip install scikit-learn --upgrade #upgrade scikit-learn package\n",
        "!pip install numpy --upgrade #upgrade numpy package\n",
        "!pip install pandas --upgrade #--upgrade #upgrade pandas package\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/d0/860c4f6a7027e00acff373d9f5327f4ae3ed5872234b3cbdd7bcb52e5eff/scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.3.3)\n",
            "Installing collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.21.3\n",
            "    Uninstalling scikit-learn-0.21.3:\n",
            "      Successfully uninstalled scikit-learn-0.21.3\n",
            "Successfully installed scikit-learn-0.22\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsvljyaGuws2",
        "colab_type": "text"
      },
      "source": [
        "Κατεβάζουμε το αρχείο που περιέχει το dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPGXEYNcqtmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL_pZToQuTs4",
        "colab_type": "code",
        "outputId": "3ef6e403-d21e-43b0-afa9-fdc644edf40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "download(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00384/Quality%20Assessment%20-%20Digital%20Colposcopy.zip\",\"QADC.zip\")\n",
        "print(\"All the files are downloaded\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download file... QADC.zip ...\n",
            "File downloaded\n",
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrdHuDyTvAD7",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι το αρχείο όντως κατέβηκε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1fY1PPQ7L9d",
        "colab_type": "code",
        "outputId": "5d54bb5d-51c8-4619-e40b-bdecd2913739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QADC.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egTI09KgvG47",
        "colab_type": "text"
      },
      "source": [
        "Επειδή είναι compressed, πρεπει να το κάνουμε decompress και να δουμε ότι όντως έγινε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFe5qqIq6jSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"QADC.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AZDcWwK7IKb",
        "colab_type": "code",
        "outputId": "57e0ed00-c472-4785-f147-48ab70ca92bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " QADC.zip  'Quality Assessment - Digital Colposcopy'   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDKrxf0wzLvH",
        "colab_type": "text"
      },
      "source": [
        "Μετονομάζουμε τον φάκελο για να έχει μικρότερο όνομα:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxlh8yEDy47d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv Quality\\ Assessment\\ -\\ Digital\\ Colposcopy QADC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PDIZ5HBzx9i",
        "colab_type": "text"
      },
      "source": [
        "Ανοίγοντας τον φάκελο, είδαμε ότι έχουμε τρια αρχεία, τα οποία έχουν headers, και πρέπει 1) να ενοποιηθούν σε ένα αρχείο και 2) να αφαιρεθούν οι επιπρόσθετοι headers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhtKAnFCyl0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"QADC/hinselmann.csv\",'r') as f:\n",
        "    with open(\"QADC/hinselmann1.csv\",'w') as f1:\n",
        "        next(f) # skip header line\n",
        "        for line in f:\n",
        "            f1.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcY__jBiymmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"QADC/schiller.csv\",'r') as f:\n",
        "    with open(\"QADC/schiller1.csv\",'w') as f1:\n",
        "        next(f) # skip header line\n",
        "        for line in f:\n",
        "            f1.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hTiHGpp0JR2",
        "colab_type": "text"
      },
      "source": [
        "Φτιάχνουμε ένα μεγάλο αρχείο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VimXlyS7fGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat QADC/green.csv QADC/hinselmann1.csv QADC/schiller1.csv > all.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPLZO8N605LP",
        "colab_type": "text"
      },
      "source": [
        "Για να δούμε λοιπόν το dataset μας ολόκληρο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVGBnlk67zwC",
        "colab_type": "code",
        "outputId": "6799354c-0f56-4c49-e992-54bde304f7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"all.csv\")\n",
        "df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cervix_area</th>\n",
              "      <th>os_area</th>\n",
              "      <th>walls_area</th>\n",
              "      <th>speculum_area</th>\n",
              "      <th>artifacts_area</th>\n",
              "      <th>cervix_artifacts_area</th>\n",
              "      <th>os_artifacts_area</th>\n",
              "      <th>walls_artifacts_area</th>\n",
              "      <th>speculum_artifacts_area</th>\n",
              "      <th>cervix_specularities_area</th>\n",
              "      <th>os_specularities_area</th>\n",
              "      <th>walls_specularities_area</th>\n",
              "      <th>speculum_specularities_area</th>\n",
              "      <th>specularities_area</th>\n",
              "      <th>area_h_max_diff</th>\n",
              "      <th>rgb_cervix_r_mean</th>\n",
              "      <th>rgb_cervix_r_std</th>\n",
              "      <th>rgb_cervix_r_mean_minus_std</th>\n",
              "      <th>rgb_cervix_r_mean_plus_std</th>\n",
              "      <th>rgb_cervix_g_mean</th>\n",
              "      <th>rgb_cervix_g_std</th>\n",
              "      <th>rgb_cervix_g_mean_minus_std</th>\n",
              "      <th>rgb_cervix_g_mean_plus_std</th>\n",
              "      <th>rgb_cervix_b_mean</th>\n",
              "      <th>rgb_cervix_b_std</th>\n",
              "      <th>rgb_cervix_b_mean_minus_std</th>\n",
              "      <th>rgb_cervix_b_mean_plus_std</th>\n",
              "      <th>rgb_total_r_mean</th>\n",
              "      <th>rgb_total_r_std</th>\n",
              "      <th>rgb_total_r_mean_minus_std</th>\n",
              "      <th>rgb_total_r_mean_plus_std</th>\n",
              "      <th>rgb_total_g_mean</th>\n",
              "      <th>rgb_total_g_std</th>\n",
              "      <th>rgb_total_g_mean_minus_std</th>\n",
              "      <th>rgb_total_g_mean_plus_std</th>\n",
              "      <th>rgb_total_b_mean</th>\n",
              "      <th>rgb_total_b_std</th>\n",
              "      <th>rgb_total_b_mean_minus_std</th>\n",
              "      <th>rgb_total_b_mean_plus_std</th>\n",
              "      <th>hsv_cervix_h_mean</th>\n",
              "      <th>hsv_cervix_h_std</th>\n",
              "      <th>hsv_cervix_s_mean</th>\n",
              "      <th>hsv_cervix_s_std</th>\n",
              "      <th>hsv_cervix_v_mean</th>\n",
              "      <th>hsv_cervix_v_std</th>\n",
              "      <th>hsv_total_h_mean</th>\n",
              "      <th>hsv_total_h_std</th>\n",
              "      <th>hsv_total_s_mean</th>\n",
              "      <th>hsv_total_s_std</th>\n",
              "      <th>hsv_total_v_mean</th>\n",
              "      <th>hsv_total_v_std</th>\n",
              "      <th>fit_cervix_hull_rate</th>\n",
              "      <th>fit_cervix_hull_total</th>\n",
              "      <th>fit_cervix_bbox_rate</th>\n",
              "      <th>fit_cervix_bbox_total</th>\n",
              "      <th>fit_circle_rate</th>\n",
              "      <th>fit_circle_total</th>\n",
              "      <th>fit_ellipse_rate</th>\n",
              "      <th>fit_ellipse_total</th>\n",
              "      <th>fit_ellipse_goodness</th>\n",
              "      <th>dist_to_center_cervix</th>\n",
              "      <th>dist_to_center_os</th>\n",
              "      <th>experts::0</th>\n",
              "      <th>experts::1</th>\n",
              "      <th>experts::2</th>\n",
              "      <th>experts::3</th>\n",
              "      <th>experts::4</th>\n",
              "      <th>experts::5</th>\n",
              "      <th>consensus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.344647</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>0.047522</td>\n",
              "      <td>0.288216</td>\n",
              "      <td>0.178585</td>\n",
              "      <td>0.016564</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.263560</td>\n",
              "      <td>37.594458</td>\n",
              "      <td>15.785021</td>\n",
              "      <td>21.809437</td>\n",
              "      <td>53.379479</td>\n",
              "      <td>109.918445</td>\n",
              "      <td>38.735421</td>\n",
              "      <td>71.183024</td>\n",
              "      <td>148.653865</td>\n",
              "      <td>55.029618</td>\n",
              "      <td>22.160330</td>\n",
              "      <td>32.869287</td>\n",
              "      <td>77.189948</td>\n",
              "      <td>38.561367</td>\n",
              "      <td>38.119059</td>\n",
              "      <td>0.442308</td>\n",
              "      <td>76.680426</td>\n",
              "      <td>95.109755</td>\n",
              "      <td>51.565052</td>\n",
              "      <td>43.544702</td>\n",
              "      <td>146.674807</td>\n",
              "      <td>48.808474</td>\n",
              "      <td>40.765228</td>\n",
              "      <td>8.043247</td>\n",
              "      <td>89.573702</td>\n",
              "      <td>5.014628</td>\n",
              "      <td>2.991944</td>\n",
              "      <td>167.952780</td>\n",
              "      <td>25.813163</td>\n",
              "      <td>109.919447</td>\n",
              "      <td>38.733741</td>\n",
              "      <td>5.090801</td>\n",
              "      <td>2.936650</td>\n",
              "      <td>159.486916</td>\n",
              "      <td>38.437294</td>\n",
              "      <td>95.123889</td>\n",
              "      <td>51.583029</td>\n",
              "      <td>0.923067</td>\n",
              "      <td>0.373371</td>\n",
              "      <td>0.844454</td>\n",
              "      <td>0.408130</td>\n",
              "      <td>0.603399</td>\n",
              "      <td>0.571175</td>\n",
              "      <td>0.962995</td>\n",
              "      <td>0.357890</td>\n",
              "      <td>85.474311</td>\n",
              "      <td>0.265933</td>\n",
              "      <td>0.346294</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.165329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048236</td>\n",
              "      <td>0.504736</td>\n",
              "      <td>0.502783</td>\n",
              "      <td>0.007012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097405</td>\n",
              "      <td>0.973837</td>\n",
              "      <td>0.004055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054999</td>\n",
              "      <td>0.028431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.505882</td>\n",
              "      <td>24.361877</td>\n",
              "      <td>35.144005</td>\n",
              "      <td>83.867760</td>\n",
              "      <td>122.366075</td>\n",
              "      <td>44.742407</td>\n",
              "      <td>77.623669</td>\n",
              "      <td>167.108482</td>\n",
              "      <td>78.058434</td>\n",
              "      <td>30.818729</td>\n",
              "      <td>47.239706</td>\n",
              "      <td>108.877163</td>\n",
              "      <td>54.932467</td>\n",
              "      <td>39.447415</td>\n",
              "      <td>15.485052</td>\n",
              "      <td>94.379883</td>\n",
              "      <td>101.680459</td>\n",
              "      <td>46.028852</td>\n",
              "      <td>55.651607</td>\n",
              "      <td>147.709311</td>\n",
              "      <td>63.218931</td>\n",
              "      <td>43.925912</td>\n",
              "      <td>19.293019</td>\n",
              "      <td>107.144843</td>\n",
              "      <td>4.944382</td>\n",
              "      <td>2.965108</td>\n",
              "      <td>130.260492</td>\n",
              "      <td>24.143867</td>\n",
              "      <td>122.366647</td>\n",
              "      <td>44.743932</td>\n",
              "      <td>5.080063</td>\n",
              "      <td>2.894163</td>\n",
              "      <td>128.251978</td>\n",
              "      <td>33.000693</td>\n",
              "      <td>101.725519</td>\n",
              "      <td>46.093510</td>\n",
              "      <td>0.850861</td>\n",
              "      <td>0.194308</td>\n",
              "      <td>0.646645</td>\n",
              "      <td>0.255673</td>\n",
              "      <td>0.497315</td>\n",
              "      <td>0.332444</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.184803</td>\n",
              "      <td>124.794129</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.283059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.457010</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.242888</td>\n",
              "      <td>0.212859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083055</td>\n",
              "      <td>0.018591</td>\n",
              "      <td>0.269798</td>\n",
              "      <td>39.353851</td>\n",
              "      <td>19.417332</td>\n",
              "      <td>19.936519</td>\n",
              "      <td>58.771183</td>\n",
              "      <td>109.543386</td>\n",
              "      <td>49.753490</td>\n",
              "      <td>59.789896</td>\n",
              "      <td>159.296876</td>\n",
              "      <td>54.642888</td>\n",
              "      <td>27.781965</td>\n",
              "      <td>26.860923</td>\n",
              "      <td>82.424853</td>\n",
              "      <td>41.242230</td>\n",
              "      <td>34.196356</td>\n",
              "      <td>7.045875</td>\n",
              "      <td>75.438586</td>\n",
              "      <td>109.592342</td>\n",
              "      <td>57.576640</td>\n",
              "      <td>52.015702</td>\n",
              "      <td>167.168982</td>\n",
              "      <td>53.470241</td>\n",
              "      <td>38.344391</td>\n",
              "      <td>15.125850</td>\n",
              "      <td>91.814632</td>\n",
              "      <td>5.049946</td>\n",
              "      <td>2.983966</td>\n",
              "      <td>163.576979</td>\n",
              "      <td>24.973042</td>\n",
              "      <td>109.544201</td>\n",
              "      <td>49.753659</td>\n",
              "      <td>5.078936</td>\n",
              "      <td>2.968023</td>\n",
              "      <td>162.268659</td>\n",
              "      <td>33.590792</td>\n",
              "      <td>109.597127</td>\n",
              "      <td>57.584515</td>\n",
              "      <td>0.918514</td>\n",
              "      <td>0.497554</td>\n",
              "      <td>0.747443</td>\n",
              "      <td>0.611432</td>\n",
              "      <td>0.633925</td>\n",
              "      <td>0.720923</td>\n",
              "      <td>0.920287</td>\n",
              "      <td>0.496596</td>\n",
              "      <td>94.948697</td>\n",
              "      <td>0.518740</td>\n",
              "      <td>0.419375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.513244</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.213781</td>\n",
              "      <td>0.251819</td>\n",
              "      <td>0.079795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.107022</td>\n",
              "      <td>46.322391</td>\n",
              "      <td>17.711957</td>\n",
              "      <td>28.610434</td>\n",
              "      <td>64.034349</td>\n",
              "      <td>116.075087</td>\n",
              "      <td>43.593124</td>\n",
              "      <td>72.481962</td>\n",
              "      <td>159.668211</td>\n",
              "      <td>51.430923</td>\n",
              "      <td>18.573016</td>\n",
              "      <td>32.857907</td>\n",
              "      <td>70.003940</td>\n",
              "      <td>40.365565</td>\n",
              "      <td>17.259087</td>\n",
              "      <td>23.106478</td>\n",
              "      <td>57.624652</td>\n",
              "      <td>102.641859</td>\n",
              "      <td>38.606995</td>\n",
              "      <td>64.034863</td>\n",
              "      <td>141.248854</td>\n",
              "      <td>50.805205</td>\n",
              "      <td>18.072101</td>\n",
              "      <td>32.733104</td>\n",
              "      <td>68.877306</td>\n",
              "      <td>5.177654</td>\n",
              "      <td>2.969214</td>\n",
              "      <td>156.242754</td>\n",
              "      <td>25.499379</td>\n",
              "      <td>116.081770</td>\n",
              "      <td>43.582671</td>\n",
              "      <td>5.071879</td>\n",
              "      <td>2.909002</td>\n",
              "      <td>158.343946</td>\n",
              "      <td>28.273928</td>\n",
              "      <td>102.648278</td>\n",
              "      <td>38.598300</td>\n",
              "      <td>0.951710</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.855409</td>\n",
              "      <td>0.599998</td>\n",
              "      <td>0.618140</td>\n",
              "      <td>0.830304</td>\n",
              "      <td>0.964611</td>\n",
              "      <td>0.532073</td>\n",
              "      <td>74.221670</td>\n",
              "      <td>0.347202</td>\n",
              "      <td>0.361672</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.390319</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.272884</td>\n",
              "      <td>0.373487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.071442</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>0.442831</td>\n",
              "      <td>37.552979</td>\n",
              "      <td>14.454975</td>\n",
              "      <td>23.098004</td>\n",
              "      <td>52.007953</td>\n",
              "      <td>101.044906</td>\n",
              "      <td>37.171973</td>\n",
              "      <td>63.872932</td>\n",
              "      <td>138.216879</td>\n",
              "      <td>53.971671</td>\n",
              "      <td>17.591669</td>\n",
              "      <td>36.380003</td>\n",
              "      <td>71.563340</td>\n",
              "      <td>40.717229</td>\n",
              "      <td>38.810699</td>\n",
              "      <td>1.906530</td>\n",
              "      <td>79.527927</td>\n",
              "      <td>97.446185</td>\n",
              "      <td>48.106253</td>\n",
              "      <td>49.339932</td>\n",
              "      <td>145.552438</td>\n",
              "      <td>51.733004</td>\n",
              "      <td>39.866956</td>\n",
              "      <td>11.866047</td>\n",
              "      <td>91.599960</td>\n",
              "      <td>4.978534</td>\n",
              "      <td>2.964685</td>\n",
              "      <td>158.160578</td>\n",
              "      <td>30.308891</td>\n",
              "      <td>101.050711</td>\n",
              "      <td>37.163383</td>\n",
              "      <td>5.051587</td>\n",
              "      <td>2.923540</td>\n",
              "      <td>157.131407</td>\n",
              "      <td>39.439642</td>\n",
              "      <td>97.457304</td>\n",
              "      <td>48.125747</td>\n",
              "      <td>0.955996</td>\n",
              "      <td>0.408286</td>\n",
              "      <td>0.882990</td>\n",
              "      <td>0.442043</td>\n",
              "      <td>0.623938</td>\n",
              "      <td>0.625574</td>\n",
              "      <td>0.957604</td>\n",
              "      <td>0.407600</td>\n",
              "      <td>61.546536</td>\n",
              "      <td>0.437852</td>\n",
              "      <td>0.673196</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.610160</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.298345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028185</td>\n",
              "      <td>0.046193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015983</td>\n",
              "      <td>0.104929</td>\n",
              "      <td>73.465862</td>\n",
              "      <td>33.856824</td>\n",
              "      <td>39.609038</td>\n",
              "      <td>107.322686</td>\n",
              "      <td>44.657984</td>\n",
              "      <td>35.514453</td>\n",
              "      <td>9.143531</td>\n",
              "      <td>80.172437</td>\n",
              "      <td>97.007389</td>\n",
              "      <td>67.135578</td>\n",
              "      <td>29.871812</td>\n",
              "      <td>164.142967</td>\n",
              "      <td>80.241955</td>\n",
              "      <td>32.698142</td>\n",
              "      <td>47.543813</td>\n",
              "      <td>112.940096</td>\n",
              "      <td>49.969255</td>\n",
              "      <td>32.728615</td>\n",
              "      <td>17.240641</td>\n",
              "      <td>82.697870</td>\n",
              "      <td>103.504549</td>\n",
              "      <td>66.143863</td>\n",
              "      <td>37.360686</td>\n",
              "      <td>169.648412</td>\n",
              "      <td>4.376708</td>\n",
              "      <td>2.426611</td>\n",
              "      <td>149.832364</td>\n",
              "      <td>34.523784</td>\n",
              "      <td>107.238951</td>\n",
              "      <td>58.226891</td>\n",
              "      <td>4.243756</td>\n",
              "      <td>2.459024</td>\n",
              "      <td>141.193904</td>\n",
              "      <td>33.987154</td>\n",
              "      <td>112.817594</td>\n",
              "      <td>57.899252</td>\n",
              "      <td>0.951236</td>\n",
              "      <td>0.641439</td>\n",
              "      <td>0.681541</td>\n",
              "      <td>0.895266</td>\n",
              "      <td>0.559964</td>\n",
              "      <td>1.089641</td>\n",
              "      <td>0.916873</td>\n",
              "      <td>0.665480</td>\n",
              "      <td>141.328331</td>\n",
              "      <td>0.356888</td>\n",
              "      <td>0.353911</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.439891</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>0.031853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.458246</td>\n",
              "      <td>36.279936</td>\n",
              "      <td>23.325050</td>\n",
              "      <td>12.954885</td>\n",
              "      <td>59.604986</td>\n",
              "      <td>31.549222</td>\n",
              "      <td>23.099706</td>\n",
              "      <td>8.449515</td>\n",
              "      <td>54.648928</td>\n",
              "      <td>39.162020</td>\n",
              "      <td>33.679494</td>\n",
              "      <td>5.482527</td>\n",
              "      <td>72.841514</td>\n",
              "      <td>37.256992</td>\n",
              "      <td>27.997780</td>\n",
              "      <td>9.259212</td>\n",
              "      <td>65.254772</td>\n",
              "      <td>32.188663</td>\n",
              "      <td>27.246836</td>\n",
              "      <td>4.941828</td>\n",
              "      <td>59.435499</td>\n",
              "      <td>38.946898</td>\n",
              "      <td>37.576950</td>\n",
              "      <td>1.369948</td>\n",
              "      <td>76.523849</td>\n",
              "      <td>4.047217</td>\n",
              "      <td>2.068656</td>\n",
              "      <td>85.995172</td>\n",
              "      <td>37.051364</td>\n",
              "      <td>44.437573</td>\n",
              "      <td>31.678540</td>\n",
              "      <td>5.053266</td>\n",
              "      <td>1.795977</td>\n",
              "      <td>86.474830</td>\n",
              "      <td>39.921044</td>\n",
              "      <td>45.112433</td>\n",
              "      <td>35.381825</td>\n",
              "      <td>0.988825</td>\n",
              "      <td>0.444862</td>\n",
              "      <td>0.810836</td>\n",
              "      <td>0.542516</td>\n",
              "      <td>0.667897</td>\n",
              "      <td>0.658622</td>\n",
              "      <td>1.001402</td>\n",
              "      <td>0.439276</td>\n",
              "      <td>59.687772</td>\n",
              "      <td>0.529688</td>\n",
              "      <td>0.417762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.696616</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.169087</td>\n",
              "      <td>0.146312</td>\n",
              "      <td>0.017217</td>\n",
              "      <td>0.024715</td>\n",
              "      <td>0.078889</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>0.038447</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.342323</td>\n",
              "      <td>0.060042</td>\n",
              "      <td>0.301857</td>\n",
              "      <td>74.766472</td>\n",
              "      <td>33.667006</td>\n",
              "      <td>41.099466</td>\n",
              "      <td>108.433478</td>\n",
              "      <td>44.113166</td>\n",
              "      <td>29.555688</td>\n",
              "      <td>14.557478</td>\n",
              "      <td>73.668854</td>\n",
              "      <td>68.044433</td>\n",
              "      <td>42.402254</td>\n",
              "      <td>25.642180</td>\n",
              "      <td>110.446687</td>\n",
              "      <td>95.119483</td>\n",
              "      <td>56.497295</td>\n",
              "      <td>38.622189</td>\n",
              "      <td>151.616778</td>\n",
              "      <td>62.400161</td>\n",
              "      <td>54.006420</td>\n",
              "      <td>8.393741</td>\n",
              "      <td>116.406581</td>\n",
              "      <td>104.351296</td>\n",
              "      <td>74.683000</td>\n",
              "      <td>29.668296</td>\n",
              "      <td>179.034296</td>\n",
              "      <td>3.439162</td>\n",
              "      <td>2.374293</td>\n",
              "      <td>116.377066</td>\n",
              "      <td>29.226454</td>\n",
              "      <td>81.122596</td>\n",
              "      <td>38.389404</td>\n",
              "      <td>3.774469</td>\n",
              "      <td>2.431975</td>\n",
              "      <td>117.490035</td>\n",
              "      <td>41.194225</td>\n",
              "      <td>114.484831</td>\n",
              "      <td>69.298856</td>\n",
              "      <td>0.977561</td>\n",
              "      <td>0.712606</td>\n",
              "      <td>0.830197</td>\n",
              "      <td>0.839097</td>\n",
              "      <td>0.534820</td>\n",
              "      <td>1.302525</td>\n",
              "      <td>1.020938</td>\n",
              "      <td>0.682329</td>\n",
              "      <td>196.767870</td>\n",
              "      <td>0.453209</td>\n",
              "      <td>0.590185</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026749</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.453958</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>0.220804</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cervix_area   os_area  walls_area  ...  experts::4  experts::5  consensus\n",
              "0       0.344647  0.003080    0.047522  ...         1.0         1.0        1.0\n",
              "1       0.165329  0.000000    0.048236  ...         0.0         0.0        0.0\n",
              "2       0.457010  0.001681    0.242888  ...         0.0         0.0        0.0\n",
              "3       0.513244  0.005711    0.213781  ...         1.0         1.0        1.0\n",
              "4       0.390319  0.009454    0.272884  ...         1.0         1.0        1.0\n",
              "..           ...       ...         ...  ...         ...         ...        ...\n",
              "282     0.610160  0.002726    0.298345  ...         0.0         0.0        0.0\n",
              "283     0.439891  0.006005    0.000000  ...         0.0         0.0        1.0\n",
              "284     0.696616  0.006693    0.169087  ...         1.0         0.0        1.0\n",
              "285     1.000000  0.000000    0.000000  ...         0.0         0.0        0.0\n",
              "286     1.000000  0.007517    0.000000  ...         0.0         0.0        0.0\n",
              "\n",
              "[287 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_deFZG7YvKWh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Το dataset περιγράφει μετρήσεις κολποσκοπήσεων και την κατάσταση (ετικέτα) των αντίστοιχων κόλπων, όπως προκύπτει από τις εκτιμήσεις καθενός από 6 ειδικούς και την εκτίμηση της πλειοψηφίας. Οι μετρήσεις και οι εκτιμήσεις είναι διαφορετικές για τις διαφορετικές μεθόδους με τις οποίες πραγματοποιούνται οι κολποσκοπήσεις: Hinselmann, Green, Schiller.\n",
        "\n",
        "\n",
        "\n",
        "2.    Έχουμε συνολικά 287 δείγματα, 92 από τη μέθοδο Schiller, 98 από τη μέθοδο Green και 97 από τη μέθοδο Hinselmann. Σε κάθε δείγμα περιέχονται μετρήσεις για 62 χαρακτηριστικά-παρατηρήσεις από τα οποία προέκυψαν οι εκτιμήσεις. Όλα τα χαρακτηριστικά είναι διατεταγμένα.Οι μετρήσεις είναι αριθμητικά δεδομένα και αφορούν τιμές όπως τις επιφάνειες περιοχών του κόπλου.\n",
        "\n",
        "3.    Υπάρχουν επικεφαλίδες στην πρώτη γραμμή πάνω από τα χαρακτηριστικά και τις ετικέτες, οι οποίες θα πρέπει να αφαιρεθούν. Δεν υπάρχει στήλη για την αρίθμηση των γραμμών.\n",
        "\n",
        "4.    Η τιμή της κατάστασης μπορεί να πάρει δύο τιμές, 0 για κακή και 1 για καλή. Όπως υποδεικνύεται από τις FAQ το πρόβλημα θα αναλυθεί ως binary classification λαμβάνοντας υπόψην μόνο τις εκτιμήσεις της πλειοψηφίας. Έτσι οι στήλες των ετικετών που αφορούν μεμονωμένα τον κάθε ειδικό αφαιρούνται και μένει μόνο η τελευταία στήλη με τις εκτιμήσεις της πλειοψηφίας. Η στήλη αυτή είναι η τελευταία (θέση 69 στον αρχικό πίνακα). Παρατηρούμε οτι όλες οι ισοψηφίες (3-3) επιλύονται θεωρώντας την κατάσταση ως καλή, γεγονός που ίσως μας αναγκάσει να αυξήσουμε αργότερα το πλήθος των δειγμάτων με ετικέτα 0 ή να διαγράψουμε δείγματα με ετικέτα 1.\n",
        "\n",
        "    \n",
        "5.    Συνενώνουμε τα αρχεία που αφορούν τις 3 διαφορετικές μεθόδους. Μετά τη συνένωση οι επικεφαλίδες προστέθηκαν ως γραμμές, με αποτέλεσμα να έχουμε 3 φορές την ίδια γραμμή με τις επικεφαλίδες, τις οποίες και αφαιρούμε.\n",
        "\n",
        "6.    Δεν υπάρχουν απουσιάζουσες τιμές.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0110eLi0kT7",
        "colab_type": "text"
      },
      "source": [
        "Ας τυπώσουμε τον αριθμό, τα ονόματα και τους τύπους του κάθε attribute, για να τα εξετάσουμε καλύτερα:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ1Qrlcl0h_t",
        "colab_type": "code",
        "outputId": "8407a6a8-bad3-4b7b-c5ee-8cca83ad1316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"We have \",len(df.columns), \" attributes.\")\n",
        "for i in range(0, len(df.columns)):\n",
        "    print('{:<10}{:<40}{:<10}{:<20}'.format(str(i+1), str(df.columns[i]),\"type: \", str(df.dtypes[df.columns[i]])))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have  69  attributes.\n",
            "1         cervix_area                             type:     float64             \n",
            "2         os_area                                 type:     float64             \n",
            "3         walls_area                              type:     float64             \n",
            "4         speculum_area                           type:     float64             \n",
            "5         artifacts_area                          type:     float64             \n",
            "6         cervix_artifacts_area                   type:     float64             \n",
            "7         os_artifacts_area                       type:     float64             \n",
            "8         walls_artifacts_area                    type:     float64             \n",
            "9         speculum_artifacts_area                 type:     float64             \n",
            "10        cervix_specularities_area               type:     float64             \n",
            "11        os_specularities_area                   type:     float64             \n",
            "12        walls_specularities_area                type:     float64             \n",
            "13        speculum_specularities_area             type:     float64             \n",
            "14        specularities_area                      type:     float64             \n",
            "15        area_h_max_diff                         type:     float64             \n",
            "16        rgb_cervix_r_mean                       type:     float64             \n",
            "17        rgb_cervix_r_std                        type:     float64             \n",
            "18        rgb_cervix_r_mean_minus_std             type:     float64             \n",
            "19        rgb_cervix_r_mean_plus_std              type:     float64             \n",
            "20        rgb_cervix_g_mean                       type:     float64             \n",
            "21        rgb_cervix_g_std                        type:     float64             \n",
            "22        rgb_cervix_g_mean_minus_std             type:     float64             \n",
            "23        rgb_cervix_g_mean_plus_std              type:     float64             \n",
            "24        rgb_cervix_b_mean                       type:     float64             \n",
            "25        rgb_cervix_b_std                        type:     float64             \n",
            "26        rgb_cervix_b_mean_minus_std             type:     float64             \n",
            "27        rgb_cervix_b_mean_plus_std              type:     float64             \n",
            "28        rgb_total_r_mean                        type:     float64             \n",
            "29        rgb_total_r_std                         type:     float64             \n",
            "30        rgb_total_r_mean_minus_std              type:     float64             \n",
            "31        rgb_total_r_mean_plus_std               type:     float64             \n",
            "32        rgb_total_g_mean                        type:     float64             \n",
            "33        rgb_total_g_std                         type:     float64             \n",
            "34        rgb_total_g_mean_minus_std              type:     float64             \n",
            "35        rgb_total_g_mean_plus_std               type:     float64             \n",
            "36        rgb_total_b_mean                        type:     float64             \n",
            "37        rgb_total_b_std                         type:     float64             \n",
            "38        rgb_total_b_mean_minus_std              type:     float64             \n",
            "39        rgb_total_b_mean_plus_std               type:     float64             \n",
            "40        hsv_cervix_h_mean                       type:     float64             \n",
            "41        hsv_cervix_h_std                        type:     float64             \n",
            "42        hsv_cervix_s_mean                       type:     float64             \n",
            "43        hsv_cervix_s_std                        type:     float64             \n",
            "44        hsv_cervix_v_mean                       type:     float64             \n",
            "45        hsv_cervix_v_std                        type:     float64             \n",
            "46        hsv_total_h_mean                        type:     float64             \n",
            "47        hsv_total_h_std                         type:     float64             \n",
            "48        hsv_total_s_mean                        type:     float64             \n",
            "49        hsv_total_s_std                         type:     float64             \n",
            "50        hsv_total_v_mean                        type:     float64             \n",
            "51        hsv_total_v_std                         type:     float64             \n",
            "52        fit_cervix_hull_rate                    type:     float64             \n",
            "53        fit_cervix_hull_total                   type:     float64             \n",
            "54        fit_cervix_bbox_rate                    type:     float64             \n",
            "55        fit_cervix_bbox_total                   type:     float64             \n",
            "56        fit_circle_rate                         type:     float64             \n",
            "57        fit_circle_total                        type:     float64             \n",
            "58        fit_ellipse_rate                        type:     float64             \n",
            "59        fit_ellipse_total                       type:     float64             \n",
            "60        fit_ellipse_goodness                    type:     float64             \n",
            "61        dist_to_center_cervix                   type:     float64             \n",
            "62        dist_to_center_os                       type:     float64             \n",
            "63        experts::0                              type:     float64             \n",
            "64        experts::1                              type:     float64             \n",
            "65        experts::2                              type:     float64             \n",
            "66        experts::3                              type:     float64             \n",
            "67        experts::4                              type:     float64             \n",
            "68        experts::5                              type:     float64             \n",
            "69        consensus                               type:     float64             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSAYGUotwWE_",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι έχουμε 5 experts, αλλά εμείς θα δουλέψουμε μόνο με την συνολική άποψη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab4WaECKwgnl",
        "colab_type": "code",
        "outputId": "4c6441ea-4a58-414d-acb3-d770a033b98e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df = df.drop(df.columns[[62, 63, 64, 65, 66, 67]], axis=1)\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cervix_area</th>\n",
              "      <th>os_area</th>\n",
              "      <th>walls_area</th>\n",
              "      <th>speculum_area</th>\n",
              "      <th>artifacts_area</th>\n",
              "      <th>cervix_artifacts_area</th>\n",
              "      <th>os_artifacts_area</th>\n",
              "      <th>walls_artifacts_area</th>\n",
              "      <th>speculum_artifacts_area</th>\n",
              "      <th>cervix_specularities_area</th>\n",
              "      <th>os_specularities_area</th>\n",
              "      <th>walls_specularities_area</th>\n",
              "      <th>speculum_specularities_area</th>\n",
              "      <th>specularities_area</th>\n",
              "      <th>area_h_max_diff</th>\n",
              "      <th>rgb_cervix_r_mean</th>\n",
              "      <th>rgb_cervix_r_std</th>\n",
              "      <th>rgb_cervix_r_mean_minus_std</th>\n",
              "      <th>rgb_cervix_r_mean_plus_std</th>\n",
              "      <th>rgb_cervix_g_mean</th>\n",
              "      <th>rgb_cervix_g_std</th>\n",
              "      <th>rgb_cervix_g_mean_minus_std</th>\n",
              "      <th>rgb_cervix_g_mean_plus_std</th>\n",
              "      <th>rgb_cervix_b_mean</th>\n",
              "      <th>rgb_cervix_b_std</th>\n",
              "      <th>rgb_cervix_b_mean_minus_std</th>\n",
              "      <th>rgb_cervix_b_mean_plus_std</th>\n",
              "      <th>rgb_total_r_mean</th>\n",
              "      <th>rgb_total_r_std</th>\n",
              "      <th>rgb_total_r_mean_minus_std</th>\n",
              "      <th>rgb_total_r_mean_plus_std</th>\n",
              "      <th>rgb_total_g_mean</th>\n",
              "      <th>rgb_total_g_std</th>\n",
              "      <th>rgb_total_g_mean_minus_std</th>\n",
              "      <th>rgb_total_g_mean_plus_std</th>\n",
              "      <th>rgb_total_b_mean</th>\n",
              "      <th>rgb_total_b_std</th>\n",
              "      <th>rgb_total_b_mean_minus_std</th>\n",
              "      <th>rgb_total_b_mean_plus_std</th>\n",
              "      <th>hsv_cervix_h_mean</th>\n",
              "      <th>hsv_cervix_h_std</th>\n",
              "      <th>hsv_cervix_s_mean</th>\n",
              "      <th>hsv_cervix_s_std</th>\n",
              "      <th>hsv_cervix_v_mean</th>\n",
              "      <th>hsv_cervix_v_std</th>\n",
              "      <th>hsv_total_h_mean</th>\n",
              "      <th>hsv_total_h_std</th>\n",
              "      <th>hsv_total_s_mean</th>\n",
              "      <th>hsv_total_s_std</th>\n",
              "      <th>hsv_total_v_mean</th>\n",
              "      <th>hsv_total_v_std</th>\n",
              "      <th>fit_cervix_hull_rate</th>\n",
              "      <th>fit_cervix_hull_total</th>\n",
              "      <th>fit_cervix_bbox_rate</th>\n",
              "      <th>fit_cervix_bbox_total</th>\n",
              "      <th>fit_circle_rate</th>\n",
              "      <th>fit_circle_total</th>\n",
              "      <th>fit_ellipse_rate</th>\n",
              "      <th>fit_ellipse_total</th>\n",
              "      <th>fit_ellipse_goodness</th>\n",
              "      <th>dist_to_center_cervix</th>\n",
              "      <th>dist_to_center_os</th>\n",
              "      <th>consensus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.344647</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>0.047522</td>\n",
              "      <td>0.288216</td>\n",
              "      <td>0.178585</td>\n",
              "      <td>0.016564</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.263560</td>\n",
              "      <td>37.594458</td>\n",
              "      <td>15.785021</td>\n",
              "      <td>21.809437</td>\n",
              "      <td>53.379479</td>\n",
              "      <td>109.918445</td>\n",
              "      <td>38.735421</td>\n",
              "      <td>71.183024</td>\n",
              "      <td>148.653865</td>\n",
              "      <td>55.029618</td>\n",
              "      <td>22.160330</td>\n",
              "      <td>32.869287</td>\n",
              "      <td>77.189948</td>\n",
              "      <td>38.561367</td>\n",
              "      <td>38.119059</td>\n",
              "      <td>0.442308</td>\n",
              "      <td>76.680426</td>\n",
              "      <td>95.109755</td>\n",
              "      <td>51.565052</td>\n",
              "      <td>43.544702</td>\n",
              "      <td>146.674807</td>\n",
              "      <td>48.808474</td>\n",
              "      <td>40.765228</td>\n",
              "      <td>8.043247</td>\n",
              "      <td>89.573702</td>\n",
              "      <td>5.014628</td>\n",
              "      <td>2.991944</td>\n",
              "      <td>167.952780</td>\n",
              "      <td>25.813163</td>\n",
              "      <td>109.919447</td>\n",
              "      <td>38.733741</td>\n",
              "      <td>5.090801</td>\n",
              "      <td>2.936650</td>\n",
              "      <td>159.486916</td>\n",
              "      <td>38.437294</td>\n",
              "      <td>95.123889</td>\n",
              "      <td>51.583029</td>\n",
              "      <td>0.923067</td>\n",
              "      <td>0.373371</td>\n",
              "      <td>0.844454</td>\n",
              "      <td>0.408130</td>\n",
              "      <td>0.603399</td>\n",
              "      <td>0.571175</td>\n",
              "      <td>0.962995</td>\n",
              "      <td>0.357890</td>\n",
              "      <td>85.474311</td>\n",
              "      <td>0.265933</td>\n",
              "      <td>0.346294</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.165329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048236</td>\n",
              "      <td>0.504736</td>\n",
              "      <td>0.502783</td>\n",
              "      <td>0.007012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097405</td>\n",
              "      <td>0.973837</td>\n",
              "      <td>0.004055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054999</td>\n",
              "      <td>0.028431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.505882</td>\n",
              "      <td>24.361877</td>\n",
              "      <td>35.144005</td>\n",
              "      <td>83.867760</td>\n",
              "      <td>122.366075</td>\n",
              "      <td>44.742407</td>\n",
              "      <td>77.623669</td>\n",
              "      <td>167.108482</td>\n",
              "      <td>78.058434</td>\n",
              "      <td>30.818729</td>\n",
              "      <td>47.239706</td>\n",
              "      <td>108.877163</td>\n",
              "      <td>54.932467</td>\n",
              "      <td>39.447415</td>\n",
              "      <td>15.485052</td>\n",
              "      <td>94.379883</td>\n",
              "      <td>101.680459</td>\n",
              "      <td>46.028852</td>\n",
              "      <td>55.651607</td>\n",
              "      <td>147.709311</td>\n",
              "      <td>63.218931</td>\n",
              "      <td>43.925912</td>\n",
              "      <td>19.293019</td>\n",
              "      <td>107.144843</td>\n",
              "      <td>4.944382</td>\n",
              "      <td>2.965108</td>\n",
              "      <td>130.260492</td>\n",
              "      <td>24.143867</td>\n",
              "      <td>122.366647</td>\n",
              "      <td>44.743932</td>\n",
              "      <td>5.080063</td>\n",
              "      <td>2.894163</td>\n",
              "      <td>128.251978</td>\n",
              "      <td>33.000693</td>\n",
              "      <td>101.725519</td>\n",
              "      <td>46.093510</td>\n",
              "      <td>0.850861</td>\n",
              "      <td>0.194308</td>\n",
              "      <td>0.646645</td>\n",
              "      <td>0.255673</td>\n",
              "      <td>0.497315</td>\n",
              "      <td>0.332444</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.184803</td>\n",
              "      <td>124.794129</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.283059</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.457010</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.242888</td>\n",
              "      <td>0.212859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083055</td>\n",
              "      <td>0.018591</td>\n",
              "      <td>0.269798</td>\n",
              "      <td>39.353851</td>\n",
              "      <td>19.417332</td>\n",
              "      <td>19.936519</td>\n",
              "      <td>58.771183</td>\n",
              "      <td>109.543386</td>\n",
              "      <td>49.753490</td>\n",
              "      <td>59.789896</td>\n",
              "      <td>159.296876</td>\n",
              "      <td>54.642888</td>\n",
              "      <td>27.781965</td>\n",
              "      <td>26.860923</td>\n",
              "      <td>82.424853</td>\n",
              "      <td>41.242230</td>\n",
              "      <td>34.196356</td>\n",
              "      <td>7.045875</td>\n",
              "      <td>75.438586</td>\n",
              "      <td>109.592342</td>\n",
              "      <td>57.576640</td>\n",
              "      <td>52.015702</td>\n",
              "      <td>167.168982</td>\n",
              "      <td>53.470241</td>\n",
              "      <td>38.344391</td>\n",
              "      <td>15.125850</td>\n",
              "      <td>91.814632</td>\n",
              "      <td>5.049946</td>\n",
              "      <td>2.983966</td>\n",
              "      <td>163.576979</td>\n",
              "      <td>24.973042</td>\n",
              "      <td>109.544201</td>\n",
              "      <td>49.753659</td>\n",
              "      <td>5.078936</td>\n",
              "      <td>2.968023</td>\n",
              "      <td>162.268659</td>\n",
              "      <td>33.590792</td>\n",
              "      <td>109.597127</td>\n",
              "      <td>57.584515</td>\n",
              "      <td>0.918514</td>\n",
              "      <td>0.497554</td>\n",
              "      <td>0.747443</td>\n",
              "      <td>0.611432</td>\n",
              "      <td>0.633925</td>\n",
              "      <td>0.720923</td>\n",
              "      <td>0.920287</td>\n",
              "      <td>0.496596</td>\n",
              "      <td>94.948697</td>\n",
              "      <td>0.518740</td>\n",
              "      <td>0.419375</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.513244</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.213781</td>\n",
              "      <td>0.251819</td>\n",
              "      <td>0.079795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.107022</td>\n",
              "      <td>46.322391</td>\n",
              "      <td>17.711957</td>\n",
              "      <td>28.610434</td>\n",
              "      <td>64.034349</td>\n",
              "      <td>116.075087</td>\n",
              "      <td>43.593124</td>\n",
              "      <td>72.481962</td>\n",
              "      <td>159.668211</td>\n",
              "      <td>51.430923</td>\n",
              "      <td>18.573016</td>\n",
              "      <td>32.857907</td>\n",
              "      <td>70.003940</td>\n",
              "      <td>40.365565</td>\n",
              "      <td>17.259087</td>\n",
              "      <td>23.106478</td>\n",
              "      <td>57.624652</td>\n",
              "      <td>102.641859</td>\n",
              "      <td>38.606995</td>\n",
              "      <td>64.034863</td>\n",
              "      <td>141.248854</td>\n",
              "      <td>50.805205</td>\n",
              "      <td>18.072101</td>\n",
              "      <td>32.733104</td>\n",
              "      <td>68.877306</td>\n",
              "      <td>5.177654</td>\n",
              "      <td>2.969214</td>\n",
              "      <td>156.242754</td>\n",
              "      <td>25.499379</td>\n",
              "      <td>116.081770</td>\n",
              "      <td>43.582671</td>\n",
              "      <td>5.071879</td>\n",
              "      <td>2.909002</td>\n",
              "      <td>158.343946</td>\n",
              "      <td>28.273928</td>\n",
              "      <td>102.648278</td>\n",
              "      <td>38.598300</td>\n",
              "      <td>0.951710</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.855409</td>\n",
              "      <td>0.599998</td>\n",
              "      <td>0.618140</td>\n",
              "      <td>0.830304</td>\n",
              "      <td>0.964611</td>\n",
              "      <td>0.532073</td>\n",
              "      <td>74.221670</td>\n",
              "      <td>0.347202</td>\n",
              "      <td>0.361672</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.390319</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.272884</td>\n",
              "      <td>0.373487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.071442</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>0.442831</td>\n",
              "      <td>37.552979</td>\n",
              "      <td>14.454975</td>\n",
              "      <td>23.098004</td>\n",
              "      <td>52.007953</td>\n",
              "      <td>101.044906</td>\n",
              "      <td>37.171973</td>\n",
              "      <td>63.872932</td>\n",
              "      <td>138.216879</td>\n",
              "      <td>53.971671</td>\n",
              "      <td>17.591669</td>\n",
              "      <td>36.380003</td>\n",
              "      <td>71.563340</td>\n",
              "      <td>40.717229</td>\n",
              "      <td>38.810699</td>\n",
              "      <td>1.906530</td>\n",
              "      <td>79.527927</td>\n",
              "      <td>97.446185</td>\n",
              "      <td>48.106253</td>\n",
              "      <td>49.339932</td>\n",
              "      <td>145.552438</td>\n",
              "      <td>51.733004</td>\n",
              "      <td>39.866956</td>\n",
              "      <td>11.866047</td>\n",
              "      <td>91.599960</td>\n",
              "      <td>4.978534</td>\n",
              "      <td>2.964685</td>\n",
              "      <td>158.160578</td>\n",
              "      <td>30.308891</td>\n",
              "      <td>101.050711</td>\n",
              "      <td>37.163383</td>\n",
              "      <td>5.051587</td>\n",
              "      <td>2.923540</td>\n",
              "      <td>157.131407</td>\n",
              "      <td>39.439642</td>\n",
              "      <td>97.457304</td>\n",
              "      <td>48.125747</td>\n",
              "      <td>0.955996</td>\n",
              "      <td>0.408286</td>\n",
              "      <td>0.882990</td>\n",
              "      <td>0.442043</td>\n",
              "      <td>0.623938</td>\n",
              "      <td>0.625574</td>\n",
              "      <td>0.957604</td>\n",
              "      <td>0.407600</td>\n",
              "      <td>61.546536</td>\n",
              "      <td>0.437852</td>\n",
              "      <td>0.673196</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.610160</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.298345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028185</td>\n",
              "      <td>0.046193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015983</td>\n",
              "      <td>0.104929</td>\n",
              "      <td>73.465862</td>\n",
              "      <td>33.856824</td>\n",
              "      <td>39.609038</td>\n",
              "      <td>107.322686</td>\n",
              "      <td>44.657984</td>\n",
              "      <td>35.514453</td>\n",
              "      <td>9.143531</td>\n",
              "      <td>80.172437</td>\n",
              "      <td>97.007389</td>\n",
              "      <td>67.135578</td>\n",
              "      <td>29.871812</td>\n",
              "      <td>164.142967</td>\n",
              "      <td>80.241955</td>\n",
              "      <td>32.698142</td>\n",
              "      <td>47.543813</td>\n",
              "      <td>112.940096</td>\n",
              "      <td>49.969255</td>\n",
              "      <td>32.728615</td>\n",
              "      <td>17.240641</td>\n",
              "      <td>82.697870</td>\n",
              "      <td>103.504549</td>\n",
              "      <td>66.143863</td>\n",
              "      <td>37.360686</td>\n",
              "      <td>169.648412</td>\n",
              "      <td>4.376708</td>\n",
              "      <td>2.426611</td>\n",
              "      <td>149.832364</td>\n",
              "      <td>34.523784</td>\n",
              "      <td>107.238951</td>\n",
              "      <td>58.226891</td>\n",
              "      <td>4.243756</td>\n",
              "      <td>2.459024</td>\n",
              "      <td>141.193904</td>\n",
              "      <td>33.987154</td>\n",
              "      <td>112.817594</td>\n",
              "      <td>57.899252</td>\n",
              "      <td>0.951236</td>\n",
              "      <td>0.641439</td>\n",
              "      <td>0.681541</td>\n",
              "      <td>0.895266</td>\n",
              "      <td>0.559964</td>\n",
              "      <td>1.089641</td>\n",
              "      <td>0.916873</td>\n",
              "      <td>0.665480</td>\n",
              "      <td>141.328331</td>\n",
              "      <td>0.356888</td>\n",
              "      <td>0.353911</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.439891</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>0.031853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.458246</td>\n",
              "      <td>36.279936</td>\n",
              "      <td>23.325050</td>\n",
              "      <td>12.954885</td>\n",
              "      <td>59.604986</td>\n",
              "      <td>31.549222</td>\n",
              "      <td>23.099706</td>\n",
              "      <td>8.449515</td>\n",
              "      <td>54.648928</td>\n",
              "      <td>39.162020</td>\n",
              "      <td>33.679494</td>\n",
              "      <td>5.482527</td>\n",
              "      <td>72.841514</td>\n",
              "      <td>37.256992</td>\n",
              "      <td>27.997780</td>\n",
              "      <td>9.259212</td>\n",
              "      <td>65.254772</td>\n",
              "      <td>32.188663</td>\n",
              "      <td>27.246836</td>\n",
              "      <td>4.941828</td>\n",
              "      <td>59.435499</td>\n",
              "      <td>38.946898</td>\n",
              "      <td>37.576950</td>\n",
              "      <td>1.369948</td>\n",
              "      <td>76.523849</td>\n",
              "      <td>4.047217</td>\n",
              "      <td>2.068656</td>\n",
              "      <td>85.995172</td>\n",
              "      <td>37.051364</td>\n",
              "      <td>44.437573</td>\n",
              "      <td>31.678540</td>\n",
              "      <td>5.053266</td>\n",
              "      <td>1.795977</td>\n",
              "      <td>86.474830</td>\n",
              "      <td>39.921044</td>\n",
              "      <td>45.112433</td>\n",
              "      <td>35.381825</td>\n",
              "      <td>0.988825</td>\n",
              "      <td>0.444862</td>\n",
              "      <td>0.810836</td>\n",
              "      <td>0.542516</td>\n",
              "      <td>0.667897</td>\n",
              "      <td>0.658622</td>\n",
              "      <td>1.001402</td>\n",
              "      <td>0.439276</td>\n",
              "      <td>59.687772</td>\n",
              "      <td>0.529688</td>\n",
              "      <td>0.417762</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.696616</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.169087</td>\n",
              "      <td>0.146312</td>\n",
              "      <td>0.017217</td>\n",
              "      <td>0.024715</td>\n",
              "      <td>0.078889</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>0.038447</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.342323</td>\n",
              "      <td>0.060042</td>\n",
              "      <td>0.301857</td>\n",
              "      <td>74.766472</td>\n",
              "      <td>33.667006</td>\n",
              "      <td>41.099466</td>\n",
              "      <td>108.433478</td>\n",
              "      <td>44.113166</td>\n",
              "      <td>29.555688</td>\n",
              "      <td>14.557478</td>\n",
              "      <td>73.668854</td>\n",
              "      <td>68.044433</td>\n",
              "      <td>42.402254</td>\n",
              "      <td>25.642180</td>\n",
              "      <td>110.446687</td>\n",
              "      <td>95.119483</td>\n",
              "      <td>56.497295</td>\n",
              "      <td>38.622189</td>\n",
              "      <td>151.616778</td>\n",
              "      <td>62.400161</td>\n",
              "      <td>54.006420</td>\n",
              "      <td>8.393741</td>\n",
              "      <td>116.406581</td>\n",
              "      <td>104.351296</td>\n",
              "      <td>74.683000</td>\n",
              "      <td>29.668296</td>\n",
              "      <td>179.034296</td>\n",
              "      <td>3.439162</td>\n",
              "      <td>2.374293</td>\n",
              "      <td>116.377066</td>\n",
              "      <td>29.226454</td>\n",
              "      <td>81.122596</td>\n",
              "      <td>38.389404</td>\n",
              "      <td>3.774469</td>\n",
              "      <td>2.431975</td>\n",
              "      <td>117.490035</td>\n",
              "      <td>41.194225</td>\n",
              "      <td>114.484831</td>\n",
              "      <td>69.298856</td>\n",
              "      <td>0.977561</td>\n",
              "      <td>0.712606</td>\n",
              "      <td>0.830197</td>\n",
              "      <td>0.839097</td>\n",
              "      <td>0.534820</td>\n",
              "      <td>1.302525</td>\n",
              "      <td>1.020938</td>\n",
              "      <td>0.682329</td>\n",
              "      <td>196.767870</td>\n",
              "      <td>0.453209</td>\n",
              "      <td>0.590185</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026749</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.453958</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>0.220804</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 63 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cervix_area   os_area  ...  dist_to_center_os  consensus\n",
              "0       0.344647  0.003080  ...           0.346294        1.0\n",
              "1       0.165329  0.000000  ...           0.283059        0.0\n",
              "2       0.457010  0.001681  ...           0.419375        0.0\n",
              "3       0.513244  0.005711  ...           0.361672        1.0\n",
              "4       0.390319  0.009454  ...           0.673196        1.0\n",
              "..           ...       ...  ...                ...        ...\n",
              "282     0.610160  0.002726  ...           0.353911        0.0\n",
              "283     0.439891  0.006005  ...           0.417762        1.0\n",
              "284     0.696616  0.006693  ...           0.590185        1.0\n",
              "285     1.000000  0.000000  ...           0.402563        0.0\n",
              "286     1.000000  0.007517  ...           0.402563        0.0\n",
              "\n",
              "[287 rows x 63 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFW2ZAJs6pIz",
        "colab_type": "text"
      },
      "source": [
        "Ας μετρήσουμε τώρα, πόσοι είναι υγιείς και πόσοι όχι, και θα το αποθηκεύσουμε αυτό σαν τα labels μας (βεβαίως, θα πρέπει μετά να αφαιρέσουμε αυτήν την κολώνα χαρακτηριστικών, πριν κάνουμε training).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2757HtVM6vna",
        "colab_type": "code",
        "outputId": "3f9597f1-0bf2-448d-9a55-db3a353211f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels_df = df.iloc[:, [62]]\n",
        "labels = labels_df.values.reshape(287,)\n",
        "positive = 0\n",
        "negative = 0\n",
        "for i in range(0, len(labels)):\n",
        "    positive += labels[i]==1\n",
        "    negative += labels[i]==0\n",
        "print(\"positive = \", positive, \" negative: \", negative)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive =  216  negative:  71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe8KtmADxNwS",
        "colab_type": "text"
      },
      "source": [
        "Βλεπουμε οτι εχουμε unbalanced data set, καθως η κλαση των positive(υγειων) ειναι κατα πολυ μεγαλυτερη σε μεγεθος απο την κλαση των negative. (75-25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2l2kgIc_qXj",
        "colab_type": "text"
      },
      "source": [
        "Θα πρέπει αρχικά να κάνουμε split τα δεδομένα μας, όπως λέει η εκφώνηση 80-20. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBRy3W1k8o82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train, test, train_labels, test_labels = train_test_split(df, labels, test_size=0.2, random_state=78)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCBzUF3cxwp8",
        "colab_type": "text"
      },
      "source": [
        "Ας επαληθεύσουμε ότι πάλι ο λόγος των positive(υγειων) με των negative(άρρωστων) είναι πάλι skewed:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF_OBTmhx9IA",
        "colab_type": "code",
        "outputId": "9738ad37-a907-47e9-e257-aff10b942974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "positive = 0\n",
        "negative = 0\n",
        "for i in range(0, len(train_labels)):\n",
        "    positive += train_labels[i]==1\n",
        "    negative += train_labels[i]==0\n",
        "print(\"positive = \", positive, \" negative: \", negative)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive =  174  negative:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PTz3kzyO5S",
        "colab_type": "text"
      },
      "source": [
        "Ας γράψουμε τώρα τις συναρτήσεις μετασχηματισμών και δημιουργίας των classifiers, που θα τις χρησιμοποιήσουμε εντός του pipeline. Έχουμε γράψει συναρτήσεις για selection δεδομένων, resampling (κάτι που θα χρειαστεί σίγουρα, καθώς δεν έχουμε ισορροπημένο dataset), standardization και τέλος για την δημιοργία των κατηγοριοποιητών. Όλες αυτές οι μέθοδοι είναι αρκετά γενικευμένοι και μπορούν να πάρουν σαν παραμέτρους διαφορετικές τεχνικές που ίσως να θέλουμε να εφαρμόσουμε στην κάθε περίπτωση."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miepy3x5hvpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies a transformation (selection) on a dataset, \n",
        "    based on the type of the selector. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no transformation happens. The selector argument\n",
        "    is a string and can be one of two things: either \"PCA\" or \n",
        "    \"Variance Threshold\". In either case, there can be arguments, and in \"PCA\"\n",
        "    the arguments can be an int of the number of principal components, and in\n",
        "    \"Variance Threshold\", it can be a float. Returns the transformed dataset.\n",
        "\"\"\"\n",
        "def Apply_selector(data, selector = None, arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    \n",
        "    if selector == \"PCA\":    \n",
        "        if arguments:\n",
        "            number_of_components = arguments\n",
        "            pca = PCA(n_components = number_of_components)\n",
        "            pca.fit(train)\n",
        "            train_reduced = pca.transform(train)\n",
        "            test_reduced = pca.transform(test)\n",
        "        else:\n",
        "            pca = PCA()\n",
        "            pca.fit(train)\n",
        "            train_reduced = pca.transform(train)\n",
        "            test_reduced = pca.transform(test)\n",
        "    elif selector == \"Variance Threshold\":\n",
        "        if arguments:\n",
        "            t = arguments\n",
        "            sel = VarianceThreshold(threshold=t)\n",
        "            train_reduced = sel.fit_transform(train)\n",
        "            test_reduced = sel.transform(test)\n",
        "        else:\n",
        "            sel = VarianceThreshold()\n",
        "            train_reduced = sel.fit_transform(train)\n",
        "            test_reduced = sel.transform(test)\n",
        "    else:\n",
        "        (train_reduced, train_labels, test_reduced, test_labels) = data\n",
        "    return (train_reduced, train_labels, test_reduced, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05vv4ikkh3Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies resampling on a dataset, based on the \n",
        "    type of sampling method. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no sampling happens. The sampling argument\n",
        "    is a string and can be one of three things: \"Over\", \"Under\" or \n",
        "    \"OverUnder\". In any case, there can be arguments, and in Under or Over or\n",
        "    OverUnder it can be a float with the ratio of samples between classes. \n",
        "    Returns the resampled dataset.\n",
        "\"\"\"\n",
        "def Apply_sampling(data, sampling = None, arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    if sampling == \"Over\":    \n",
        "        if arguments:\n",
        "            ratio = arguments\n",
        "            ros = RandomOverSampler(sampling_strategy=ratio)\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "        else:\n",
        "            ros = RandomOverSampler()\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "        return (train_oversampled, train_labels_oversampled, test, test_labels)\n",
        "    elif sampling == \"Under\":\n",
        "        if arguments:\n",
        "            ratio = arguments\n",
        "            rus = RandomUnderSampler(sampling_strategy=ratio)\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train,train_labels)\n",
        "        else:\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train,train_labels)\n",
        "        return (train_undersampled, train_labels_undersampled, test, test_labels)\n",
        "    elif sampling==\"OverUnder\":\n",
        "        if arguments:    \n",
        "            ratio = arguments\n",
        "            ros = RandomOverSampler(sampling_strategy=ratio)\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train_oversampled,train_labels_oversampled)\n",
        "        else:\n",
        "            ros = RandomOverSampler()\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train_oversampled,train_labels_oversampled)\n",
        "        return (train_undersampled, train_labels_undersampled, test, test_labels)\n",
        "    else:\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qw5kMxliJud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats as st\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies a transformation (standardization) on a dataset, \n",
        "    based on the type of the standardizer. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no transformation happens. The selector argument\n",
        "    is a string and can be one of two things: either \"zscore\" or \n",
        "    \"minmax\". Argument arguments is always None. Returns the transformed dataset.\n",
        "\"\"\"\n",
        "def Apply_standardizer(data, standardizer = None,  arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    if standardizer==\"zscore\":\n",
        "        std_train = st.zscore(train)\n",
        "    elif standardizer==\"minmax\":\n",
        "        std_train = (train - np.min(train) )/ (np.max(train) - np.min(train))\n",
        "    else:\n",
        "        std_train = train\n",
        "    return (std_train, train_labels, test, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2doolScyIJqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that creates a classifier object. It does not train, \n",
        "    it takes the name of the classifier, along with the arguments we want \n",
        "    to pass to the instance creator for the classifier and returns the \n",
        "    classifier object.\n",
        "\"\"\"\n",
        "def Create_classifier(classifier,  arguments = None):\n",
        "    if classifier==\"kNN\":\n",
        "        if arguments and arguments!=-1:\n",
        "            clf = neighbors.KNeighborsClassifier(n_jobs = -1, n_neighbors = arguments)\n",
        "        else:\n",
        "            clf = neighbors.KNeighborsClassifier(n_jobs = -1)\n",
        "    else:\n",
        "        if arguments:\n",
        "            strat = arguments\n",
        "            clf = DummyClassifier(strategy=strat)\n",
        "        else:\n",
        "            clf = neighbors.KNeighborsClassifier()\n",
        "    return clf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVhGuPcy6eh",
        "colab_type": "text"
      },
      "source": [
        "Εδώ ορίζουμε την pipeline, η οποία αντιστοιχίζει τα steps της προεπεξεργασίας δεδομένων και της δημιοργίας των classifiers με τις παραπάνω μεθόδους. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMX219HCiTQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    This is an implementation for the function imblearn.pipeline.Pipeline().\n",
        "    The way this function works is, it takes as input two arguments. The first\n",
        "    is a list. This list contains the steps that need to be executed in the \n",
        "    pipeline, IN ORDER. Always the last step is the classifier. Each step is\n",
        "    a tuple, which contains the name of the step, and the arguments needed to \n",
        "    execute the step, like so: (\"name\", arguments). The second argument of the \n",
        "    function is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The function outputs a\n",
        "    tuple, which contains the processed data, ready for fitting the classier, \n",
        "    and the classifier object, as specified by the arguments, like so:\n",
        "    returns: (processed_data, classifier_object)\n",
        "\"\"\"\n",
        "def Pipeline(steps, data):\n",
        "    steps_dict = {\"selector\": Apply_selector,\n",
        "                  \"sampling\": Apply_sampling,\n",
        "                  \"standardizer\": Apply_standardizer,\n",
        "                  \"kNN\": Create_classifier,\n",
        "                  \"dummy\": Create_classifier}\n",
        "    for step in steps:\n",
        "        if step[0]!=\"kNN\" and step[0]!=\"dummy\":\n",
        "            data = steps_dict[step[0]](data,step[1])\n",
        "        else:\n",
        "            return ( data, Create_classifier(step[0], arguments = step[1]) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvUNGmO9zFDD",
        "colab_type": "text"
      },
      "source": [
        "Η συνάρτηση model_tuning ουσιαστικά υλοποιεί την gridsearchcv, και δημιουργεί για κάθε παράμετρο, για κάθε αλληλουχία steps, για κάθε dataset ένα pipe, και στην συνέχεια κάνει train τα δεδομένα και βγάζει ένα training accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RktfiSkMmjfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\"\"\"\n",
        "    This is an implementation for the function gridsearchcv. It takes 3\n",
        "    arguments, a tuple (model_name, model_parameters), the data to be fitted\n",
        "    and trained, and the specific parameters. The data parameter needs to be \n",
        "    in the form: (train_data, train_labels, test_data, test_labels). Lastly, \n",
        "    the Steps argument is a list, that has the steps for the data preprocessing,\n",
        "    as described in the Pipeline Function. For this exercise, we are dealing \n",
        "    only with dummy classifiers and kNN. The model_parameters if not None, is \n",
        "    a list of parameters (strings for dummy, ints for kNN).\n",
        "    Returns (classifier_object, best_parameter, accuracy_list)\n",
        "\"\"\"\n",
        "def model_tuning(model, folded_data, preprocessing_steps = None):\n",
        "    #Start measuring the time:\n",
        "    start_time = time.time()\n",
        "    #if the classifier is a dummy:\n",
        "    if model[0]==\"dummy\":\n",
        "        #and it has parameters (strategy):\n",
        "        if model[1]!=None:\n",
        "\n",
        "\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "            best_parameter = model[1][0]\n",
        "            #for each parameter, run the \n",
        "            for parameter in model[1]:\n",
        "    \n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], parameter)]\n",
        "                else:\n",
        "                    steps = [(model[0], parameter)]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "                \n",
        "                \n",
        "                \n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list.append(sum(accuracy_list)/len(accuracy_list))\n",
        "                \n",
        "                #if the average accuracy of the 10 folds is the best one yet, \n",
        "                #keep the parameter\n",
        "                if (sum(accuracy_list)/len(accuracy_list)) >= max(avg_accuracy_list):\n",
        "                    best_parameter = parameter\n",
        "        \n",
        "        \n",
        "        \n",
        "        #if the dummy classifier has no parameters:                    \n",
        "        else:\n",
        "                #initialize the lists with the possible best parameters, and the average\n",
        "                #accuracies for each of the model parameter\n",
        "                avg_accuracy_list = []\n",
        "\n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], model[1])]\n",
        "                else:\n",
        "                    steps = [(model[0], model[1])]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                \n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "                \n",
        "                #since no parameter list has been given, best_param = param\n",
        "                best_parameter = None\n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "    \n",
        "    \n",
        "    #if the classifier is a kNN:\n",
        "    elif model[0] == \"kNN\":\n",
        "\n",
        "        #if the kNN classifier has no parameters:    \n",
        "        if model[1]== -1:\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "\n",
        "            #if there are preprocessing steps, add the model_creation \n",
        "            #to the pipeline:\n",
        "            if preprocessing_steps:\n",
        "                steps = preprocessing_steps + [(model[0], model[1])]\n",
        "            else:\n",
        "                steps = [(model[0], model[1])]\n",
        "            #create the pipeline, returns the processed data, the \n",
        "            #classifier ready for training and evaluating:\n",
        "\n",
        "            \n",
        "            #initialize a list with the accuracies for the folded data:\n",
        "            accuracy_list = []\n",
        "                \n",
        "            #train for each of the 10 folds, begin from scratch every time,\n",
        "            #and then average out the results.\n",
        "            for data in folded_data:\n",
        "                \n",
        "                #process the data, create the classifier\n",
        "                (processed_data, clf) = Pipeline(steps, data)\n",
        "                (train, train_labels, test, test_labels) = processed_data\n",
        "                #fit the classifier\n",
        "                clf = clf.fit(train, train_labels)\n",
        "                #make the predictions based on the training\n",
        "                preds = clf.predict(test)\n",
        "                accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "            \n",
        "            #since no parameter list has been given, best_param = None\n",
        "            best_parameter = None\n",
        "            #store the average accuracy\n",
        "            avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "\n",
        "        #if it has parameters (number_of_neighbors):    \n",
        "        else:\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "            best_parameter = model[1][0]\n",
        "            #for each parameter, run the \n",
        "            for parameter in model[1]:\n",
        "    \n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], parameter)]\n",
        "                else:\n",
        "                    steps = [(model[0], parameter)]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                \n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "                print(accuracy_list)\n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list.append(sum(accuracy_list)/len(accuracy_list))\n",
        "\n",
        "\n",
        "                #if the average accuracy of the 10 folds is the best one yet, \n",
        "                #keep the parameter\n",
        "                if (sum(accuracy_list)/len(accuracy_list)) >= max(avg_accuracy_list):\n",
        "                    best_parameter = parameter\n",
        "                \n",
        "        \n",
        "        \n",
        "\n",
        "    print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
        "    return (clf, best_parameter, avg_accuracy_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8P93LtzWsy",
        "colab_type": "text"
      },
      "source": [
        "Η συνάρτηση αυτή είναι η υλοποίηση της Kfold(), και σπάει σε διπλώματα το dataset. Συγκεκριμένα, το σπάει σε 10 διαφορετικά dataset, προκειμένου να αξιοποιήσουμε καλύτερα τον σχετικά μικρό όγκο δεδομένων μας. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRmPXV7pibkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "\"\"\"This function is an implementation for KFold(). It takes one argument that \n",
        "is a tuple in the form (train, train_labels, test, test_labels), and returns\n",
        "a list of k (10) tuples in the same form. It shuffles the train and train \n",
        "labels, to randomize the data, before splitting. \"\"\"\n",
        "def ten_fold(data):\n",
        "    #In our exercise, the number of folds is 10:\n",
        "    k = 10\n",
        "    #Reading the data, no need to read the test and test_labels data.\n",
        "    train, train_labels, _, _ = data\n",
        "    #Randomize the data:\n",
        "    train, train_labels = shuffle(train, train_labels)\n",
        "    #train is a pandas dataframe, need to make it a numpy array:\n",
        "    train = np.asarray(train)\n",
        "    #This is the size of the test set each time:\n",
        "    fold_size = int(len(train)/k)\n",
        "    #Create the list to be returned, fill it:\n",
        "    folds = []\n",
        "    for i in range(0,10):\n",
        "        train1 = []\n",
        "        train_labels1 = []\n",
        "        test = []\n",
        "        test_labels = []\n",
        "        for j in range(0, len(train)):\n",
        "            if j>= i*fold_size and j<(i+1)*fold_size:\n",
        "                test.append(train[j])\n",
        "                test_labels.append(train_labels[j])\n",
        "            else:\n",
        "                train1.append(train[j])\n",
        "                train_labels1.append(train_labels[j])\n",
        "        folds.append((train1, train_labels1, test, test_labels))\n",
        "    return folds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzzeMryyznY7",
        "colab_type": "text"
      },
      "source": [
        "Ας ξαναδιαβάσουμε από την αρχή τα δεδομένα μας, και ας τα χωρίσουμε πάλι σε training data και test data πριν προχωρήσουμε στο Γ. Πρέπει όμως πριν, να αφαιρέσουμε την στήλη που περιέχει τις ετικέτες για την υγεία των ασθενών."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ2oqrmDedlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"all.csv\")\n",
        "df = df.drop(df.columns[[62, 63, 64, 65, 66, 67]], axis=1)\n",
        "labels_df = df.iloc[:, [62]]\n",
        "labels = labels_df.values.reshape(287,)\n",
        "\n",
        "\n",
        "df = df.drop(df.columns[62], axis=1)\n",
        "train, test, train_labels, test_labels = train_test_split(df, labels, test_size=0.2, random_state=78)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjNCI0V_z-lz",
        "colab_type": "text"
      },
      "source": [
        "#Γ \n",
        "---\n",
        "\n",
        "Για το section αυτό, αρχικά πρέπει να εκπαιδεύσουμε τους classifiers (dummy και kNN) στο train dataset, με deafult τιμές. Καλόυμε την συνάρτηση model_tuning, και για τα δύο μοντέλα, χωρίς επιπλέον παραμέτρους, και θα λάβουμε πίσω τους classifiers, την καλύτερη παράμετρο (εδώ None, αφού δεν τους έχουμε δώσει κάποια παράμετρο για training) και το average training accuracy πάνω σε 10 folds. \n",
        "\n",
        "Δεν θα κάνουμε κάποια preprocessing steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqq8OnaOlBuY",
        "colab_type": "code",
        "outputId": "8a840aca-a812-458f-af8a-7713324eae62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#Group the splitted dataset into a tuple:\n",
        "data = (train, train_labels, test,  test_labels)\n",
        "\n",
        "dummy_default, best_parameter1, acc_list1 = model_tuning( (\"dummy\", None),  ten_fold(data)  )\n",
        "kNN_default, best_parameter2, acc_list2 = model_tuning( (\"kNN\", -1),  ten_fold(data)  )\n",
        "\n",
        "print(\"The average training accuracy of the default dummy classifier is: \", acc_list1)\n",
        "print(\"The average training accuracy of the default kNN classifier is: \", acc_list2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Συνολικός χρόνος fit και predict: 0.03234052658081055 seconds\n",
            "Συνολικός χρόνος fit και predict: 1.045217752456665 seconds\n",
            "The average training accuracy of the default dummy classifier is:  0.7727272727272728\n",
            "The average training accuracy of the default kNN classifier is:  0.7636363636363637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV0XlTdz2N1T",
        "colab_type": "text"
      },
      "source": [
        "Τώρα θα κάνουμε εκτίμηση στο test set και για τους δύο, θα βρούμε το confusion matrix τους, και θα τυπώσουμε το f1-micro και f1-macro average:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPXFLHN2NZN",
        "colab_type": "code",
        "outputId": "8a3bba79-8ffd-42e0-d4b6-b68b7514ee1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preds1 = dummy_default.predict(test)\n",
        "preds2 = kNN_default.predict(test)\n",
        "\n",
        "#print the accuracies:\n",
        "print(\"The average testing accuracy of the default dummy classifier is: \", accuracy_score(test_labels, preds1))\n",
        "print(\"The average testing accuracy of the default kNN classifier is: \", accuracy_score(test_labels, preds2))\n",
        "\n",
        "#plot the confusion matrices:\n",
        "disp1 = plot_confusion_matrix(dummy_default, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "disp1 = plot_confusion_matrix(kNN_default, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#print the classification reports, where f1 macro average is shown:\n",
        "print(classification_report(test_labels, preds1, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "print(classification_report(test_labels, preds2, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "\n",
        "\n",
        "#print the f1 micro scores:\n",
        "print(\"F1 micro score for the default dummy classifier is: \", f1_score(test_labels, preds1, average = 'micro'))\n",
        "print(\"F1 micro score for the default kNN classifier is: \", f1_score(test_labels, preds2, average = 'micro'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average testing accuracy of the default dummy classifier is:  0.7586206896551724\n",
            "The average testing accuracy of the default kNN classifier is:  0.7586206896551724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEKCAYAAABaND37AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7gAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEKCAYAAABaND37AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAd7UlEQVR4nO3de5xWdbn38c93BgQFxANKoiRtD5mH\nAEG3WbaFbW4zD1Ramtu0fCTdZfqYVpYlam4rM5/tIdugCbY9m6bbykOGqSTIQVBQg5RK8QAoKKaS\n4PX8sdbozTQz97pn7ln3msX3zWu9Zh1/67qZeV3zm2ut9VuKCMzMLD9NjQ7AzGx948RrZpYzJ14z\ns5w58ZqZ5cyJ18wsZ068ZmY5c+I1M6uBpGZJj0i6I11+n6QZkv4k6QZJG1Rrw4nXzKw2JwNPVCz/\nALgoIrYHVgDHVWvAidfMLCNJ2wCfAK5IlwWMBW5Od5kCjKvWTq/uCnB9sPmgQTH0vds2OgyrwZt/\nX9voEKwGLyx5hpUrXlJX2mjeeNuINW9k2jfeWLYAeLNi1cSImFix/P+ArwMD0uXNgZURsSZdfhbY\nutp5nHi7YOh7t+XeB2Y0OgyrwYIlrzY6BKvB+E+N7XIbseYN+rz/M5n2fXPuZW9GxOi2tkk6CFga\nEbMl7duVmJx4zazkBKpLVfXDwCGSDgT6AhsD/wVsIqlX2uvdBlhSrSHXeM2s3AQ0NWebOhARZ0TE\nNhExDDgC+F1EHAVMBQ5LdzsGuK1aSE68ZlZ+Urapc74BnCrpTyQ13yurHeBSg5mVXN1KDe+IiPuA\n+9L5p4E9azneidfMyq/zvdlu4cRrZuUm6t7j7SonXjMruS7Vb7uFE6+ZlV+VOxby5sRrZiVX/4tr\nXeXEa2blJlxqMDPLnXu8ZmZ5cqnBzCxfApp9cc3MLF+u8ZqZ5cmlBjOz/LnHa2aWM/d4zcxy1LUh\nH7uFE6+ZlZ8fGTYzy5MvrpmZ5c+lBjOzHHk8XjOzvLnUYGaWv4JdXCvWrwEzs+5Qh7cMS+or6WFJ\n8yQtkHR2un6ypMWS5qbTiGrhuMdrZuWmupUaVgNjI+I1Sb2BByX9Jt12ekTcnLUhJ14zK7863NUQ\nEQG8li72TqfoTFsuNZhZ6UnKNAGDJM2qmMa3aqdZ0lxgKXBPRMxIN50n6VFJF0nqUy0e93jNrNSS\nN/9k7vEuj4jR7W2MiLXACEmbALdK2hU4A3gB2ACYCHwDOKejk7jHa2blJqGmbFNWEbESmAocEBHP\nR2I1cBWwZ7XjnXjNrPRqKDV01MYWaU8XSRsCHwOelLRVuk7AOGB+tXhcajCz0quh1NCRrYApkppJ\nOq03RsQdkn4naQuSqsZc4IRqDTnxmlnp1SPxRsSjwMg21o+ttS0nXjMrN6VTgTjxmlmpier127w5\n8ZpZ6TU1Fes+AideMys993jNzPLkGq+ZWf7c4zUzy5EvrpmZNUAtjwPnwYnXzMpNLjWYmeXOidfM\nLGdOvGZmOfLFNTOzRihW3nXiNbOSkx8ZNjPLnUsNZmZ5K1bedeK1da1d+zYH/p8Lec8WA5nyw/HV\nD7BcXXj5rcyYs5BNNu7HxAu/AsCk/7mL6bP/SO9ezWw1eDO+duI4+vfbsMGRFkvReryFKXxIOlbS\npXVqa7Kkw9L5UyRtVLHttXqco6yuvOn3bL/t4EaHYe3Y/19Gct4ZR6+zbvfdtmPij77MTy/4Mltv\ntTnX//KBBkVXTFnft5Znci5M4u1GpwAbVd3LeG7pSu596HE+d/BejQ7F2rHbzsMY0H/d3uyo4dvT\n3NwMwAd22IblL73aiNAKbb1JvJKGSZpfsXyapAmS7pP0A0kPS1ooaZ+Kw4ZIulPSIkk/rDh2f0kP\nSZoj6SZJ/dP135U0U9J8SRPV6n9O0leBIcBUSVMr1p8naZ6k6ZIGSxogabGk3un2jSuX1xcTLr6V\nb594SOH+LLPs7po6hz1G7tDoMAqnHq93l9Q3zVvzJC2QdHa6/n2SZkj6k6QbJG1QLZ5G9Xh7RcSe\nJL3RsyrWjwA+C+wGfFbSUEmDgDOB/SJid2AWcGq6/6URsUdE7ApsCBxUeZKIuBh4DhgTEWPS1f2A\n6RExHLgfOD4iVgH3AZ9I9zkCuCUi3moduKTxkmZJmvXS8uVd+18okN9OW8CgTfrzwZ2GNjoU66Rr\nb/k9zc3NjP3IBxsdSuHUqce7Ghib5o4RwAGS9gJ+AFwUEdsDK4DjqjXUqMR7S/p1NjCsYv29EfFK\nRLwJPA5sC+wF7AxMkzQXOCZdDzAm/U3zGDAW2CXDuf8O3NHG+a8AvpDOfwG4qq2DI2JiRIyOiNGb\nDxqU4XQ9w8zHnubuafPZ67Cz+fKEq5k2exEnnfPzRodlGd193yM8POePfOOkT/svltZUn8QbiZZr\nRL3TKUhyz83p+inAuGohdeddDWtYN7H3rZhfnX5d2yqG1RXzLdsE3BMRR1Y2Lqkv8BNgdEQ8I2lC\nq3O0562IiNbnj4hpaXlkX6A5Iua310AZnXHCwZxxwsEA/GHOIv77+qlc8t2jqxxlRTBz7iJuuv1B\nLpjwRfr2qfpX7npHQL1+F0lqJumwbQ9cBjwFrIyINekuzwJbV2unOxPvi8CWkjYHXiMpA9zZiXam\nA5dJ2j4i/iSpH8kHW5puX57WfA/j3d86lVYBA4AsdYGrgWuBczsRp1m3O/+/buLRxxfzyqrXOerE\nH3H04WO4/pcP8NaaNZzxvSkA7LTDNpx8/CENjrRIarpwNkjSrIrliRExsWUhItYCIyRtAtwK7NSZ\niLot8UbEW5LOAR4GlgBPdrKdZZKOBa6T1CddfWZELJQ0CZgPvADMbKeJicCdkp6rqPO25xrge8B1\nnYm1LPbefQf23t0XaIrojJMP/4d1B4wd1YBIepam7AOhL4+I0dV2ioiV6QX7DwGbSOqV9nq3Icl3\nHerWByjSi1sXd7B9OWmNNSImA5Mrth1UMf87YI82jj+T5MJb6/XHVsxfAlxSsdy/Yv5m1u0lfwS4\nOSJWdvCxzKwnUX1KDZK2IClVrpS0IfAxkgtrU0n+4r6e5BrUbdXa8pNrKUmXAB8HDmx0LGZWP6Km\nHm9HtgKmpHXeJuDGiLhD0uPA9ZK+BzwCXFmtISfeVESc1OgYzKx71KPHGxGPAiPbWP80sGctbTnx\nmlnpFe0WOydeMyu3OtV468mJ18xKTcgDoZuZ5c09XjOznLnGa2aWJ9d4zczylYzVUKzM68RrZqVX\nsLzrxGtm5VenJ9fqxonXzMpNLjWYmeWqnuPx1osTr5mVXL4vsszCidfMSq9gedeJ18xKTr64ZmaW\nK9/Ha2bWAE68ZmY5K1jedeI1s/Jzj9fMLE8eJMfMLF/JQOjFyrzFGpbdzKwbNEmZpo5IGippqqTH\nJS2QdHK6foKkJZLmplPVN5W7x2tmpVenUsMa4GsRMUfSAGC2pHvSbRdFxI+yNuTEa2alpjoNkhMR\nzwPPp/OrJD0BbN2ZttotNUjauKOpc6GbmeWvSdkmYJCkWRXT+LbakzQMGAnMSFd9RdKjkn4madNq\n8XTU410ABMmDHy1algN4b7XGzcyKoIaLa8sjYnRHO0jqD/wCOCUiXpV0OXAuSV48F7gQ+GJHbbSb\neCNiaNZIzcyKSiR3NtSlLak3SdK9JiJuAYiIFyu2TwLuqNZOprsaJB0h6Vvp/DaSRnUqajOzBqih\n1NAuJYXiK4EnIuLHFeu3qtjtk8D8avFUvbgm6VKgN/BR4D+B14GfAntUO9bMrOFUt/F4PwwcDTwm\naW667lvAkZJGkJQa/gx8qVpDWe5q2Dsidpf0CEBEvCxpg06FbWbWAPXIuxHxILRZs/h1rW1lSbxv\nSWoiyeZI2hx4u9YTmZk1gqDqwxF5y5J4LyMpJm8h6WzgM8DZ3RqVmVkdFe2R4aqJNyKuljQb2C9d\ndXhEVC0em5kVgXrwIDnNwFsk5QaP72BmPUrRSg1Vk6ikbwPXAUOAbYBrJZ3R3YGZmdWLMk55ydLj\n/TwwMiJeB5B0HvAIcH53BmZmVi89cSD051vt1ytdZ2ZWeMldDY2OYl3tJl5JF5HUdF8GFki6K13e\nH5iZT3hmZl2k4g2E3lGPt+XOhQXAryrWT+++cMzM6q/HlBoi4so8AzEz6w49qtTQQtJ2wHnAzkDf\nlvURsWM3xmVmVjdF6/FmuSd3MnAVyS+OjwM3Ajd0Y0xmZnVVtNvJsiTejSLiLoCIeCoiziRJwGZm\nhSdBc5MyTXnJcjvZ6nSQnKcknQAsAQZ0b1hmZvVTtFJDlsT7f4F+wFdJar0DqfJaCzOzIilY3s00\nSE7Ly9xWkQwCbGbWYwgVbqyGjh6guJV0DN62RMSnuiUiM7N66mGjk12aWxQ9VLNEvz5ZB3izIvi3\nz3630SFYDVY//Vxd2ukxNd6IuDfPQMzMuoNIOklF4u6amZVe0Z5c86DmZlZ6dXq9+1BJUyU9LmmB\npJPT9ZtJukfSovTrplXjyRq4pD5Z9zUzK4rk1T/KNFWxBvhaROwM7AV8WdLOwDeBeyNiB+DedLlD\nWd5Asaekx4BF6fJwSZdUO87MrCjq0eONiOcjYk46vwp4AtgaOBSYku42BRhXNZ4MMV8MHAS8lJ5w\nHjAmw3FmZoXQ8sLLahMwSNKsiml82+1pGDASmAEMjoiWl0O8AAyuFk+Wi2tNEfGXVt3wtRmOMzNr\nOAG9st/VsDwiRnfYntQf+AVwSkS8WpkbIyIktfv8Q4ssPd5nJO0JhKRmSacACzMcZ2ZWCDX0eKu0\no94kSfeaiLglXf2ipK3S7VsBS6u1kyXxngicCrwXeJGkqHxihuPMzBpOSh4ZzjJVaUfAlcATEfHj\nik23A8ek88cAt1WLKctYDUuBI6rtZ2ZWVHV6fuLDJOPVPCZpbrruW8D3gRslHQf8BfhMtYayvIFi\nEm2M2RARbRadzcyKph4PUETEg7Q/Xvq/1tJWlotrv62Y7wt8EnimlpOYmTWKINdBzrPIUmpY5zU/\nkn4OPNhtEZmZ1VOGe3Tz1pmxGt5HhvvUzMyKQrm+Ua26LDXeFbxb420CXibDI3FmZkXQ417vnt4+\nMZzkPWsAb0dE1ZuDzcyKpGiJt8P7eNMk++uIWJtOTrpm1uPUaZCcusnyAMVcSSO7PRIzs26QvN49\n25SXjt651isi1pAMBDFT0lPA30hKJhERu+cUo5lZl/SYl10CDwO7A4fkFIuZWd31tItrAoiIp3KK\nxcysWxSsw9th4t1C0qntbWw1SISZWUGJph50H28z0J/2n002Mys80bN6vM9HxDm5RWJm1h0EvQpW\n5K1a4zUz68l6Wo+3pmHOzMyKqsfcThYRL+cZiJlZdylY3u3U6GRmZj2GyPaIbp6ceM2s3NSDSg1m\nZmWQPLlWrMRbtB64mVndKeNUtR3pZ5KWSppfsW6CpCWS5qbTgdXaceI1s9KTsk0ZTAYOaGP9RREx\nIp1+Xa0RlxrMrOTqN9ZuRNwvaVhX23GP18xKreWuhiwTMEjSrIppfMbTfEXSo2kpYtNqO7vHa2al\nV8PFteURMbrG5i8HziV5N+W5wIXAFzs6wInXzMpNdOtrfSLixXdOJU0C7qh2jEsNZlZqNZYaam9f\n2qpi8ZPA/Pb2beEer5mVXr16vJKuA/YlqQU/C5wF7CtpBEmp4c/Al6q148RrZqVXr0JDRBzZxuor\na23HidfMSk1Ac8GeXHPiNbPSK1jedeI1s7ITKth7HZx4zaz03OM1M8tRcjtZsTKvE6+ZlVv2AXBy\n48RrZqVXtPF4nXjNrNSSgdAbHcW6nHjNrPR8V4OZWc4KVmlw4rXEsy+s4MQJV7Ps5VUIOOaTH+aE\nI8c0OixrR1OTmHr113l+6SsccepPee+QzbnyvC+w2cB+zH3yr5zw3at5a83aRodZGEXr8RZqdDJJ\nwyrfZdSFdo6VdGk6P07SzhXb7pNU63ibpderVxPfO+VTTL/xTO6+6jSuuPl+nnz6+UaHZe044Ygx\nLFz8zmiETPjKoVx+7VRGfepsXnn1DY4+9EMNjK5YWmq8Waa8FCrxdpNxwM5V91rPvWfQQIbvNBSA\nAf36suOw9/D8spUNjsraMmTLTdj/I7tw9W1/eGfdR/fYkdt+9wgA1/1qBgf+y/BGhVc8Ek0Zp7wU\nMfE2S5okaYGkuyVtKGk7SXdKmi3pAUk7AUg6WNIMSY9I+q2kwZUNSdobOAS4IH3753bppsMlPSxp\noaR90n3vT4d2azn2QUnr5U/vX597iUf/+CyjdhnW6FCsDf956qc56+Jf8vbbAcBmA/vxyqo3WLv2\nbQCeW7qCIVsObGSIhVOvtwzXSxET7w7AZRGxC7AS+DQwETgpIkYBpwE/Sfd9ENgrIkYC1wNfr2wo\nIv4A3A6cnr7986l0U6+I2BM4hWQ8TUiGdjsWQNKOQN+ImNc6OEnjW97HtGz5snp95sJ47fXVfP4b\nV3D+qZ9m4/4bNjoca+XfPrIry1esYt6TzzQ6lB4jKTUUq8dbxItriyNibjo/GxgG7A3cVDGYcZ/0\n6zbADekI8BsAizOe45ZW7QPcBHxH0ukk70ua3NaBETGR5BcBo0aNjozn6xHeWrOWY74xicMPGM3B\nY0dUP8By98/D/4kD9tmNj+29C3369GZAv758/7TDGDhgQ5qbm1i79m2GbLkpzy19pdGhFkqxLq0V\ns8e7umJ+LbAZsLLinfUjIuID6fZLgEsjYjeSUd/71niOtaS/fCLideAe4FDgM8A1XfsYPUtEcNK5\n17DjsPfw5aP+tdHhWDvOuex2dj3oOww/9CyO+9ZVPDBzIeO/M4UHZi3k0LEjATjyE//Mb+5/tMGR\nFkzBag1FTLytvQoslnQ4gBIttdeBwJJ0/ph2jl8FDMh4riuAi4GZEbGik/H2SNPnPc0Nv36Y+2ct\nZJ/Pnc8+nzufu6ctaHRYltGES2/jP44aw+xbzmLTgRvx89seanRIheJSQ+ccBVwu6UygN0k9dx4w\ngaQEsQL4HfC+No69Hpgk6avAYR2dJCJmS3oVuKqOsfcIHxqxHStmXtroMKwG0+YsYtqcRQD8ZclL\n7HfsjxocUXEVrdRQqMQbEX8Gdq1YrvxJOqCN/W8Dbmtj/WTSGm1ETGPd28n2rdhvOe/WeJE0hOSv\ngLs7E7+ZFVTBMm9PKDXkQtLngRnAtyPi7UbHY2b1kZRvs/2r2pb0M0lLKx/0krSZpHskLUq/blqt\nHSfeVERcHRFDI+KmRsdiZnWUjsebZcpgMv/41/c3gXsjYgfg3nS5Q068ZlZ69bqpISLuB15utfpQ\nYEo6P4XkadkOFarGa2ZWf0LZ71gYJGlWxfLE9N79jgyOiJaBTV4ABne0Mzjxmtl6oIY7xZZHRKcH\n0YqIkFT1wSqXGsys1LKWGbpw48OL6dOzpF+XVjvAidfMyq97M+/tvPsA1zG0cYtra068ZlZ6dbyd\n7DrgIeD9kp6VdBzwfeBjkhYB+6XLHXKN18xKr15PA0fEke1sqmmAEydeMyu37Pfo5saJ18xKr2jv\nXHPiNbNSE+7xmpnlrmB514nXzNYDBcu8TrxmVnp5DnKehROvmZVesdKuE6+ZrQ8KlnmdeM2s1FoG\nQi8SJ14zKzc/QGFmlr+C5V0nXjMru5oGQs+FE6+ZlV7B8q4Tr5mVWxcHOe8WTrxmVn4Fy7xOvGZW\ner6dzMwsZ67xmpnlSdDkxGtmlrdiZV4nXjMrtXoOhC7pz8AqYC2wJiJGd6YdJ14zK70693fHRMTy\nrjTgxGtmpVe0i2tNjQ7AzKy7Sco0ZRDA3ZJmSxrf2Xjc4zWz0quhwztI0qyK5YkRMbFi+SMRsUTS\nlsA9kp6MiPtrjceJ18xKTbUNC7m8owtmEbEk/bpU0q3AnkDNidelBjMrPWX812EbUj9JA1rmgf2B\n+Z2Jxz1eMyu/+lxcGwzcmtaCewHXRsSdnWnIidfMSq8eeTcingaG16EpJ14zKzv59e5mZnmq55Nr\n9eKLa2ZmOXOP18xKr2g9XideMys9D4RuZpan2h6gyIUTr5mVWhEvrjnxmlnpudRgZpYz93jNzHJW\nsLzrxGtm64GCZV4nXjMrNUHhHhlWRDQ6hh5L0jLgL42OoxsMArr0TinLXVm/Z9tGxBZdaUDSnST/\nP1ksj4gDunK+LJx47R9ImtXZt6daY/h71rN4rAYzs5w58ZqZ5cyJ19oysfouVjD+nvUgrvGameXM\nPV4zs5w58ZqZ5cyJdz0g6VhJl9aprcmSDkvnT5G0UcW21+pxjvWJpGGSOvWK8FbtvPM9ljRO0s4V\n2+6T5FvNCsSJ17riFGCjqntZ3sYBO1fdyxrGibcHat1LknSapAlpz+YHkh6WtFDSPhWHDZF0p6RF\nkn5Ycez+kh6SNEfSTZL6p+u/K2mmpPmSJkrrPnMp6avAEGCqpKkV68+TNE/SdEmDJQ2QtFhS73T7\nxpXLBkCzpEmSFki6W9KGkrZLv1+zJT0gaScASQdLmiHpEUm/lTS4siFJewOHABdImitpu3TT4a1/\nLiTdL2lExbEPSqrL68utY0685dMrIvYk6Y2eVbF+BPBZYDfgs5KGShoEnAnsFxG7A7OAU9P9L42I\nPSJiV2BD4KDKk0TExcBzwJiIGJOu7gdMj4jhwP3A8RGxCrgP+ES6zxHALRHxVj0/dA+3A3BZROwC\nrAQ+TXJ72EkRMQo4DfhJuu+DwF4RMRK4Hvh6ZUMR8QfgduD0iBgREU+lm9r6ubgSOBZA0o5A34iY\n1z0f0Sp5kJzyuSX9OhsYVrH+3oh4BUDS48C2wCYkf5JOSzu0GwAPpfuPkfR1klLCZsAC4H+rnPvv\nwB0V5/9YOn8FSYL4JfAF4PhOfK4yWxwRc9P5lu/b3sBNFX9o9Em/bgPcIGkrku/X4oznaOvn4ibg\nO5JOB74ITO5c+FYrJ96eaQ3r/rXSt2J+dfp1Let+f1dXzLdsE3BPRBxZ2bikviQ9rNER8YykCa3O\n0Z634t0bw985f0RMS8sj+wLNEdHli0kl0/p7MxhYGREj2tj3EuDHEXF7+v85ocZzVH5fXpd0D3Ao\n8BlgVO2hW2e41NAzvQhsKWlzSX1oVQaowXTgw5K2B5DUr+VPznT78rTme1g7x68CBmQ819XAtcBV\nnYx1ffIqsFjS4QBKtNReBwJL0vlj2jm+lu/LFcDFwMyIWNHJeK1GTrw9UFofPQd4GLgHeLKT7Swj\nqfFdJ+lRkjLDThGxEpgEzAfuAma208RE4M7Ki2sduAbYFLiuM7Guh44CjpM0j6TMc2i6fgJJCWI2\n7Q8DeT1wenoBbrt29gEgImaTJHr/QsyRHxm2XKT3/h4aEUc3OhZ7l6QhJBc/d4qItxscznrDNV7r\ndpIuAT4OHNjoWOxdkj4PnAec6qSbL/d4zcxy5hqvmVnOnHjNzHLmxGtmljMnXus2ktam4wXMT8eB\n6PSAOpL2lXRHOn+IpG92sO8mkv6jE+eYIOm0rOtb7fPOqG0Zz1WXUcmsZ3Lite70RjpewK4kjxOf\nULkxfTCg5p/BiLg9Ir7fwS6bADUnXrO8OPFaXh4Atk97en+UdDXJAxpDOxgh7QBJT0qaA3yqpSGt\nO/bsYEm3piOizUtH5/o+sF3a274g3e/0dLS1RyWdXdHWt9MRux4E3l/tQ0g6Pm1nnqRftOrF7ydp\nVtreQen+zZIuqDj3l7r6H2k9nxOvdTtJvUju430sXbUD8JN0NK6/0cYIael4EZOAg0nGEHhPO81f\nDPw+HRFtd5KnvL4JPJX2tk+XtH96zj1JRmkbJemjkkaRjJY2guQe4z0yfJxb0lHbhgNPAMdVbBuW\nnuMTwE/Tz3Ac8EpE7JG2f7yk92U4j5WYH6Cw7rShpJZRtx4gGYZwCPCXiJiert+LtkdI24lk1K5F\nAJL+BxjfxjnGAp8HiIi1wCuSNm21z/7p9Ei63J8kEQ8Abo2I19Nz3J7hM+0q6Xsk5Yz+JI9Ut7gx\nfRBhkaSn08+wP/DBivrvwPTcCzOcy0rKide60xutR9hKk+vfKlfR9ghpbY3M1VkCzo+I/251jlM6\n0dZkYFxEzJN0LLBvxbbWTyNFeu6TIqIyQSNpWCfObSXhUoM1WnsjpD0JDKsY5OXIdo6/FzgxPbZZ\n0kD+cXSuu4AvVtSOt5a0Jclg7eOUvPFhAElZo5oBwPNK3qBxVKtth0tqSmP+J+CP6blP1Ltv4NhR\nUr8M57ESc4/XGioilqU9x+vSIS4BzoyIhZLGA7+S9DpJqaKtoQ5PBiZKOo5krNkTI+IhSdPS27V+\nk9Z5PwA8lPa4XwP+PSLmSLoBmAcspf1R2Cp9B5gBLEu/Vsb0V5IR4zYGToiINyVdQVL7naPk5MtI\n3olm6zGP1WBmljOXGszMcubEa2aWMydeM7OcOfGameXMidfMLGdOvGZmOXPiNTPL2f8HV7mZkTHu\n7gAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.67      0.25      0.36        16\n",
            "     healthy       0.77      0.95      0.85        42\n",
            "\n",
            "    accuracy                           0.76        58\n",
            "   macro avg       0.72      0.60      0.61        58\n",
            "weighted avg       0.74      0.76      0.72        58\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.67      0.25      0.36        16\n",
            "     healthy       0.77      0.95      0.85        42\n",
            "\n",
            "    accuracy                           0.76        58\n",
            "   macro avg       0.72      0.60      0.61        58\n",
            "weighted avg       0.74      0.76      0.72        58\n",
            "\n",
            "F1 micro score for the default dummy classifier is:  0.7586206896551724\n",
            "F1 micro score for the default kNN classifier is:  0.7586206896551724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rdSr2lzEhBJ",
        "colab_type": "text"
      },
      "source": [
        "Τυπώνουμε δύο bar plots, που δείχνουν τις τιμές των δύο classifiers μας (default_dummy, default_kNN) για τα averaged metrics f1 micro και f1 macro:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxoVbCF4E4Kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "92cdc26e-e1cc-44f5-ac4a-049d1c1a5986"
      },
      "source": [
        "n_groups = 2\n",
        "f1_micro = (f1_score(test_labels, preds1, average = 'micro'), f1_score(test_labels, preds2, average = 'micro'))\n",
        "f1_macro = (f1_score(test_labels, preds1, average = 'macro'), f1_score(test_labels, preds2, average = 'macro'))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(n_groups)\n",
        "bar_width = 0.35\n",
        "opacity = 0.8\n",
        "\n",
        "rects1 = plt.bar(index, f1_micro, bar_width,\n",
        "alpha=opacity,\n",
        "color='b',\n",
        "label='F1 micro')\n",
        "\n",
        "rects2 = plt.bar(index + bar_width, f1_macro, bar_width,\n",
        "alpha=opacity,\n",
        "color='g',\n",
        "label='F1 macro')\n",
        "\n",
        "plt.xlabel('Classifier')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Scores by classifier')\n",
        "plt.xticks(index + bar_width, ('default Dummy', 'default kNN'))\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8WwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTEz5VvLd20r",
        "colab_type": "text"
      },
      "source": [
        "//Sxoliasmos gia ta barplots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ndY0gk7pR0",
        "colab_type": "text"
      },
      "source": [
        "#Δ\n",
        "---\n",
        "\n",
        "Για το section αυτό,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V238HWfk6byA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}