{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 1 Small - NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeaidinis/NTUA/blob/master/NN/Lab%201/Lab_1_Small_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cBcurO-uUmo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "#Lab 1: Επιβλεπόμενη Μάθηση - Ταξινόμηση - Μικρό Dataset (S11 - Quality Assessment of Digital Colposcopies)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKqs0XolswuQ",
        "colab_type": "text"
      },
      "source": [
        "#Section A\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Αϊδίνης Γιώργος 03116031\n",
        "\n",
        "Κολιός Παναγιώτης 03116100\n",
        "\n",
        "---\n",
        "\n",
        "Ομάδα M.B.8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-LPsGgQufbH",
        "colab_type": "text"
      },
      "source": [
        "# Section B\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypUIaKPuxyCI",
        "colab_type": "text"
      },
      "source": [
        "Αρχικά ενημερώνουμε τις βιβλιοθήκες που θα χρησιμοποιήσουμε.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZTpGgaqwT3G",
        "colab_type": "code",
        "outputId": "6b860f90-c728-4738-a638-b8e57dd6a266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "!pip install --upgrade pip #upgrade pip package installer\n",
        "!pip install scikit-learn --upgrade #upgrade scikit-learn package\n",
        "!pip install numpy --upgrade #upgrade numpy package\n",
        "!pip install pandas --upgrade #--upgrade #upgrade pandas package\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsvljyaGuws2",
        "colab_type": "text"
      },
      "source": [
        "Κατεβάζουμε το αρχείο που περιέχει το dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPGXEYNcqtmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL_pZToQuTs4",
        "colab_type": "code",
        "outputId": "0dd3f25e-e3d7-4e37-a4c6-01532b0a43eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "download(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00384/Quality%20Assessment%20-%20Digital%20Colposcopy.zip\",\"QADC.zip\")\n",
        "print(\"All the files are downloaded\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrdHuDyTvAD7",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι το αρχείο όντως κατέβηκε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1fY1PPQ7L9d",
        "colab_type": "code",
        "outputId": "f2261446-5647-457b-b914-7c053911bd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all.csv  QADC  QADC.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egTI09KgvG47",
        "colab_type": "text"
      },
      "source": [
        "Επειδή είναι compressed, πρεπει να το κάνουμε decompress και να δουμε ότι όντως έγινε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFe5qqIq6jSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"QADC.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AZDcWwK7IKb",
        "colab_type": "code",
        "outputId": "32f3defa-4a2d-49da-f72a-db3f14dd8c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " all.csv   QADC.zip\t\t\t\t      sample_data\n",
            " QADC\t  'Quality Assessment - Digital Colposcopy'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDKrxf0wzLvH",
        "colab_type": "text"
      },
      "source": [
        "Μετονομάζουμε τον φάκελο για να έχει μικρότερο όνομα:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxlh8yEDy47d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv Quality\\ Assessment\\ -\\ Digital\\ Colposcopy QADC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PDIZ5HBzx9i",
        "colab_type": "text"
      },
      "source": [
        "Ανοίγοντας τον φάκελο, είδαμε ότι έχουμε τρια αρχεία, τα οποία έχουν headers, και πρέπει 1) να ενοποιηθούν σε ένα αρχείο και 2) να αφαιρεθούν οι επιπρόσθετοι headers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhtKAnFCyl0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"QADC/hinselmann.csv\",'r') as f:\n",
        "    with open(\"QADC/hinselmann1.csv\",'w') as f1:\n",
        "        next(f) # skip header line\n",
        "        for line in f:\n",
        "            f1.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcY__jBiymmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"QADC/schiller.csv\",'r') as f:\n",
        "    with open(\"QADC/schiller1.csv\",'w') as f1:\n",
        "        next(f) # skip header line\n",
        "        for line in f:\n",
        "            f1.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hTiHGpp0JR2",
        "colab_type": "text"
      },
      "source": [
        "Φτιάχνουμε ένα μεγάλο αρχείο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VimXlyS7fGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat QADC/green.csv QADC/hinselmann1.csv QADC/schiller1.csv > all.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPLZO8N605LP",
        "colab_type": "text"
      },
      "source": [
        "Για να δούμε λοιπόν το dataset μας ολόκληρο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVGBnlk67zwC",
        "colab_type": "code",
        "outputId": "28270032-743d-44a4-d88a-bf05d89bfe90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"all.csv\")\n",
        "df"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cervix_area</th>\n",
              "      <th>os_area</th>\n",
              "      <th>walls_area</th>\n",
              "      <th>speculum_area</th>\n",
              "      <th>artifacts_area</th>\n",
              "      <th>cervix_artifacts_area</th>\n",
              "      <th>os_artifacts_area</th>\n",
              "      <th>walls_artifacts_area</th>\n",
              "      <th>speculum_artifacts_area</th>\n",
              "      <th>cervix_specularities_area</th>\n",
              "      <th>os_specularities_area</th>\n",
              "      <th>walls_specularities_area</th>\n",
              "      <th>speculum_specularities_area</th>\n",
              "      <th>specularities_area</th>\n",
              "      <th>area_h_max_diff</th>\n",
              "      <th>rgb_cervix_r_mean</th>\n",
              "      <th>rgb_cervix_r_std</th>\n",
              "      <th>rgb_cervix_r_mean_minus_std</th>\n",
              "      <th>rgb_cervix_r_mean_plus_std</th>\n",
              "      <th>rgb_cervix_g_mean</th>\n",
              "      <th>rgb_cervix_g_std</th>\n",
              "      <th>rgb_cervix_g_mean_minus_std</th>\n",
              "      <th>rgb_cervix_g_mean_plus_std</th>\n",
              "      <th>rgb_cervix_b_mean</th>\n",
              "      <th>rgb_cervix_b_std</th>\n",
              "      <th>rgb_cervix_b_mean_minus_std</th>\n",
              "      <th>rgb_cervix_b_mean_plus_std</th>\n",
              "      <th>rgb_total_r_mean</th>\n",
              "      <th>rgb_total_r_std</th>\n",
              "      <th>rgb_total_r_mean_minus_std</th>\n",
              "      <th>rgb_total_r_mean_plus_std</th>\n",
              "      <th>rgb_total_g_mean</th>\n",
              "      <th>rgb_total_g_std</th>\n",
              "      <th>rgb_total_g_mean_minus_std</th>\n",
              "      <th>rgb_total_g_mean_plus_std</th>\n",
              "      <th>rgb_total_b_mean</th>\n",
              "      <th>rgb_total_b_std</th>\n",
              "      <th>rgb_total_b_mean_minus_std</th>\n",
              "      <th>rgb_total_b_mean_plus_std</th>\n",
              "      <th>hsv_cervix_h_mean</th>\n",
              "      <th>hsv_cervix_h_std</th>\n",
              "      <th>hsv_cervix_s_mean</th>\n",
              "      <th>hsv_cervix_s_std</th>\n",
              "      <th>hsv_cervix_v_mean</th>\n",
              "      <th>hsv_cervix_v_std</th>\n",
              "      <th>hsv_total_h_mean</th>\n",
              "      <th>hsv_total_h_std</th>\n",
              "      <th>hsv_total_s_mean</th>\n",
              "      <th>hsv_total_s_std</th>\n",
              "      <th>hsv_total_v_mean</th>\n",
              "      <th>hsv_total_v_std</th>\n",
              "      <th>fit_cervix_hull_rate</th>\n",
              "      <th>fit_cervix_hull_total</th>\n",
              "      <th>fit_cervix_bbox_rate</th>\n",
              "      <th>fit_cervix_bbox_total</th>\n",
              "      <th>fit_circle_rate</th>\n",
              "      <th>fit_circle_total</th>\n",
              "      <th>fit_ellipse_rate</th>\n",
              "      <th>fit_ellipse_total</th>\n",
              "      <th>fit_ellipse_goodness</th>\n",
              "      <th>dist_to_center_cervix</th>\n",
              "      <th>dist_to_center_os</th>\n",
              "      <th>experts::0</th>\n",
              "      <th>experts::1</th>\n",
              "      <th>experts::2</th>\n",
              "      <th>experts::3</th>\n",
              "      <th>experts::4</th>\n",
              "      <th>experts::5</th>\n",
              "      <th>consensus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.344647</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>0.047522</td>\n",
              "      <td>0.288216</td>\n",
              "      <td>0.178585</td>\n",
              "      <td>0.016564</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.263560</td>\n",
              "      <td>37.594458</td>\n",
              "      <td>15.785021</td>\n",
              "      <td>21.809437</td>\n",
              "      <td>53.379479</td>\n",
              "      <td>109.918445</td>\n",
              "      <td>38.735421</td>\n",
              "      <td>71.183024</td>\n",
              "      <td>148.653865</td>\n",
              "      <td>55.029618</td>\n",
              "      <td>22.160330</td>\n",
              "      <td>32.869287</td>\n",
              "      <td>77.189948</td>\n",
              "      <td>38.561367</td>\n",
              "      <td>38.119059</td>\n",
              "      <td>0.442308</td>\n",
              "      <td>76.680426</td>\n",
              "      <td>95.109755</td>\n",
              "      <td>51.565052</td>\n",
              "      <td>43.544702</td>\n",
              "      <td>146.674807</td>\n",
              "      <td>48.808474</td>\n",
              "      <td>40.765228</td>\n",
              "      <td>8.043247</td>\n",
              "      <td>89.573702</td>\n",
              "      <td>5.014628</td>\n",
              "      <td>2.991944</td>\n",
              "      <td>167.952780</td>\n",
              "      <td>25.813163</td>\n",
              "      <td>109.919447</td>\n",
              "      <td>38.733741</td>\n",
              "      <td>5.090801</td>\n",
              "      <td>2.936650</td>\n",
              "      <td>159.486916</td>\n",
              "      <td>38.437294</td>\n",
              "      <td>95.123889</td>\n",
              "      <td>51.583029</td>\n",
              "      <td>0.923067</td>\n",
              "      <td>0.373371</td>\n",
              "      <td>0.844454</td>\n",
              "      <td>0.408130</td>\n",
              "      <td>0.603399</td>\n",
              "      <td>0.571175</td>\n",
              "      <td>0.962995</td>\n",
              "      <td>0.357890</td>\n",
              "      <td>85.474311</td>\n",
              "      <td>0.265933</td>\n",
              "      <td>0.346294</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.165329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048236</td>\n",
              "      <td>0.504736</td>\n",
              "      <td>0.502783</td>\n",
              "      <td>0.007012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097405</td>\n",
              "      <td>0.973837</td>\n",
              "      <td>0.004055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054999</td>\n",
              "      <td>0.028431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.505882</td>\n",
              "      <td>24.361877</td>\n",
              "      <td>35.144005</td>\n",
              "      <td>83.867760</td>\n",
              "      <td>122.366075</td>\n",
              "      <td>44.742407</td>\n",
              "      <td>77.623669</td>\n",
              "      <td>167.108482</td>\n",
              "      <td>78.058434</td>\n",
              "      <td>30.818729</td>\n",
              "      <td>47.239706</td>\n",
              "      <td>108.877163</td>\n",
              "      <td>54.932467</td>\n",
              "      <td>39.447415</td>\n",
              "      <td>15.485052</td>\n",
              "      <td>94.379883</td>\n",
              "      <td>101.680459</td>\n",
              "      <td>46.028852</td>\n",
              "      <td>55.651607</td>\n",
              "      <td>147.709311</td>\n",
              "      <td>63.218931</td>\n",
              "      <td>43.925912</td>\n",
              "      <td>19.293019</td>\n",
              "      <td>107.144843</td>\n",
              "      <td>4.944382</td>\n",
              "      <td>2.965108</td>\n",
              "      <td>130.260492</td>\n",
              "      <td>24.143867</td>\n",
              "      <td>122.366647</td>\n",
              "      <td>44.743932</td>\n",
              "      <td>5.080063</td>\n",
              "      <td>2.894163</td>\n",
              "      <td>128.251978</td>\n",
              "      <td>33.000693</td>\n",
              "      <td>101.725519</td>\n",
              "      <td>46.093510</td>\n",
              "      <td>0.850861</td>\n",
              "      <td>0.194308</td>\n",
              "      <td>0.646645</td>\n",
              "      <td>0.255673</td>\n",
              "      <td>0.497315</td>\n",
              "      <td>0.332444</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.184803</td>\n",
              "      <td>124.794129</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.283059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.457010</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.242888</td>\n",
              "      <td>0.212859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083055</td>\n",
              "      <td>0.018591</td>\n",
              "      <td>0.269798</td>\n",
              "      <td>39.353851</td>\n",
              "      <td>19.417332</td>\n",
              "      <td>19.936519</td>\n",
              "      <td>58.771183</td>\n",
              "      <td>109.543386</td>\n",
              "      <td>49.753490</td>\n",
              "      <td>59.789896</td>\n",
              "      <td>159.296876</td>\n",
              "      <td>54.642888</td>\n",
              "      <td>27.781965</td>\n",
              "      <td>26.860923</td>\n",
              "      <td>82.424853</td>\n",
              "      <td>41.242230</td>\n",
              "      <td>34.196356</td>\n",
              "      <td>7.045875</td>\n",
              "      <td>75.438586</td>\n",
              "      <td>109.592342</td>\n",
              "      <td>57.576640</td>\n",
              "      <td>52.015702</td>\n",
              "      <td>167.168982</td>\n",
              "      <td>53.470241</td>\n",
              "      <td>38.344391</td>\n",
              "      <td>15.125850</td>\n",
              "      <td>91.814632</td>\n",
              "      <td>5.049946</td>\n",
              "      <td>2.983966</td>\n",
              "      <td>163.576979</td>\n",
              "      <td>24.973042</td>\n",
              "      <td>109.544201</td>\n",
              "      <td>49.753659</td>\n",
              "      <td>5.078936</td>\n",
              "      <td>2.968023</td>\n",
              "      <td>162.268659</td>\n",
              "      <td>33.590792</td>\n",
              "      <td>109.597127</td>\n",
              "      <td>57.584515</td>\n",
              "      <td>0.918514</td>\n",
              "      <td>0.497554</td>\n",
              "      <td>0.747443</td>\n",
              "      <td>0.611432</td>\n",
              "      <td>0.633925</td>\n",
              "      <td>0.720923</td>\n",
              "      <td>0.920287</td>\n",
              "      <td>0.496596</td>\n",
              "      <td>94.948697</td>\n",
              "      <td>0.518740</td>\n",
              "      <td>0.419375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.513244</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.213781</td>\n",
              "      <td>0.251819</td>\n",
              "      <td>0.079795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.107022</td>\n",
              "      <td>46.322391</td>\n",
              "      <td>17.711957</td>\n",
              "      <td>28.610434</td>\n",
              "      <td>64.034349</td>\n",
              "      <td>116.075087</td>\n",
              "      <td>43.593124</td>\n",
              "      <td>72.481962</td>\n",
              "      <td>159.668211</td>\n",
              "      <td>51.430923</td>\n",
              "      <td>18.573016</td>\n",
              "      <td>32.857907</td>\n",
              "      <td>70.003940</td>\n",
              "      <td>40.365565</td>\n",
              "      <td>17.259087</td>\n",
              "      <td>23.106478</td>\n",
              "      <td>57.624652</td>\n",
              "      <td>102.641859</td>\n",
              "      <td>38.606995</td>\n",
              "      <td>64.034863</td>\n",
              "      <td>141.248854</td>\n",
              "      <td>50.805205</td>\n",
              "      <td>18.072101</td>\n",
              "      <td>32.733104</td>\n",
              "      <td>68.877306</td>\n",
              "      <td>5.177654</td>\n",
              "      <td>2.969214</td>\n",
              "      <td>156.242754</td>\n",
              "      <td>25.499379</td>\n",
              "      <td>116.081770</td>\n",
              "      <td>43.582671</td>\n",
              "      <td>5.071879</td>\n",
              "      <td>2.909002</td>\n",
              "      <td>158.343946</td>\n",
              "      <td>28.273928</td>\n",
              "      <td>102.648278</td>\n",
              "      <td>38.598300</td>\n",
              "      <td>0.951710</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.855409</td>\n",
              "      <td>0.599998</td>\n",
              "      <td>0.618140</td>\n",
              "      <td>0.830304</td>\n",
              "      <td>0.964611</td>\n",
              "      <td>0.532073</td>\n",
              "      <td>74.221670</td>\n",
              "      <td>0.347202</td>\n",
              "      <td>0.361672</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.390319</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.272884</td>\n",
              "      <td>0.373487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.071442</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>0.442831</td>\n",
              "      <td>37.552979</td>\n",
              "      <td>14.454975</td>\n",
              "      <td>23.098004</td>\n",
              "      <td>52.007953</td>\n",
              "      <td>101.044906</td>\n",
              "      <td>37.171973</td>\n",
              "      <td>63.872932</td>\n",
              "      <td>138.216879</td>\n",
              "      <td>53.971671</td>\n",
              "      <td>17.591669</td>\n",
              "      <td>36.380003</td>\n",
              "      <td>71.563340</td>\n",
              "      <td>40.717229</td>\n",
              "      <td>38.810699</td>\n",
              "      <td>1.906530</td>\n",
              "      <td>79.527927</td>\n",
              "      <td>97.446185</td>\n",
              "      <td>48.106253</td>\n",
              "      <td>49.339932</td>\n",
              "      <td>145.552438</td>\n",
              "      <td>51.733004</td>\n",
              "      <td>39.866956</td>\n",
              "      <td>11.866047</td>\n",
              "      <td>91.599960</td>\n",
              "      <td>4.978534</td>\n",
              "      <td>2.964685</td>\n",
              "      <td>158.160578</td>\n",
              "      <td>30.308891</td>\n",
              "      <td>101.050711</td>\n",
              "      <td>37.163383</td>\n",
              "      <td>5.051587</td>\n",
              "      <td>2.923540</td>\n",
              "      <td>157.131407</td>\n",
              "      <td>39.439642</td>\n",
              "      <td>97.457304</td>\n",
              "      <td>48.125747</td>\n",
              "      <td>0.955996</td>\n",
              "      <td>0.408286</td>\n",
              "      <td>0.882990</td>\n",
              "      <td>0.442043</td>\n",
              "      <td>0.623938</td>\n",
              "      <td>0.625574</td>\n",
              "      <td>0.957604</td>\n",
              "      <td>0.407600</td>\n",
              "      <td>61.546536</td>\n",
              "      <td>0.437852</td>\n",
              "      <td>0.673196</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.610160</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.298345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028185</td>\n",
              "      <td>0.046193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015983</td>\n",
              "      <td>0.104929</td>\n",
              "      <td>73.465862</td>\n",
              "      <td>33.856824</td>\n",
              "      <td>39.609038</td>\n",
              "      <td>107.322686</td>\n",
              "      <td>44.657984</td>\n",
              "      <td>35.514453</td>\n",
              "      <td>9.143531</td>\n",
              "      <td>80.172437</td>\n",
              "      <td>97.007389</td>\n",
              "      <td>67.135578</td>\n",
              "      <td>29.871812</td>\n",
              "      <td>164.142967</td>\n",
              "      <td>80.241955</td>\n",
              "      <td>32.698142</td>\n",
              "      <td>47.543813</td>\n",
              "      <td>112.940096</td>\n",
              "      <td>49.969255</td>\n",
              "      <td>32.728615</td>\n",
              "      <td>17.240641</td>\n",
              "      <td>82.697870</td>\n",
              "      <td>103.504549</td>\n",
              "      <td>66.143863</td>\n",
              "      <td>37.360686</td>\n",
              "      <td>169.648412</td>\n",
              "      <td>4.376708</td>\n",
              "      <td>2.426611</td>\n",
              "      <td>149.832364</td>\n",
              "      <td>34.523784</td>\n",
              "      <td>107.238951</td>\n",
              "      <td>58.226891</td>\n",
              "      <td>4.243756</td>\n",
              "      <td>2.459024</td>\n",
              "      <td>141.193904</td>\n",
              "      <td>33.987154</td>\n",
              "      <td>112.817594</td>\n",
              "      <td>57.899252</td>\n",
              "      <td>0.951236</td>\n",
              "      <td>0.641439</td>\n",
              "      <td>0.681541</td>\n",
              "      <td>0.895266</td>\n",
              "      <td>0.559964</td>\n",
              "      <td>1.089641</td>\n",
              "      <td>0.916873</td>\n",
              "      <td>0.665480</td>\n",
              "      <td>141.328331</td>\n",
              "      <td>0.356888</td>\n",
              "      <td>0.353911</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.439891</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>0.031853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.458246</td>\n",
              "      <td>36.279936</td>\n",
              "      <td>23.325050</td>\n",
              "      <td>12.954885</td>\n",
              "      <td>59.604986</td>\n",
              "      <td>31.549222</td>\n",
              "      <td>23.099706</td>\n",
              "      <td>8.449515</td>\n",
              "      <td>54.648928</td>\n",
              "      <td>39.162020</td>\n",
              "      <td>33.679494</td>\n",
              "      <td>5.482527</td>\n",
              "      <td>72.841514</td>\n",
              "      <td>37.256992</td>\n",
              "      <td>27.997780</td>\n",
              "      <td>9.259212</td>\n",
              "      <td>65.254772</td>\n",
              "      <td>32.188663</td>\n",
              "      <td>27.246836</td>\n",
              "      <td>4.941828</td>\n",
              "      <td>59.435499</td>\n",
              "      <td>38.946898</td>\n",
              "      <td>37.576950</td>\n",
              "      <td>1.369948</td>\n",
              "      <td>76.523849</td>\n",
              "      <td>4.047217</td>\n",
              "      <td>2.068656</td>\n",
              "      <td>85.995172</td>\n",
              "      <td>37.051364</td>\n",
              "      <td>44.437573</td>\n",
              "      <td>31.678540</td>\n",
              "      <td>5.053266</td>\n",
              "      <td>1.795977</td>\n",
              "      <td>86.474830</td>\n",
              "      <td>39.921044</td>\n",
              "      <td>45.112433</td>\n",
              "      <td>35.381825</td>\n",
              "      <td>0.988825</td>\n",
              "      <td>0.444862</td>\n",
              "      <td>0.810836</td>\n",
              "      <td>0.542516</td>\n",
              "      <td>0.667897</td>\n",
              "      <td>0.658622</td>\n",
              "      <td>1.001402</td>\n",
              "      <td>0.439276</td>\n",
              "      <td>59.687772</td>\n",
              "      <td>0.529688</td>\n",
              "      <td>0.417762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.696616</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.169087</td>\n",
              "      <td>0.146312</td>\n",
              "      <td>0.017217</td>\n",
              "      <td>0.024715</td>\n",
              "      <td>0.078889</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>0.038447</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.342323</td>\n",
              "      <td>0.060042</td>\n",
              "      <td>0.301857</td>\n",
              "      <td>74.766472</td>\n",
              "      <td>33.667006</td>\n",
              "      <td>41.099466</td>\n",
              "      <td>108.433478</td>\n",
              "      <td>44.113166</td>\n",
              "      <td>29.555688</td>\n",
              "      <td>14.557478</td>\n",
              "      <td>73.668854</td>\n",
              "      <td>68.044433</td>\n",
              "      <td>42.402254</td>\n",
              "      <td>25.642180</td>\n",
              "      <td>110.446687</td>\n",
              "      <td>95.119483</td>\n",
              "      <td>56.497295</td>\n",
              "      <td>38.622189</td>\n",
              "      <td>151.616778</td>\n",
              "      <td>62.400161</td>\n",
              "      <td>54.006420</td>\n",
              "      <td>8.393741</td>\n",
              "      <td>116.406581</td>\n",
              "      <td>104.351296</td>\n",
              "      <td>74.683000</td>\n",
              "      <td>29.668296</td>\n",
              "      <td>179.034296</td>\n",
              "      <td>3.439162</td>\n",
              "      <td>2.374293</td>\n",
              "      <td>116.377066</td>\n",
              "      <td>29.226454</td>\n",
              "      <td>81.122596</td>\n",
              "      <td>38.389404</td>\n",
              "      <td>3.774469</td>\n",
              "      <td>2.431975</td>\n",
              "      <td>117.490035</td>\n",
              "      <td>41.194225</td>\n",
              "      <td>114.484831</td>\n",
              "      <td>69.298856</td>\n",
              "      <td>0.977561</td>\n",
              "      <td>0.712606</td>\n",
              "      <td>0.830197</td>\n",
              "      <td>0.839097</td>\n",
              "      <td>0.534820</td>\n",
              "      <td>1.302525</td>\n",
              "      <td>1.020938</td>\n",
              "      <td>0.682329</td>\n",
              "      <td>196.767870</td>\n",
              "      <td>0.453209</td>\n",
              "      <td>0.590185</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026749</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.453958</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>0.220804</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cervix_area   os_area  walls_area  ...  experts::4  experts::5  consensus\n",
              "0       0.344647  0.003080    0.047522  ...         1.0         1.0        1.0\n",
              "1       0.165329  0.000000    0.048236  ...         0.0         0.0        0.0\n",
              "2       0.457010  0.001681    0.242888  ...         0.0         0.0        0.0\n",
              "3       0.513244  0.005711    0.213781  ...         1.0         1.0        1.0\n",
              "4       0.390319  0.009454    0.272884  ...         1.0         1.0        1.0\n",
              "..           ...       ...         ...  ...         ...         ...        ...\n",
              "282     0.610160  0.002726    0.298345  ...         0.0         0.0        0.0\n",
              "283     0.439891  0.006005    0.000000  ...         0.0         0.0        1.0\n",
              "284     0.696616  0.006693    0.169087  ...         1.0         0.0        1.0\n",
              "285     1.000000  0.000000    0.000000  ...         0.0         0.0        0.0\n",
              "286     1.000000  0.007517    0.000000  ...         0.0         0.0        0.0\n",
              "\n",
              "[287 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_deFZG7YvKWh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Το dataset περιγράφει μετρήσεις κολποσκοπήσεων και την κατάσταση (ετικέτα) των αντίστοιχων κόλπων, όπως προκύπτει από τις εκτιμήσεις καθενός από 6 ειδικούς και την εκτίμηση της πλειοψηφίας. Οι μετρήσεις και οι εκτιμήσεις είναι διαφορετικές για τις διαφορετικές μεθόδους με τις οποίες πραγματοποιούνται οι κολποσκοπήσεις: Hinselmann, Green, Schiller.\n",
        "\n",
        "\n",
        "\n",
        "2.    Έχουμε συνολικά 287 δείγματα, 92 από τη μέθοδο Schiller, 98 από τη μέθοδο Green και 97 από τη μέθοδο Hinselmann. Σε κάθε δείγμα περιέχονται μετρήσεις για 62 χαρακτηριστικά-παρατηρήσεις από τα οποία προέκυψαν οι εκτιμήσεις. Όλα τα χαρακτηριστικά είναι διατεταγμένα.Οι μετρήσεις είναι αριθμητικά δεδομένα και αφορούν τιμές όπως τις επιφάνειες περιοχών του κόπλου.\n",
        "\n",
        "3.    Υπάρχουν επικεφαλίδες στην πρώτη γραμμή πάνω από τα χαρακτηριστικά και τις ετικέτες, οι οποίες θα πρέπει να αφαιρεθούν. Δεν υπάρχει στήλη για την αρίθμηση των γραμμών.\n",
        "\n",
        "4.    Η τιμή της κατάστασης μπορεί να πάρει δύο τιμές, 0 για κακή και 1 για καλή. Όπως υποδεικνύεται από τις FAQ το πρόβλημα θα αναλυθεί ως binary classification λαμβάνοντας υπόψην μόνο τις εκτιμήσεις της πλειοψηφίας. Έτσι οι στήλες των ετικετών που αφορούν μεμονωμένα τον κάθε ειδικό αφαιρούνται και μένει μόνο η τελευταία στήλη με τις εκτιμήσεις της πλειοψηφίας. Η στήλη αυτή είναι η τελευταία (θέση 69 στον αρχικό πίνακα). Παρατηρούμε οτι όλες οι ισοψηφίες (3-3) επιλύονται θεωρώντας την κατάσταση ως καλή, γεγονός που ίσως μας αναγκάσει να αυξήσουμε αργότερα το πλήθος των δειγμάτων με ετικέτα 0 ή να διαγράψουμε δείγματα με ετικέτα 1.\n",
        "\n",
        "    \n",
        "5.    Συνενώνουμε τα αρχεία που αφορούν τις 3 διαφορετικές μεθόδους. Μετά τη συνένωση οι επικεφαλίδες προστέθηκαν ως γραμμές, με αποτέλεσμα να έχουμε 3 φορές την ίδια γραμμή με τις επικεφαλίδες, τις οποίες και αφαιρούμε.\n",
        "\n",
        "6.    Δεν υπάρχουν απουσιάζουσες τιμές.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0110eLi0kT7",
        "colab_type": "text"
      },
      "source": [
        "Ας τυπώσουμε τον αριθμό, τα ονόματα και τους τύπους του κάθε attribute, για να τα εξετάσουμε καλύτερα:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ1Qrlcl0h_t",
        "colab_type": "code",
        "outputId": "00c4ed32-d8e6-456d-dca5-f444a3cd1938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"We have \",len(df.columns), \" attributes.\")\n",
        "for i in range(0, len(df.columns)):\n",
        "    print('{:<10}{:<40}{:<10}{:<20}'.format(str(i+1), str(df.columns[i]),\"type: \", str(df.dtypes[df.columns[i]])))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have  69  attributes.\n",
            "1         cervix_area                             type:     float64             \n",
            "2         os_area                                 type:     float64             \n",
            "3         walls_area                              type:     float64             \n",
            "4         speculum_area                           type:     float64             \n",
            "5         artifacts_area                          type:     float64             \n",
            "6         cervix_artifacts_area                   type:     float64             \n",
            "7         os_artifacts_area                       type:     float64             \n",
            "8         walls_artifacts_area                    type:     float64             \n",
            "9         speculum_artifacts_area                 type:     float64             \n",
            "10        cervix_specularities_area               type:     float64             \n",
            "11        os_specularities_area                   type:     float64             \n",
            "12        walls_specularities_area                type:     float64             \n",
            "13        speculum_specularities_area             type:     float64             \n",
            "14        specularities_area                      type:     float64             \n",
            "15        area_h_max_diff                         type:     float64             \n",
            "16        rgb_cervix_r_mean                       type:     float64             \n",
            "17        rgb_cervix_r_std                        type:     float64             \n",
            "18        rgb_cervix_r_mean_minus_std             type:     float64             \n",
            "19        rgb_cervix_r_mean_plus_std              type:     float64             \n",
            "20        rgb_cervix_g_mean                       type:     float64             \n",
            "21        rgb_cervix_g_std                        type:     float64             \n",
            "22        rgb_cervix_g_mean_minus_std             type:     float64             \n",
            "23        rgb_cervix_g_mean_plus_std              type:     float64             \n",
            "24        rgb_cervix_b_mean                       type:     float64             \n",
            "25        rgb_cervix_b_std                        type:     float64             \n",
            "26        rgb_cervix_b_mean_minus_std             type:     float64             \n",
            "27        rgb_cervix_b_mean_plus_std              type:     float64             \n",
            "28        rgb_total_r_mean                        type:     float64             \n",
            "29        rgb_total_r_std                         type:     float64             \n",
            "30        rgb_total_r_mean_minus_std              type:     float64             \n",
            "31        rgb_total_r_mean_plus_std               type:     float64             \n",
            "32        rgb_total_g_mean                        type:     float64             \n",
            "33        rgb_total_g_std                         type:     float64             \n",
            "34        rgb_total_g_mean_minus_std              type:     float64             \n",
            "35        rgb_total_g_mean_plus_std               type:     float64             \n",
            "36        rgb_total_b_mean                        type:     float64             \n",
            "37        rgb_total_b_std                         type:     float64             \n",
            "38        rgb_total_b_mean_minus_std              type:     float64             \n",
            "39        rgb_total_b_mean_plus_std               type:     float64             \n",
            "40        hsv_cervix_h_mean                       type:     float64             \n",
            "41        hsv_cervix_h_std                        type:     float64             \n",
            "42        hsv_cervix_s_mean                       type:     float64             \n",
            "43        hsv_cervix_s_std                        type:     float64             \n",
            "44        hsv_cervix_v_mean                       type:     float64             \n",
            "45        hsv_cervix_v_std                        type:     float64             \n",
            "46        hsv_total_h_mean                        type:     float64             \n",
            "47        hsv_total_h_std                         type:     float64             \n",
            "48        hsv_total_s_mean                        type:     float64             \n",
            "49        hsv_total_s_std                         type:     float64             \n",
            "50        hsv_total_v_mean                        type:     float64             \n",
            "51        hsv_total_v_std                         type:     float64             \n",
            "52        fit_cervix_hull_rate                    type:     float64             \n",
            "53        fit_cervix_hull_total                   type:     float64             \n",
            "54        fit_cervix_bbox_rate                    type:     float64             \n",
            "55        fit_cervix_bbox_total                   type:     float64             \n",
            "56        fit_circle_rate                         type:     float64             \n",
            "57        fit_circle_total                        type:     float64             \n",
            "58        fit_ellipse_rate                        type:     float64             \n",
            "59        fit_ellipse_total                       type:     float64             \n",
            "60        fit_ellipse_goodness                    type:     float64             \n",
            "61        dist_to_center_cervix                   type:     float64             \n",
            "62        dist_to_center_os                       type:     float64             \n",
            "63        experts::0                              type:     float64             \n",
            "64        experts::1                              type:     float64             \n",
            "65        experts::2                              type:     float64             \n",
            "66        experts::3                              type:     float64             \n",
            "67        experts::4                              type:     float64             \n",
            "68        experts::5                              type:     float64             \n",
            "69        consensus                               type:     float64             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSAYGUotwWE_",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι έχουμε 5 experts, αλλά εμείς θα δουλέψουμε μόνο με την συνολική άποψη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab4WaECKwgnl",
        "colab_type": "code",
        "outputId": "4381820f-2126-46cd-ac77-303891ee0cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df = df.drop(df.columns[[62, 63, 64, 65, 66, 67]], axis=1)\n",
        "df"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cervix_area</th>\n",
              "      <th>os_area</th>\n",
              "      <th>walls_area</th>\n",
              "      <th>speculum_area</th>\n",
              "      <th>artifacts_area</th>\n",
              "      <th>cervix_artifacts_area</th>\n",
              "      <th>os_artifacts_area</th>\n",
              "      <th>walls_artifacts_area</th>\n",
              "      <th>speculum_artifacts_area</th>\n",
              "      <th>cervix_specularities_area</th>\n",
              "      <th>os_specularities_area</th>\n",
              "      <th>walls_specularities_area</th>\n",
              "      <th>speculum_specularities_area</th>\n",
              "      <th>specularities_area</th>\n",
              "      <th>area_h_max_diff</th>\n",
              "      <th>rgb_cervix_r_mean</th>\n",
              "      <th>rgb_cervix_r_std</th>\n",
              "      <th>rgb_cervix_r_mean_minus_std</th>\n",
              "      <th>rgb_cervix_r_mean_plus_std</th>\n",
              "      <th>rgb_cervix_g_mean</th>\n",
              "      <th>rgb_cervix_g_std</th>\n",
              "      <th>rgb_cervix_g_mean_minus_std</th>\n",
              "      <th>rgb_cervix_g_mean_plus_std</th>\n",
              "      <th>rgb_cervix_b_mean</th>\n",
              "      <th>rgb_cervix_b_std</th>\n",
              "      <th>rgb_cervix_b_mean_minus_std</th>\n",
              "      <th>rgb_cervix_b_mean_plus_std</th>\n",
              "      <th>rgb_total_r_mean</th>\n",
              "      <th>rgb_total_r_std</th>\n",
              "      <th>rgb_total_r_mean_minus_std</th>\n",
              "      <th>rgb_total_r_mean_plus_std</th>\n",
              "      <th>rgb_total_g_mean</th>\n",
              "      <th>rgb_total_g_std</th>\n",
              "      <th>rgb_total_g_mean_minus_std</th>\n",
              "      <th>rgb_total_g_mean_plus_std</th>\n",
              "      <th>rgb_total_b_mean</th>\n",
              "      <th>rgb_total_b_std</th>\n",
              "      <th>rgb_total_b_mean_minus_std</th>\n",
              "      <th>rgb_total_b_mean_plus_std</th>\n",
              "      <th>hsv_cervix_h_mean</th>\n",
              "      <th>hsv_cervix_h_std</th>\n",
              "      <th>hsv_cervix_s_mean</th>\n",
              "      <th>hsv_cervix_s_std</th>\n",
              "      <th>hsv_cervix_v_mean</th>\n",
              "      <th>hsv_cervix_v_std</th>\n",
              "      <th>hsv_total_h_mean</th>\n",
              "      <th>hsv_total_h_std</th>\n",
              "      <th>hsv_total_s_mean</th>\n",
              "      <th>hsv_total_s_std</th>\n",
              "      <th>hsv_total_v_mean</th>\n",
              "      <th>hsv_total_v_std</th>\n",
              "      <th>fit_cervix_hull_rate</th>\n",
              "      <th>fit_cervix_hull_total</th>\n",
              "      <th>fit_cervix_bbox_rate</th>\n",
              "      <th>fit_cervix_bbox_total</th>\n",
              "      <th>fit_circle_rate</th>\n",
              "      <th>fit_circle_total</th>\n",
              "      <th>fit_ellipse_rate</th>\n",
              "      <th>fit_ellipse_total</th>\n",
              "      <th>fit_ellipse_goodness</th>\n",
              "      <th>dist_to_center_cervix</th>\n",
              "      <th>dist_to_center_os</th>\n",
              "      <th>consensus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.344647</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>0.047522</td>\n",
              "      <td>0.288216</td>\n",
              "      <td>0.178585</td>\n",
              "      <td>0.016564</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.263560</td>\n",
              "      <td>37.594458</td>\n",
              "      <td>15.785021</td>\n",
              "      <td>21.809437</td>\n",
              "      <td>53.379479</td>\n",
              "      <td>109.918445</td>\n",
              "      <td>38.735421</td>\n",
              "      <td>71.183024</td>\n",
              "      <td>148.653865</td>\n",
              "      <td>55.029618</td>\n",
              "      <td>22.160330</td>\n",
              "      <td>32.869287</td>\n",
              "      <td>77.189948</td>\n",
              "      <td>38.561367</td>\n",
              "      <td>38.119059</td>\n",
              "      <td>0.442308</td>\n",
              "      <td>76.680426</td>\n",
              "      <td>95.109755</td>\n",
              "      <td>51.565052</td>\n",
              "      <td>43.544702</td>\n",
              "      <td>146.674807</td>\n",
              "      <td>48.808474</td>\n",
              "      <td>40.765228</td>\n",
              "      <td>8.043247</td>\n",
              "      <td>89.573702</td>\n",
              "      <td>5.014628</td>\n",
              "      <td>2.991944</td>\n",
              "      <td>167.952780</td>\n",
              "      <td>25.813163</td>\n",
              "      <td>109.919447</td>\n",
              "      <td>38.733741</td>\n",
              "      <td>5.090801</td>\n",
              "      <td>2.936650</td>\n",
              "      <td>159.486916</td>\n",
              "      <td>38.437294</td>\n",
              "      <td>95.123889</td>\n",
              "      <td>51.583029</td>\n",
              "      <td>0.923067</td>\n",
              "      <td>0.373371</td>\n",
              "      <td>0.844454</td>\n",
              "      <td>0.408130</td>\n",
              "      <td>0.603399</td>\n",
              "      <td>0.571175</td>\n",
              "      <td>0.962995</td>\n",
              "      <td>0.357890</td>\n",
              "      <td>85.474311</td>\n",
              "      <td>0.265933</td>\n",
              "      <td>0.346294</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.165329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048236</td>\n",
              "      <td>0.504736</td>\n",
              "      <td>0.502783</td>\n",
              "      <td>0.007012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097405</td>\n",
              "      <td>0.973837</td>\n",
              "      <td>0.004055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054999</td>\n",
              "      <td>0.028431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.505882</td>\n",
              "      <td>24.361877</td>\n",
              "      <td>35.144005</td>\n",
              "      <td>83.867760</td>\n",
              "      <td>122.366075</td>\n",
              "      <td>44.742407</td>\n",
              "      <td>77.623669</td>\n",
              "      <td>167.108482</td>\n",
              "      <td>78.058434</td>\n",
              "      <td>30.818729</td>\n",
              "      <td>47.239706</td>\n",
              "      <td>108.877163</td>\n",
              "      <td>54.932467</td>\n",
              "      <td>39.447415</td>\n",
              "      <td>15.485052</td>\n",
              "      <td>94.379883</td>\n",
              "      <td>101.680459</td>\n",
              "      <td>46.028852</td>\n",
              "      <td>55.651607</td>\n",
              "      <td>147.709311</td>\n",
              "      <td>63.218931</td>\n",
              "      <td>43.925912</td>\n",
              "      <td>19.293019</td>\n",
              "      <td>107.144843</td>\n",
              "      <td>4.944382</td>\n",
              "      <td>2.965108</td>\n",
              "      <td>130.260492</td>\n",
              "      <td>24.143867</td>\n",
              "      <td>122.366647</td>\n",
              "      <td>44.743932</td>\n",
              "      <td>5.080063</td>\n",
              "      <td>2.894163</td>\n",
              "      <td>128.251978</td>\n",
              "      <td>33.000693</td>\n",
              "      <td>101.725519</td>\n",
              "      <td>46.093510</td>\n",
              "      <td>0.850861</td>\n",
              "      <td>0.194308</td>\n",
              "      <td>0.646645</td>\n",
              "      <td>0.255673</td>\n",
              "      <td>0.497315</td>\n",
              "      <td>0.332444</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.184803</td>\n",
              "      <td>124.794129</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.283059</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.457010</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.242888</td>\n",
              "      <td>0.212859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083055</td>\n",
              "      <td>0.018591</td>\n",
              "      <td>0.269798</td>\n",
              "      <td>39.353851</td>\n",
              "      <td>19.417332</td>\n",
              "      <td>19.936519</td>\n",
              "      <td>58.771183</td>\n",
              "      <td>109.543386</td>\n",
              "      <td>49.753490</td>\n",
              "      <td>59.789896</td>\n",
              "      <td>159.296876</td>\n",
              "      <td>54.642888</td>\n",
              "      <td>27.781965</td>\n",
              "      <td>26.860923</td>\n",
              "      <td>82.424853</td>\n",
              "      <td>41.242230</td>\n",
              "      <td>34.196356</td>\n",
              "      <td>7.045875</td>\n",
              "      <td>75.438586</td>\n",
              "      <td>109.592342</td>\n",
              "      <td>57.576640</td>\n",
              "      <td>52.015702</td>\n",
              "      <td>167.168982</td>\n",
              "      <td>53.470241</td>\n",
              "      <td>38.344391</td>\n",
              "      <td>15.125850</td>\n",
              "      <td>91.814632</td>\n",
              "      <td>5.049946</td>\n",
              "      <td>2.983966</td>\n",
              "      <td>163.576979</td>\n",
              "      <td>24.973042</td>\n",
              "      <td>109.544201</td>\n",
              "      <td>49.753659</td>\n",
              "      <td>5.078936</td>\n",
              "      <td>2.968023</td>\n",
              "      <td>162.268659</td>\n",
              "      <td>33.590792</td>\n",
              "      <td>109.597127</td>\n",
              "      <td>57.584515</td>\n",
              "      <td>0.918514</td>\n",
              "      <td>0.497554</td>\n",
              "      <td>0.747443</td>\n",
              "      <td>0.611432</td>\n",
              "      <td>0.633925</td>\n",
              "      <td>0.720923</td>\n",
              "      <td>0.920287</td>\n",
              "      <td>0.496596</td>\n",
              "      <td>94.948697</td>\n",
              "      <td>0.518740</td>\n",
              "      <td>0.419375</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.513244</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.213781</td>\n",
              "      <td>0.251819</td>\n",
              "      <td>0.079795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.107022</td>\n",
              "      <td>46.322391</td>\n",
              "      <td>17.711957</td>\n",
              "      <td>28.610434</td>\n",
              "      <td>64.034349</td>\n",
              "      <td>116.075087</td>\n",
              "      <td>43.593124</td>\n",
              "      <td>72.481962</td>\n",
              "      <td>159.668211</td>\n",
              "      <td>51.430923</td>\n",
              "      <td>18.573016</td>\n",
              "      <td>32.857907</td>\n",
              "      <td>70.003940</td>\n",
              "      <td>40.365565</td>\n",
              "      <td>17.259087</td>\n",
              "      <td>23.106478</td>\n",
              "      <td>57.624652</td>\n",
              "      <td>102.641859</td>\n",
              "      <td>38.606995</td>\n",
              "      <td>64.034863</td>\n",
              "      <td>141.248854</td>\n",
              "      <td>50.805205</td>\n",
              "      <td>18.072101</td>\n",
              "      <td>32.733104</td>\n",
              "      <td>68.877306</td>\n",
              "      <td>5.177654</td>\n",
              "      <td>2.969214</td>\n",
              "      <td>156.242754</td>\n",
              "      <td>25.499379</td>\n",
              "      <td>116.081770</td>\n",
              "      <td>43.582671</td>\n",
              "      <td>5.071879</td>\n",
              "      <td>2.909002</td>\n",
              "      <td>158.343946</td>\n",
              "      <td>28.273928</td>\n",
              "      <td>102.648278</td>\n",
              "      <td>38.598300</td>\n",
              "      <td>0.951710</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.855409</td>\n",
              "      <td>0.599998</td>\n",
              "      <td>0.618140</td>\n",
              "      <td>0.830304</td>\n",
              "      <td>0.964611</td>\n",
              "      <td>0.532073</td>\n",
              "      <td>74.221670</td>\n",
              "      <td>0.347202</td>\n",
              "      <td>0.361672</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.390319</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.272884</td>\n",
              "      <td>0.373487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.071442</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>0.442831</td>\n",
              "      <td>37.552979</td>\n",
              "      <td>14.454975</td>\n",
              "      <td>23.098004</td>\n",
              "      <td>52.007953</td>\n",
              "      <td>101.044906</td>\n",
              "      <td>37.171973</td>\n",
              "      <td>63.872932</td>\n",
              "      <td>138.216879</td>\n",
              "      <td>53.971671</td>\n",
              "      <td>17.591669</td>\n",
              "      <td>36.380003</td>\n",
              "      <td>71.563340</td>\n",
              "      <td>40.717229</td>\n",
              "      <td>38.810699</td>\n",
              "      <td>1.906530</td>\n",
              "      <td>79.527927</td>\n",
              "      <td>97.446185</td>\n",
              "      <td>48.106253</td>\n",
              "      <td>49.339932</td>\n",
              "      <td>145.552438</td>\n",
              "      <td>51.733004</td>\n",
              "      <td>39.866956</td>\n",
              "      <td>11.866047</td>\n",
              "      <td>91.599960</td>\n",
              "      <td>4.978534</td>\n",
              "      <td>2.964685</td>\n",
              "      <td>158.160578</td>\n",
              "      <td>30.308891</td>\n",
              "      <td>101.050711</td>\n",
              "      <td>37.163383</td>\n",
              "      <td>5.051587</td>\n",
              "      <td>2.923540</td>\n",
              "      <td>157.131407</td>\n",
              "      <td>39.439642</td>\n",
              "      <td>97.457304</td>\n",
              "      <td>48.125747</td>\n",
              "      <td>0.955996</td>\n",
              "      <td>0.408286</td>\n",
              "      <td>0.882990</td>\n",
              "      <td>0.442043</td>\n",
              "      <td>0.623938</td>\n",
              "      <td>0.625574</td>\n",
              "      <td>0.957604</td>\n",
              "      <td>0.407600</td>\n",
              "      <td>61.546536</td>\n",
              "      <td>0.437852</td>\n",
              "      <td>0.673196</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.610160</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.298345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028185</td>\n",
              "      <td>0.046193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015983</td>\n",
              "      <td>0.104929</td>\n",
              "      <td>73.465862</td>\n",
              "      <td>33.856824</td>\n",
              "      <td>39.609038</td>\n",
              "      <td>107.322686</td>\n",
              "      <td>44.657984</td>\n",
              "      <td>35.514453</td>\n",
              "      <td>9.143531</td>\n",
              "      <td>80.172437</td>\n",
              "      <td>97.007389</td>\n",
              "      <td>67.135578</td>\n",
              "      <td>29.871812</td>\n",
              "      <td>164.142967</td>\n",
              "      <td>80.241955</td>\n",
              "      <td>32.698142</td>\n",
              "      <td>47.543813</td>\n",
              "      <td>112.940096</td>\n",
              "      <td>49.969255</td>\n",
              "      <td>32.728615</td>\n",
              "      <td>17.240641</td>\n",
              "      <td>82.697870</td>\n",
              "      <td>103.504549</td>\n",
              "      <td>66.143863</td>\n",
              "      <td>37.360686</td>\n",
              "      <td>169.648412</td>\n",
              "      <td>4.376708</td>\n",
              "      <td>2.426611</td>\n",
              "      <td>149.832364</td>\n",
              "      <td>34.523784</td>\n",
              "      <td>107.238951</td>\n",
              "      <td>58.226891</td>\n",
              "      <td>4.243756</td>\n",
              "      <td>2.459024</td>\n",
              "      <td>141.193904</td>\n",
              "      <td>33.987154</td>\n",
              "      <td>112.817594</td>\n",
              "      <td>57.899252</td>\n",
              "      <td>0.951236</td>\n",
              "      <td>0.641439</td>\n",
              "      <td>0.681541</td>\n",
              "      <td>0.895266</td>\n",
              "      <td>0.559964</td>\n",
              "      <td>1.089641</td>\n",
              "      <td>0.916873</td>\n",
              "      <td>0.665480</td>\n",
              "      <td>141.328331</td>\n",
              "      <td>0.356888</td>\n",
              "      <td>0.353911</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.439891</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>0.031853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.458246</td>\n",
              "      <td>36.279936</td>\n",
              "      <td>23.325050</td>\n",
              "      <td>12.954885</td>\n",
              "      <td>59.604986</td>\n",
              "      <td>31.549222</td>\n",
              "      <td>23.099706</td>\n",
              "      <td>8.449515</td>\n",
              "      <td>54.648928</td>\n",
              "      <td>39.162020</td>\n",
              "      <td>33.679494</td>\n",
              "      <td>5.482527</td>\n",
              "      <td>72.841514</td>\n",
              "      <td>37.256992</td>\n",
              "      <td>27.997780</td>\n",
              "      <td>9.259212</td>\n",
              "      <td>65.254772</td>\n",
              "      <td>32.188663</td>\n",
              "      <td>27.246836</td>\n",
              "      <td>4.941828</td>\n",
              "      <td>59.435499</td>\n",
              "      <td>38.946898</td>\n",
              "      <td>37.576950</td>\n",
              "      <td>1.369948</td>\n",
              "      <td>76.523849</td>\n",
              "      <td>4.047217</td>\n",
              "      <td>2.068656</td>\n",
              "      <td>85.995172</td>\n",
              "      <td>37.051364</td>\n",
              "      <td>44.437573</td>\n",
              "      <td>31.678540</td>\n",
              "      <td>5.053266</td>\n",
              "      <td>1.795977</td>\n",
              "      <td>86.474830</td>\n",
              "      <td>39.921044</td>\n",
              "      <td>45.112433</td>\n",
              "      <td>35.381825</td>\n",
              "      <td>0.988825</td>\n",
              "      <td>0.444862</td>\n",
              "      <td>0.810836</td>\n",
              "      <td>0.542516</td>\n",
              "      <td>0.667897</td>\n",
              "      <td>0.658622</td>\n",
              "      <td>1.001402</td>\n",
              "      <td>0.439276</td>\n",
              "      <td>59.687772</td>\n",
              "      <td>0.529688</td>\n",
              "      <td>0.417762</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.696616</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.169087</td>\n",
              "      <td>0.146312</td>\n",
              "      <td>0.017217</td>\n",
              "      <td>0.024715</td>\n",
              "      <td>0.078889</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>0.038447</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.342323</td>\n",
              "      <td>0.060042</td>\n",
              "      <td>0.301857</td>\n",
              "      <td>74.766472</td>\n",
              "      <td>33.667006</td>\n",
              "      <td>41.099466</td>\n",
              "      <td>108.433478</td>\n",
              "      <td>44.113166</td>\n",
              "      <td>29.555688</td>\n",
              "      <td>14.557478</td>\n",
              "      <td>73.668854</td>\n",
              "      <td>68.044433</td>\n",
              "      <td>42.402254</td>\n",
              "      <td>25.642180</td>\n",
              "      <td>110.446687</td>\n",
              "      <td>95.119483</td>\n",
              "      <td>56.497295</td>\n",
              "      <td>38.622189</td>\n",
              "      <td>151.616778</td>\n",
              "      <td>62.400161</td>\n",
              "      <td>54.006420</td>\n",
              "      <td>8.393741</td>\n",
              "      <td>116.406581</td>\n",
              "      <td>104.351296</td>\n",
              "      <td>74.683000</td>\n",
              "      <td>29.668296</td>\n",
              "      <td>179.034296</td>\n",
              "      <td>3.439162</td>\n",
              "      <td>2.374293</td>\n",
              "      <td>116.377066</td>\n",
              "      <td>29.226454</td>\n",
              "      <td>81.122596</td>\n",
              "      <td>38.389404</td>\n",
              "      <td>3.774469</td>\n",
              "      <td>2.431975</td>\n",
              "      <td>117.490035</td>\n",
              "      <td>41.194225</td>\n",
              "      <td>114.484831</td>\n",
              "      <td>69.298856</td>\n",
              "      <td>0.977561</td>\n",
              "      <td>0.712606</td>\n",
              "      <td>0.830197</td>\n",
              "      <td>0.839097</td>\n",
              "      <td>0.534820</td>\n",
              "      <td>1.302525</td>\n",
              "      <td>1.020938</td>\n",
              "      <td>0.682329</td>\n",
              "      <td>196.767870</td>\n",
              "      <td>0.453209</td>\n",
              "      <td>0.590185</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026749</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.453958</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>0.220804</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 63 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cervix_area   os_area  ...  dist_to_center_os  consensus\n",
              "0       0.344647  0.003080  ...           0.346294        1.0\n",
              "1       0.165329  0.000000  ...           0.283059        0.0\n",
              "2       0.457010  0.001681  ...           0.419375        0.0\n",
              "3       0.513244  0.005711  ...           0.361672        1.0\n",
              "4       0.390319  0.009454  ...           0.673196        1.0\n",
              "..           ...       ...  ...                ...        ...\n",
              "282     0.610160  0.002726  ...           0.353911        0.0\n",
              "283     0.439891  0.006005  ...           0.417762        1.0\n",
              "284     0.696616  0.006693  ...           0.590185        1.0\n",
              "285     1.000000  0.000000  ...           0.402563        0.0\n",
              "286     1.000000  0.007517  ...           0.402563        0.0\n",
              "\n",
              "[287 rows x 63 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFW2ZAJs6pIz",
        "colab_type": "text"
      },
      "source": [
        "Ας μετρήσουμε τώρα, πόσοι είναι υγιείς και πόσοι όχι, και θα το αποθηκεύσουμε αυτό σαν τα labels μας (βεβαίως, θα πρέπει μετά να αφαιρέσουμε αυτήν την κολώνα χαρακτηριστικών, πριν κάνουμε training).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2757HtVM6vna",
        "colab_type": "code",
        "outputId": "7da69031-5642-441e-99ed-d8e4cddcb804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels_df = df.iloc[:, [62]]\n",
        "labels = labels_df.values.reshape(287,)\n",
        "positive = 0\n",
        "negative = 0\n",
        "for i in range(0, len(labels)):\n",
        "    positive += labels[i]==1\n",
        "    negative += labels[i]==0\n",
        "print(\"positive = \", positive, \" negative: \", negative)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive =  216  negative:  71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe8KtmADxNwS",
        "colab_type": "text"
      },
      "source": [
        "Βλεπουμε οτι εχουμε unbalanced data set, καθως η κλαση των positive(υγειων) ειναι κατα πολυ μεγαλυτερη σε μεγεθος απο την κλαση των negative. (75-25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2l2kgIc_qXj",
        "colab_type": "text"
      },
      "source": [
        "Θα πρέπει αρχικά να κάνουμε split τα δεδομένα μας, όπως λέει η εκφώνηση 80-20. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBRy3W1k8o82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train, test, train_labels, test_labels = train_test_split(df, labels, test_size=0.2, random_state=78)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCBzUF3cxwp8",
        "colab_type": "text"
      },
      "source": [
        "Ας επαληθεύσουμε ότι πάλι ο λόγος των positive(υγειων) με των negative(άρρωστων) είναι πάλι skewed:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF_OBTmhx9IA",
        "colab_type": "code",
        "outputId": "9bc6e3eb-9e32-40fe-db76-7272599849e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "positive = 0\n",
        "negative = 0\n",
        "for i in range(0, len(train_labels)):\n",
        "    positive += train_labels[i]==1\n",
        "    negative += train_labels[i]==0\n",
        "print(\"positive = \", positive, \" negative: \", negative)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive =  174  negative:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PTz3kzyO5S",
        "colab_type": "text"
      },
      "source": [
        "Ας γράψουμε τώρα τις συναρτήσεις μετασχηματισμών και δημιουργίας των classifiers, που θα τις χρησιμοποιήσουμε εντός του pipeline. Έχουμε γράψει συναρτήσεις για selection δεδομένων, resampling (κάτι που θα χρειαστεί σίγουρα, καθώς δεν έχουμε ισορροπημένο dataset), standardization και τέλος για την δημιοργία των κατηγοριοποιητών. Όλες αυτές οι μέθοδοι είναι αρκετά γενικευμένοι και μπορούν να πάρουν σαν παραμέτρους διαφορετικές τεχνικές που ίσως να θέλουμε να εφαρμόσουμε στην κάθε περίπτωση."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miepy3x5hvpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies a transformation (selection) on a dataset, \n",
        "    based on the type of the selector. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no transformation happens. The selector argument\n",
        "    is a string and can be one of two things: either \"PCA\" or \n",
        "    \"Variance Threshold\". In either case, there can be arguments, and in \"PCA\"\n",
        "    the arguments can be an int of the number of principal components, and in\n",
        "    \"Variance Threshold\", it can be a float. Returns the transformed dataset.\n",
        "\"\"\"\n",
        "def Apply_selector(data, selector = None, arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    \n",
        "    if selector == \"PCA\":    \n",
        "        if arguments:\n",
        "            number_of_components = arguments\n",
        "            pca = PCA(n_components = number_of_components)\n",
        "            pca.fit(train)\n",
        "            train_reduced = pca.transform(train)\n",
        "            test_reduced = pca.transform(test)\n",
        "        else:\n",
        "            pca = PCA()\n",
        "            pca.fit(train)\n",
        "            train_reduced = pca.transform(train)\n",
        "            test_reduced = pca.transform(test)\n",
        "    elif selector == \"Variance Threshold\":\n",
        "        if arguments:\n",
        "            t = arguments\n",
        "            sel = VarianceThreshold(threshold=t)\n",
        "            train_reduced = sel.fit_transform(train)\n",
        "            test_reduced = sel.transform(test)\n",
        "        else:\n",
        "            sel = VarianceThreshold()\n",
        "            train_reduced = sel.fit_transform(train)\n",
        "            test_reduced = sel.transform(test)\n",
        "    else:\n",
        "        (train_reduced, train_labels, test_reduced, test_labels) = data\n",
        "    return (train_reduced, train_labels, test_reduced, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05vv4ikkh3Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies resampling on a dataset, based on the \n",
        "    type of sampling method. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no sampling happens. The sampling argument\n",
        "    is a string and can be one of three things: \"Over\", \"Under\" or \n",
        "    \"OverUnder\". In any case, there can be arguments, and in Under or Over or\n",
        "    OverUnder it can be a float with the ratio of samples between classes. \n",
        "    Returns the resampled dataset.\n",
        "\"\"\"\n",
        "def Apply_sampling(data, sampling = None, arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    if sampling == \"Over\":    \n",
        "        if arguments:\n",
        "            ratio = arguments\n",
        "            ros = RandomOverSampler(sampling_strategy=ratio)\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "        else:\n",
        "            ros = RandomOverSampler()\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "        return (train_oversampled, train_labels_oversampled, test, test_labels)\n",
        "    elif sampling == \"Under\":\n",
        "        if arguments:\n",
        "            ratio = arguments\n",
        "            rus = RandomUnderSampler(sampling_strategy=ratio)\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train,train_labels)\n",
        "        else:\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train,train_labels)\n",
        "        return (train_undersampled, train_labels_undersampled, test, test_labels)\n",
        "    elif sampling==\"OverUnder\":\n",
        "        if arguments:    \n",
        "            ratio = arguments\n",
        "            ros = RandomOverSampler(sampling_strategy=ratio)\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train_oversampled,train_labels_oversampled)\n",
        "        else:\n",
        "            ros = RandomOverSampler()\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train_oversampled,train_labels_oversampled)\n",
        "        return (train_undersampled, train_labels_undersampled, test, test_labels)\n",
        "    else:\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qw5kMxliJud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats as st\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies a transformation (standardization) on a dataset, \n",
        "    based on the type of the standardizer. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no transformation happens. The selector argument\n",
        "    is a string and can be one of two things: either \"zscore\" or \n",
        "    \"minmax\". Argument arguments is always None. Returns the transformed dataset.\n",
        "\"\"\"\n",
        "def Apply_standardizer(data, standardizer = None,  arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    if standardizer==\"zscore\":\n",
        "        std_train = st.zscore(train)\n",
        "    elif standardizer==\"minmax\":\n",
        "        std_train = (train - np.min(train) )/ (np.max(train) - np.min(train))\n",
        "    else:\n",
        "        std_train = train\n",
        "    return (std_train, train_labels, test, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2doolScyIJqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that creates a classifier object. It does not train, \n",
        "    it takes the name of the classifier, along with the arguments we want \n",
        "    to pass to the instance creator for the classifier and returns the \n",
        "    classifier object.\n",
        "\"\"\"\n",
        "def Create_classifier(classifier,  arguments = None):\n",
        "    if classifier==\"kNN\":\n",
        "        if arguments and arguments!=-1:\n",
        "            clf = neighbors.KNeighborsClassifier(n_jobs = -1, n_neighbors = arguments)\n",
        "        else:\n",
        "            clf = neighbors.KNeighborsClassifier(n_jobs = -1)\n",
        "    else:\n",
        "        if arguments:\n",
        "            strat = arguments\n",
        "            clf = DummyClassifier(strategy=strat)\n",
        "        else:\n",
        "            clf = DummyClassifier()\n",
        "    return clf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVhGuPcy6eh",
        "colab_type": "text"
      },
      "source": [
        "Εδώ ορίζουμε την pipeline, η οποία αντιστοιχίζει τα steps της προεπεξεργασίας δεδομένων και της δημιοργίας των classifiers με τις παραπάνω μεθόδους. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMX219HCiTQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    This is an implementation for the function imblearn.pipeline.Pipeline().\n",
        "    The way this function works is, it takes as input two arguments. The first\n",
        "    is a list. This list contains the steps that need to be executed in the \n",
        "    pipeline, IN ORDER. Always the last step is the classifier. Each step is\n",
        "    a tuple, which contains the name of the step, and the arguments needed to \n",
        "    execute the step, like so: (\"name\", arguments). The second argument of the \n",
        "    function is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The function outputs a\n",
        "    tuple, which contains the processed data, ready for fitting the classier, \n",
        "    and the classifier object, as specified by the arguments, like so:\n",
        "    returns: (processed_data, classifier_object)\n",
        "\"\"\"\n",
        "def Pipeline(steps, data):\n",
        "    steps_dict = {\"selector\": Apply_selector,\n",
        "                  \"sampling\": Apply_sampling,\n",
        "                  \"standardizer\": Apply_standardizer,\n",
        "                  \"kNN\": Create_classifier,\n",
        "                  \"dummy\": Create_classifier}\n",
        "    for step in steps:\n",
        "        if step[0]!=\"kNN\" and step[0]!=\"dummy\":\n",
        "            data = steps_dict[step[0]](data,step[1])\n",
        "        else:\n",
        "            return ( data, Create_classifier(step[0], arguments = step[1]) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvUNGmO9zFDD",
        "colab_type": "text"
      },
      "source": [
        "Η συνάρτηση model_tuning ουσιαστικά υλοποιεί την gridsearchcv, και δημιουργεί για κάθε παράμετρο, για κάθε αλληλουχία steps, για κάθε dataset ένα pipe, και στην συνέχεια κάνει train τα δεδομένα και βγάζει ένα training accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RktfiSkMmjfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\"\"\"\n",
        "    This is an implementation for the function gridsearchcv. It takes 3\n",
        "    arguments, a tuple (model_name, model_parameters), the data to be fitted\n",
        "    and trained, and the specific parameters. The data parameter needs to be \n",
        "    in the form: (train_data, train_labels, test_data, test_labels). Lastly, \n",
        "    the Steps argument is a list, that has the steps for the data preprocessing,\n",
        "    as described in the Pipeline Function. For this exercise, we are dealing \n",
        "    only with dummy classifiers and kNN. The model_parameters if not None, is \n",
        "    a list of parameters (strings for dummy, ints for kNN).\n",
        "    Returns (classifier_object, best_parameter, accuracy_list)\n",
        "\"\"\"\n",
        "def model_tuning(model, folded_data, preprocessing_steps = None):\n",
        "    #Start measuring the time:\n",
        "    start_time = time.time()\n",
        "    #if the classifier is a dummy:\n",
        "    if model[0]==\"dummy\":\n",
        "        #and it has parameters (strategy):\n",
        "        if model[1]!=None:\n",
        "\n",
        "\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "            best_parameter = model[1][0]\n",
        "            #for each parameter, run the \n",
        "            for parameter in model[1]:\n",
        "    \n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], parameter)]\n",
        "                else:\n",
        "                    steps = [(model[0], parameter)]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "                \n",
        "                \n",
        "                \n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list.append(sum(accuracy_list)/len(accuracy_list))\n",
        "                \n",
        "                #if the average accuracy of the 10 folds is the best one yet, \n",
        "                #keep the parameter\n",
        "                if (sum(accuracy_list)/len(accuracy_list)) >= max(avg_accuracy_list):\n",
        "                    best_parameter = parameter\n",
        "        \n",
        "        \n",
        "        \n",
        "        #if the dummy classifier has no parameters:                    \n",
        "        else:\n",
        "                #initialize the lists with the possible best parameters, and the average\n",
        "                #accuracies for each of the model parameter\n",
        "                avg_accuracy_list = []\n",
        "\n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], model[1])]\n",
        "                else:\n",
        "                    steps = [(model[0], model[1])]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                \n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "                \n",
        "                #since no parameter list has been given, best_param = param\n",
        "                best_parameter = None\n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "    \n",
        "    \n",
        "    #if the classifier is a kNN:\n",
        "    elif model[0] == \"kNN\":\n",
        "\n",
        "        #if the kNN classifier has no parameters:    \n",
        "        if model[1]== -1:\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "\n",
        "            #if there are preprocessing steps, add the model_creation \n",
        "            #to the pipeline:\n",
        "            if preprocessing_steps:\n",
        "                steps = preprocessing_steps + [(model[0], model[1])]\n",
        "            else:\n",
        "                steps = [(model[0], model[1])]\n",
        "            #create the pipeline, returns the processed data, the \n",
        "            #classifier ready for training and evaluating:\n",
        "\n",
        "            \n",
        "            #initialize a list with the accuracies for the folded data:\n",
        "            accuracy_list = []\n",
        "                \n",
        "            #train for each of the 10 folds, begin from scratch every time,\n",
        "            #and then average out the results.\n",
        "            for data in folded_data:\n",
        "                \n",
        "                #process the data, create the classifier\n",
        "                (processed_data, clf) = Pipeline(steps, data)\n",
        "                (train, train_labels, test, test_labels) = processed_data\n",
        "                #fit the classifier\n",
        "                clf = clf.fit(train, train_labels)\n",
        "                #make the predictions based on the training\n",
        "                preds = clf.predict(test)\n",
        "                accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "            \n",
        "            #since no parameter list has been given, best_param = None\n",
        "            best_parameter = None\n",
        "            #store the average accuracy\n",
        "            avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "\n",
        "        #if it has parameters (number_of_neighbors):    \n",
        "        else:\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "            best_parameter = model[1][0]\n",
        "            #for each parameter, run the \n",
        "            for parameter in model[1]:\n",
        "    \n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], parameter)]\n",
        "                else:\n",
        "                    steps = [(model[0], parameter)]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                \n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(accuracy_score(test_labels, preds))\n",
        "                print(accuracy_list)\n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list.append(sum(accuracy_list)/len(accuracy_list))\n",
        "\n",
        "\n",
        "                #if the average accuracy of the 10 folds is the best one yet, \n",
        "                #keep the parameter\n",
        "                if (sum(accuracy_list)/len(accuracy_list)) >= max(avg_accuracy_list):\n",
        "                    best_parameter = parameter\n",
        "                \n",
        "        \n",
        "        \n",
        "\n",
        "    print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
        "    return (clf, best_parameter, avg_accuracy_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8P93LtzWsy",
        "colab_type": "text"
      },
      "source": [
        "Η συνάρτηση αυτή είναι η υλοποίηση της Kfold(), και σπάει σε διπλώματα το dataset. Συγκεκριμένα, το σπάει σε 10 διαφορετικά dataset, προκειμένου να αξιοποιήσουμε καλύτερα τον σχετικά μικρό όγκο δεδομένων μας. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRmPXV7pibkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "\"\"\"This function is an implementation for KFold(). It takes one argument that \n",
        "is a tuple in the form (train, train_labels, test, test_labels), and returns\n",
        "a list of k (10) tuples in the same form. It shuffles the train and train \n",
        "labels, to randomize the data, before splitting. \"\"\"\n",
        "def ten_fold(data):\n",
        "    #In our exercise, the number of folds is 10:\n",
        "    k = 10\n",
        "    #Reading the data, no need to read the test and test_labels data.\n",
        "    train, train_labels, _, _ = data\n",
        "    #Randomize the data:\n",
        "    train, train_labels = shuffle(train, train_labels)\n",
        "    #train is a pandas dataframe, need to make it a numpy array:\n",
        "    train = np.asarray(train)\n",
        "    #This is the size of the test set each time:\n",
        "    fold_size = int(len(train)/k)\n",
        "    #Create the list to be returned, fill it:\n",
        "    folds = []\n",
        "    for i in range(0,10):\n",
        "        train1 = []\n",
        "        train_labels1 = []\n",
        "        test = []\n",
        "        test_labels = []\n",
        "        for j in range(0, len(train)):\n",
        "            if j>= i*fold_size and j<(i+1)*fold_size:\n",
        "                test.append(train[j])\n",
        "                test_labels.append(train_labels[j])\n",
        "            else:\n",
        "                train1.append(train[j])\n",
        "                train_labels1.append(train_labels[j])\n",
        "        folds.append((train1, train_labels1, test, test_labels))\n",
        "    return folds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzzeMryyznY7",
        "colab_type": "text"
      },
      "source": [
        "Ας ξαναδιαβάσουμε από την αρχή τα δεδομένα μας, και ας τα χωρίσουμε πάλι σε training data και test data πριν προχωρήσουμε στο Γ. Πρέπει όμως πριν, να αφαιρέσουμε την στήλη που περιέχει τις ετικέτες για την υγεία των ασθενών."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ2oqrmDedlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"all.csv\")\n",
        "df = df.drop(df.columns[[62, 63, 64, 65, 66, 67]], axis=1)\n",
        "labels_df = df.iloc[:, [62]]\n",
        "labels = labels_df.values.reshape(287,)\n",
        "\n",
        "\n",
        "df = df.drop(df.columns[62], axis=1)\n",
        "train, test, train_labels, test_labels = train_test_split(df, labels, test_size=0.2, random_state=78)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjNCI0V_z-lz",
        "colab_type": "text"
      },
      "source": [
        "#Γ \n",
        "---\n",
        "\n",
        "Για το section αυτό, αρχικά πρέπει να εκπαιδεύσουμε τους classifiers (dummy και kNN) στο train dataset, με deafult τιμές. Καλόυμε την συνάρτηση model_tuning, και για τα δύο μοντέλα, χωρίς επιπλέον παραμέτρους, και θα λάβουμε πίσω τους classifiers, την καλύτερη παράμετρο (εδώ None, αφού δεν τους έχουμε δώσει κάποια παράμετρο για training) και το average training accuracy πάνω σε 10 folds. \n",
        "\n",
        "Δεν θα κάνουμε κάποια preprocessing steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqq8OnaOlBuY",
        "colab_type": "code",
        "outputId": "dd2de6f5-2faf-4371-f63e-970e657a7f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Group the splitted dataset into a tuple:\n",
        "data = (train, train_labels, test,  test_labels)\n",
        "\n",
        "dummy_default = DummyClassifier()\n",
        "dummy_default.fit(train, train_labels)\n",
        "\n",
        "kNN_default = KNeighborsClassifier()\n",
        "kNN_default.fit(train, train_labels)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV0XlTdz2N1T",
        "colab_type": "text"
      },
      "source": [
        "Τώρα θα κάνουμε εκτίμηση στο test set και για τους δύο, θα βρούμε το confusion matrix τους, και θα τυπώσουμε το f1-micro και f1-macro average:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPXFLHN2NZN",
        "colab_type": "code",
        "outputId": "43225979-17e5-4a06-86e4-1e2d4fcc8b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preds1 = dummy_default.predict(test)\n",
        "preds2 = kNN_default.predict(test)\n",
        "\n",
        "#print the accuracies:\n",
        "print(\"The average testing accuracy of the default dummy classifier is: \", accuracy_score(test_labels, preds1))\n",
        "print(\"The average testing accuracy of the default kNN classifier is: \", accuracy_score(test_labels, preds2))\n",
        "\n",
        "#plot the confusion matrices:\n",
        "disp1 = plot_confusion_matrix(dummy_default, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "disp1 = plot_confusion_matrix(kNN_default, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#print the classification reports, where f1 macro average is shown:\n",
        "print(classification_report(test_labels, preds1, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "print(classification_report(test_labels, preds2, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "\n",
        "\n",
        "#print the f1 micro scores:\n",
        "print(\"F1 micro score for the default dummy classifier is: \", f1_score(test_labels, preds1, average = 'micro'))\n",
        "print(\"F1 micro score for the default kNN classifier is: \", f1_score(test_labels, preds2, average = 'micro'))\n",
        "\n",
        "#print the f1 macro scores:\n",
        "print(\"F1 macro score for the default dummy classifier is: \", f1_score(test_labels, preds1, average = 'macro'))\n",
        "print(\"F1 macro score for the default kNN classifier is: \", f1_score(test_labels, preds2, average = 'macro'))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average testing accuracy of the default dummy classifier is:  0.5689655172413793\n",
            "The average testing accuracy of the default kNN classifier is:  0.7586206896551724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEGCAYAAAAt9v2AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n18wsY/8NWp8CijT0tdoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEKCAYAAABaND37AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7gAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.15      0.12      0.14        16\n",
            "     healthy       0.69      0.74      0.71        42\n",
            "\n",
            "    accuracy                           0.57        58\n",
            "   macro avg       0.42      0.43      0.43        58\n",
            "weighted avg       0.54      0.57      0.55        58\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.67      0.25      0.36        16\n",
            "     healthy       0.77      0.95      0.85        42\n",
            "\n",
            "    accuracy                           0.76        58\n",
            "   macro avg       0.72      0.60      0.61        58\n",
            "weighted avg       0.74      0.76      0.72        58\n",
            "\n",
            "F1 micro score for the default dummy classifier is:  0.5689655172413793\n",
            "F1 micro score for the default kNN classifier is:  0.7586206896551724\n",
            "F1 macro score for the default dummy classifier is:  0.4252873563218391\n",
            "F1 macro score for the default kNN classifier is:  0.6073500967117988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rdSr2lzEhBJ",
        "colab_type": "text"
      },
      "source": [
        "Τυπώνουμε δύο bar plots, που δείχνουν τις τιμές των δύο classifiers μας (default_dummy, default_kNN) για τα averaged metrics f1 micro και f1 macro:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxoVbCF4E4Kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "8bd168ea-fc25-437e-a098-39223c67e24e"
      },
      "source": [
        "n_groups = 2\n",
        "f1_micro = (f1_score(test_labels, preds1, average = 'micro'), f1_score(test_labels, preds2, average = 'micro'))\n",
        "f1_macro = (f1_score(test_labels, preds1, average = 'macro'), f1_score(test_labels, preds2, average = 'macro'))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(n_groups)\n",
        "bar_width = 0.35\n",
        "opacity = 0.8\n",
        "\n",
        "rects1 = plt.bar(index, f1_micro, bar_width,\n",
        "alpha=opacity,\n",
        "color='b',\n",
        "label='F1 micro')\n",
        "\n",
        "rects2 = plt.bar(index + bar_width, f1_macro, bar_width,\n",
        "alpha=opacity,\n",
        "color='g',\n",
        "label='F1 macro')\n",
        "\n",
        "plt.xlabel('Classifier')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Scores by classifier')\n",
        "plt.xticks(index + bar_width, ('default Dummy', 'default kNN'))\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWS45oMzMLJf+P4DZqJij+7z1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTEz5VvLd20r",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι και οι δύο classifiers έχουν μέτρια απόδοση. Αυτό είναι αποτέλεσμα πολλών παραγόντων.  \n",
        "\n",
        "\n",
        "Αρχικά, δεν έχουμε επεξεργαστεί τα δεδομένα, κάτι που είναι πολύ σημαντικό. Τα δεδομένα μας δεν είναι καθόλου ισορροπημένα, καθώς ο αριθμός των δειγμάτων της μιας κλάσης είναι εξαιρετικά μεγαλύτερος της άλλης (τα δεδομένα είναι split μεταξύ των κλάσεων σε 75% positive - 25% negative). \n",
        "\n",
        "\n",
        "Πέρα από την ανισορροπία των δεδομένων, δεν έχει γίνει καθόλου επιλογή, και καθόλου standardization. Αυτό μπορεί να είναι εξαιρετικά κακό για τον classifier kNN, αλλά για τον dummy classifier, λόγω της απλότητάς του, να μην τον επηρεάζει. \n",
        "\n",
        "\n",
        "By default, ο default dummy classifier ακολουθεί μια στρατηγική που (σύμφωνα με το documentation) ονομάζεται \"stratified\", \"γεννά προβλέψεις λαμβάνοντας υπόψιν την κατανομή των δεδομένων\". Βλέπουμε ότι ο κατηγοριοποιητής dummy έχει accuracy περίπου 68%, κάτι που μας υποδεικνύει ότι τον επηρεάζει κάπως η κλάση με την μεγαλύτερη εμφανισιμότητα, δηλαδή τα θετικά. \n",
        "\n",
        "\n",
        "Από την άλλη, ο default kNN classifier έχει σαν όρισμα τιμή για κοντινότερους γείτονες n_neighbors = 5. Αυτή η τιμή είναι πιθανώς πολύ μικρή, και σε συνδυασμό με τα καθόλου καλά δεδομένα που του παρέχουμε, δεν εμφανίζει καλύτερη απόδοση από τον dummy classifier. \n",
        "\n",
        "\n",
        "Σαν να μην φτάναν όλα αυτά, λόγω της διάτμησης των δεδομένων με την μέθοδο kfold, τα δεδομένα που κάθε φορά λαμβάνει σαν training ο kNN classifier είναι σχετικά λίγα και το k δεν μπορεί να φτάσει σε μεγάλες τιμές (όχι ότι εκεί θα βρισκόταν απαραίτητα το βέλτιστο, αλλά καλό θα ήταν να βλέπαμε την συμπεριφορά του. \n",
        "\n",
        "\n",
        "Βλέπουμε λοιπόν ότι, τόσο οι F1 micro όσο και οι F1 macro μέσες τιμές είναι σαφώς καλύτερες στον kNN classifier. Φυσικά, επιδέχονται σημαντικής βελτίωσης.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ndY0gk7pR0",
        "colab_type": "text"
      },
      "source": [
        "#Δ\n",
        "---\n",
        "\n",
        "Για το section αυτό,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V238HWfk6byA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}