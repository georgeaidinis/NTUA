{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B8 - S11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeaidinis/NTUA/blob/master/NN/Lab%201/B8_S11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cBcurO-uUmo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "#Lab 1: Επιβλεπόμενη Μάθηση - Ταξινόμηση - Μικρό Dataset (S11 - Quality Assessment of Digital Colposcopies)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKqs0XolswuQ",
        "colab_type": "text"
      },
      "source": [
        "#Section A\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Αϊδίνης Γιώργος 03116031\n",
        "\n",
        "Κολιός Παναγιώτης 03116100\n",
        "\n",
        "---\n",
        "\n",
        "Ομάδα M.B.8\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-LPsGgQufbH",
        "colab_type": "text"
      },
      "source": [
        "# Section B\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypUIaKPuxyCI",
        "colab_type": "text"
      },
      "source": [
        "Αρχικά ενημερώνουμε τις βιβλιοθήκες που θα χρησιμοποιήσουμε.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZTpGgaqwT3G",
        "colab_type": "code",
        "outputId": "57af2457-c13b-4720-ba14-7fac59253351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "!pip install --upgrade pip #upgrade pip package installer\n",
        "!pip install scikit-learn --upgrade #upgrade scikit-learn package\n",
        "!pip install numpy --upgrade #upgrade numpy package\n",
        "!pip install pandas --upgrade #--upgrade #upgrade pandas package\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.3.1)\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/d0/860c4f6a7027e00acff373d9f5327f4ae3ed5872234b3cbdd7bcb52e5eff/scikit_learn-0.22-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.4)\n",
            "Installing collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.21.3\n",
            "    Uninstalling scikit-learn-0.21.3:\n",
            "      Successfully uninstalled scikit-learn-0.21.3\n",
            "Successfully installed scikit-learn-0.22\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.17.4)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsvljyaGuws2",
        "colab_type": "text"
      },
      "source": [
        "Κατεβάζουμε το αρχείο που περιέχει το dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPGXEYNcqtmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL_pZToQuTs4",
        "colab_type": "code",
        "outputId": "30dd94b6-d3c6-4ca3-e346-9ceb37ccfd5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "download(\"http://archive.ics.uci.edu/ml/machine-learning-databases/00384/Quality%20Assessment%20-%20Digital%20Colposcopy.zip\",\"QADC.zip\")\n",
        "print(\"All the files are downloaded\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download file... QADC.zip ...\n",
            "File downloaded\n",
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrdHuDyTvAD7",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι το αρχείο όντως κατέβηκε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1fY1PPQ7L9d",
        "colab_type": "code",
        "outputId": "0657203d-51b5-4425-d4da-52475821f836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QADC.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egTI09KgvG47",
        "colab_type": "text"
      },
      "source": [
        "Επειδή είναι compressed, πρεπει να το κάνουμε decompress και να δουμε ότι όντως έγινε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFe5qqIq6jSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"QADC.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AZDcWwK7IKb",
        "colab_type": "code",
        "outputId": "7f33ff62-db76-4a72-8ec9-8d544dcbfa45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " QADC.zip  'Quality Assessment - Digital Colposcopy'   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDKrxf0wzLvH",
        "colab_type": "text"
      },
      "source": [
        "Μετονομάζουμε τον φάκελο για να έχει μικρότερο όνομα:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxlh8yEDy47d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv Quality\\ Assessment\\ -\\ Digital\\ Colposcopy QADC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PDIZ5HBzx9i",
        "colab_type": "text"
      },
      "source": [
        "Ανοίγοντας τον φάκελο, είδαμε ότι έχουμε τρια αρχεία, τα οποία έχουν headers, και πρέπει 1) να ενοποιηθούν σε ένα αρχείο και 2) να αφαιρεθούν οι επιπρόσθετοι headers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhtKAnFCyl0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"QADC/hinselmann.csv\",'r') as f:\n",
        "    with open(\"QADC/hinselmann1.csv\",'w') as f1:\n",
        "        next(f) # skip header line\n",
        "        for line in f:\n",
        "            f1.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcY__jBiymmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"QADC/schiller.csv\",'r') as f:\n",
        "    with open(\"QADC/schiller1.csv\",'w') as f1:\n",
        "        next(f) # skip header line\n",
        "        for line in f:\n",
        "            f1.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hTiHGpp0JR2",
        "colab_type": "text"
      },
      "source": [
        "Φτιάχνουμε ένα μεγάλο αρχείο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VimXlyS7fGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat QADC/green.csv QADC/hinselmann1.csv QADC/schiller1.csv > all.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPLZO8N605LP",
        "colab_type": "text"
      },
      "source": [
        "Για να δούμε λοιπόν το dataset μας ολόκληρο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVGBnlk67zwC",
        "colab_type": "code",
        "outputId": "8c0f6089-6a99-4fe6-806b-4ccf59eda0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"all.csv\")\n",
        "df"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cervix_area</th>\n",
              "      <th>os_area</th>\n",
              "      <th>walls_area</th>\n",
              "      <th>speculum_area</th>\n",
              "      <th>artifacts_area</th>\n",
              "      <th>cervix_artifacts_area</th>\n",
              "      <th>os_artifacts_area</th>\n",
              "      <th>walls_artifacts_area</th>\n",
              "      <th>speculum_artifacts_area</th>\n",
              "      <th>cervix_specularities_area</th>\n",
              "      <th>os_specularities_area</th>\n",
              "      <th>walls_specularities_area</th>\n",
              "      <th>speculum_specularities_area</th>\n",
              "      <th>specularities_area</th>\n",
              "      <th>area_h_max_diff</th>\n",
              "      <th>rgb_cervix_r_mean</th>\n",
              "      <th>rgb_cervix_r_std</th>\n",
              "      <th>rgb_cervix_r_mean_minus_std</th>\n",
              "      <th>rgb_cervix_r_mean_plus_std</th>\n",
              "      <th>rgb_cervix_g_mean</th>\n",
              "      <th>rgb_cervix_g_std</th>\n",
              "      <th>rgb_cervix_g_mean_minus_std</th>\n",
              "      <th>rgb_cervix_g_mean_plus_std</th>\n",
              "      <th>rgb_cervix_b_mean</th>\n",
              "      <th>rgb_cervix_b_std</th>\n",
              "      <th>rgb_cervix_b_mean_minus_std</th>\n",
              "      <th>rgb_cervix_b_mean_plus_std</th>\n",
              "      <th>rgb_total_r_mean</th>\n",
              "      <th>rgb_total_r_std</th>\n",
              "      <th>rgb_total_r_mean_minus_std</th>\n",
              "      <th>rgb_total_r_mean_plus_std</th>\n",
              "      <th>rgb_total_g_mean</th>\n",
              "      <th>rgb_total_g_std</th>\n",
              "      <th>rgb_total_g_mean_minus_std</th>\n",
              "      <th>rgb_total_g_mean_plus_std</th>\n",
              "      <th>rgb_total_b_mean</th>\n",
              "      <th>rgb_total_b_std</th>\n",
              "      <th>rgb_total_b_mean_minus_std</th>\n",
              "      <th>rgb_total_b_mean_plus_std</th>\n",
              "      <th>hsv_cervix_h_mean</th>\n",
              "      <th>hsv_cervix_h_std</th>\n",
              "      <th>hsv_cervix_s_mean</th>\n",
              "      <th>hsv_cervix_s_std</th>\n",
              "      <th>hsv_cervix_v_mean</th>\n",
              "      <th>hsv_cervix_v_std</th>\n",
              "      <th>hsv_total_h_mean</th>\n",
              "      <th>hsv_total_h_std</th>\n",
              "      <th>hsv_total_s_mean</th>\n",
              "      <th>hsv_total_s_std</th>\n",
              "      <th>hsv_total_v_mean</th>\n",
              "      <th>hsv_total_v_std</th>\n",
              "      <th>fit_cervix_hull_rate</th>\n",
              "      <th>fit_cervix_hull_total</th>\n",
              "      <th>fit_cervix_bbox_rate</th>\n",
              "      <th>fit_cervix_bbox_total</th>\n",
              "      <th>fit_circle_rate</th>\n",
              "      <th>fit_circle_total</th>\n",
              "      <th>fit_ellipse_rate</th>\n",
              "      <th>fit_ellipse_total</th>\n",
              "      <th>fit_ellipse_goodness</th>\n",
              "      <th>dist_to_center_cervix</th>\n",
              "      <th>dist_to_center_os</th>\n",
              "      <th>experts::0</th>\n",
              "      <th>experts::1</th>\n",
              "      <th>experts::2</th>\n",
              "      <th>experts::3</th>\n",
              "      <th>experts::4</th>\n",
              "      <th>experts::5</th>\n",
              "      <th>consensus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.344647</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>0.047522</td>\n",
              "      <td>0.288216</td>\n",
              "      <td>0.178585</td>\n",
              "      <td>0.016564</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.263560</td>\n",
              "      <td>37.594458</td>\n",
              "      <td>15.785021</td>\n",
              "      <td>21.809437</td>\n",
              "      <td>53.379479</td>\n",
              "      <td>109.918445</td>\n",
              "      <td>38.735421</td>\n",
              "      <td>71.183024</td>\n",
              "      <td>148.653865</td>\n",
              "      <td>55.029618</td>\n",
              "      <td>22.160330</td>\n",
              "      <td>32.869287</td>\n",
              "      <td>77.189948</td>\n",
              "      <td>38.561367</td>\n",
              "      <td>38.119059</td>\n",
              "      <td>0.442308</td>\n",
              "      <td>76.680426</td>\n",
              "      <td>95.109755</td>\n",
              "      <td>51.565052</td>\n",
              "      <td>43.544702</td>\n",
              "      <td>146.674807</td>\n",
              "      <td>48.808474</td>\n",
              "      <td>40.765228</td>\n",
              "      <td>8.043247</td>\n",
              "      <td>89.573702</td>\n",
              "      <td>5.014628</td>\n",
              "      <td>2.991944</td>\n",
              "      <td>167.952780</td>\n",
              "      <td>25.813163</td>\n",
              "      <td>109.919447</td>\n",
              "      <td>38.733741</td>\n",
              "      <td>5.090801</td>\n",
              "      <td>2.936650</td>\n",
              "      <td>159.486916</td>\n",
              "      <td>38.437294</td>\n",
              "      <td>95.123889</td>\n",
              "      <td>51.583029</td>\n",
              "      <td>0.923067</td>\n",
              "      <td>0.373371</td>\n",
              "      <td>0.844454</td>\n",
              "      <td>0.408130</td>\n",
              "      <td>0.603399</td>\n",
              "      <td>0.571175</td>\n",
              "      <td>0.962995</td>\n",
              "      <td>0.357890</td>\n",
              "      <td>85.474311</td>\n",
              "      <td>0.265933</td>\n",
              "      <td>0.346294</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.165329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048236</td>\n",
              "      <td>0.504736</td>\n",
              "      <td>0.502783</td>\n",
              "      <td>0.007012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097405</td>\n",
              "      <td>0.973837</td>\n",
              "      <td>0.004055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054999</td>\n",
              "      <td>0.028431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.505882</td>\n",
              "      <td>24.361877</td>\n",
              "      <td>35.144005</td>\n",
              "      <td>83.867760</td>\n",
              "      <td>122.366075</td>\n",
              "      <td>44.742407</td>\n",
              "      <td>77.623669</td>\n",
              "      <td>167.108482</td>\n",
              "      <td>78.058434</td>\n",
              "      <td>30.818729</td>\n",
              "      <td>47.239706</td>\n",
              "      <td>108.877163</td>\n",
              "      <td>54.932467</td>\n",
              "      <td>39.447415</td>\n",
              "      <td>15.485052</td>\n",
              "      <td>94.379883</td>\n",
              "      <td>101.680459</td>\n",
              "      <td>46.028852</td>\n",
              "      <td>55.651607</td>\n",
              "      <td>147.709311</td>\n",
              "      <td>63.218931</td>\n",
              "      <td>43.925912</td>\n",
              "      <td>19.293019</td>\n",
              "      <td>107.144843</td>\n",
              "      <td>4.944382</td>\n",
              "      <td>2.965108</td>\n",
              "      <td>130.260492</td>\n",
              "      <td>24.143867</td>\n",
              "      <td>122.366647</td>\n",
              "      <td>44.743932</td>\n",
              "      <td>5.080063</td>\n",
              "      <td>2.894163</td>\n",
              "      <td>128.251978</td>\n",
              "      <td>33.000693</td>\n",
              "      <td>101.725519</td>\n",
              "      <td>46.093510</td>\n",
              "      <td>0.850861</td>\n",
              "      <td>0.194308</td>\n",
              "      <td>0.646645</td>\n",
              "      <td>0.255673</td>\n",
              "      <td>0.497315</td>\n",
              "      <td>0.332444</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.184803</td>\n",
              "      <td>124.794129</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.283059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.457010</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.242888</td>\n",
              "      <td>0.212859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083055</td>\n",
              "      <td>0.018591</td>\n",
              "      <td>0.269798</td>\n",
              "      <td>39.353851</td>\n",
              "      <td>19.417332</td>\n",
              "      <td>19.936519</td>\n",
              "      <td>58.771183</td>\n",
              "      <td>109.543386</td>\n",
              "      <td>49.753490</td>\n",
              "      <td>59.789896</td>\n",
              "      <td>159.296876</td>\n",
              "      <td>54.642888</td>\n",
              "      <td>27.781965</td>\n",
              "      <td>26.860923</td>\n",
              "      <td>82.424853</td>\n",
              "      <td>41.242230</td>\n",
              "      <td>34.196356</td>\n",
              "      <td>7.045875</td>\n",
              "      <td>75.438586</td>\n",
              "      <td>109.592342</td>\n",
              "      <td>57.576640</td>\n",
              "      <td>52.015702</td>\n",
              "      <td>167.168982</td>\n",
              "      <td>53.470241</td>\n",
              "      <td>38.344391</td>\n",
              "      <td>15.125850</td>\n",
              "      <td>91.814632</td>\n",
              "      <td>5.049946</td>\n",
              "      <td>2.983966</td>\n",
              "      <td>163.576979</td>\n",
              "      <td>24.973042</td>\n",
              "      <td>109.544201</td>\n",
              "      <td>49.753659</td>\n",
              "      <td>5.078936</td>\n",
              "      <td>2.968023</td>\n",
              "      <td>162.268659</td>\n",
              "      <td>33.590792</td>\n",
              "      <td>109.597127</td>\n",
              "      <td>57.584515</td>\n",
              "      <td>0.918514</td>\n",
              "      <td>0.497554</td>\n",
              "      <td>0.747443</td>\n",
              "      <td>0.611432</td>\n",
              "      <td>0.633925</td>\n",
              "      <td>0.720923</td>\n",
              "      <td>0.920287</td>\n",
              "      <td>0.496596</td>\n",
              "      <td>94.948697</td>\n",
              "      <td>0.518740</td>\n",
              "      <td>0.419375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.513244</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.213781</td>\n",
              "      <td>0.251819</td>\n",
              "      <td>0.079795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.107022</td>\n",
              "      <td>46.322391</td>\n",
              "      <td>17.711957</td>\n",
              "      <td>28.610434</td>\n",
              "      <td>64.034349</td>\n",
              "      <td>116.075087</td>\n",
              "      <td>43.593124</td>\n",
              "      <td>72.481962</td>\n",
              "      <td>159.668211</td>\n",
              "      <td>51.430923</td>\n",
              "      <td>18.573016</td>\n",
              "      <td>32.857907</td>\n",
              "      <td>70.003940</td>\n",
              "      <td>40.365565</td>\n",
              "      <td>17.259087</td>\n",
              "      <td>23.106478</td>\n",
              "      <td>57.624652</td>\n",
              "      <td>102.641859</td>\n",
              "      <td>38.606995</td>\n",
              "      <td>64.034863</td>\n",
              "      <td>141.248854</td>\n",
              "      <td>50.805205</td>\n",
              "      <td>18.072101</td>\n",
              "      <td>32.733104</td>\n",
              "      <td>68.877306</td>\n",
              "      <td>5.177654</td>\n",
              "      <td>2.969214</td>\n",
              "      <td>156.242754</td>\n",
              "      <td>25.499379</td>\n",
              "      <td>116.081770</td>\n",
              "      <td>43.582671</td>\n",
              "      <td>5.071879</td>\n",
              "      <td>2.909002</td>\n",
              "      <td>158.343946</td>\n",
              "      <td>28.273928</td>\n",
              "      <td>102.648278</td>\n",
              "      <td>38.598300</td>\n",
              "      <td>0.951710</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.855409</td>\n",
              "      <td>0.599998</td>\n",
              "      <td>0.618140</td>\n",
              "      <td>0.830304</td>\n",
              "      <td>0.964611</td>\n",
              "      <td>0.532073</td>\n",
              "      <td>74.221670</td>\n",
              "      <td>0.347202</td>\n",
              "      <td>0.361672</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.390319</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.272884</td>\n",
              "      <td>0.373487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.071442</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>0.442831</td>\n",
              "      <td>37.552979</td>\n",
              "      <td>14.454975</td>\n",
              "      <td>23.098004</td>\n",
              "      <td>52.007953</td>\n",
              "      <td>101.044906</td>\n",
              "      <td>37.171973</td>\n",
              "      <td>63.872932</td>\n",
              "      <td>138.216879</td>\n",
              "      <td>53.971671</td>\n",
              "      <td>17.591669</td>\n",
              "      <td>36.380003</td>\n",
              "      <td>71.563340</td>\n",
              "      <td>40.717229</td>\n",
              "      <td>38.810699</td>\n",
              "      <td>1.906530</td>\n",
              "      <td>79.527927</td>\n",
              "      <td>97.446185</td>\n",
              "      <td>48.106253</td>\n",
              "      <td>49.339932</td>\n",
              "      <td>145.552438</td>\n",
              "      <td>51.733004</td>\n",
              "      <td>39.866956</td>\n",
              "      <td>11.866047</td>\n",
              "      <td>91.599960</td>\n",
              "      <td>4.978534</td>\n",
              "      <td>2.964685</td>\n",
              "      <td>158.160578</td>\n",
              "      <td>30.308891</td>\n",
              "      <td>101.050711</td>\n",
              "      <td>37.163383</td>\n",
              "      <td>5.051587</td>\n",
              "      <td>2.923540</td>\n",
              "      <td>157.131407</td>\n",
              "      <td>39.439642</td>\n",
              "      <td>97.457304</td>\n",
              "      <td>48.125747</td>\n",
              "      <td>0.955996</td>\n",
              "      <td>0.408286</td>\n",
              "      <td>0.882990</td>\n",
              "      <td>0.442043</td>\n",
              "      <td>0.623938</td>\n",
              "      <td>0.625574</td>\n",
              "      <td>0.957604</td>\n",
              "      <td>0.407600</td>\n",
              "      <td>61.546536</td>\n",
              "      <td>0.437852</td>\n",
              "      <td>0.673196</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.610160</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.298345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028185</td>\n",
              "      <td>0.046193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015983</td>\n",
              "      <td>0.104929</td>\n",
              "      <td>73.465862</td>\n",
              "      <td>33.856824</td>\n",
              "      <td>39.609038</td>\n",
              "      <td>107.322686</td>\n",
              "      <td>44.657984</td>\n",
              "      <td>35.514453</td>\n",
              "      <td>9.143531</td>\n",
              "      <td>80.172437</td>\n",
              "      <td>97.007389</td>\n",
              "      <td>67.135578</td>\n",
              "      <td>29.871812</td>\n",
              "      <td>164.142967</td>\n",
              "      <td>80.241955</td>\n",
              "      <td>32.698142</td>\n",
              "      <td>47.543813</td>\n",
              "      <td>112.940096</td>\n",
              "      <td>49.969255</td>\n",
              "      <td>32.728615</td>\n",
              "      <td>17.240641</td>\n",
              "      <td>82.697870</td>\n",
              "      <td>103.504549</td>\n",
              "      <td>66.143863</td>\n",
              "      <td>37.360686</td>\n",
              "      <td>169.648412</td>\n",
              "      <td>4.376708</td>\n",
              "      <td>2.426611</td>\n",
              "      <td>149.832364</td>\n",
              "      <td>34.523784</td>\n",
              "      <td>107.238951</td>\n",
              "      <td>58.226891</td>\n",
              "      <td>4.243756</td>\n",
              "      <td>2.459024</td>\n",
              "      <td>141.193904</td>\n",
              "      <td>33.987154</td>\n",
              "      <td>112.817594</td>\n",
              "      <td>57.899252</td>\n",
              "      <td>0.951236</td>\n",
              "      <td>0.641439</td>\n",
              "      <td>0.681541</td>\n",
              "      <td>0.895266</td>\n",
              "      <td>0.559964</td>\n",
              "      <td>1.089641</td>\n",
              "      <td>0.916873</td>\n",
              "      <td>0.665480</td>\n",
              "      <td>141.328331</td>\n",
              "      <td>0.356888</td>\n",
              "      <td>0.353911</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.439891</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>0.031853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.458246</td>\n",
              "      <td>36.279936</td>\n",
              "      <td>23.325050</td>\n",
              "      <td>12.954885</td>\n",
              "      <td>59.604986</td>\n",
              "      <td>31.549222</td>\n",
              "      <td>23.099706</td>\n",
              "      <td>8.449515</td>\n",
              "      <td>54.648928</td>\n",
              "      <td>39.162020</td>\n",
              "      <td>33.679494</td>\n",
              "      <td>5.482527</td>\n",
              "      <td>72.841514</td>\n",
              "      <td>37.256992</td>\n",
              "      <td>27.997780</td>\n",
              "      <td>9.259212</td>\n",
              "      <td>65.254772</td>\n",
              "      <td>32.188663</td>\n",
              "      <td>27.246836</td>\n",
              "      <td>4.941828</td>\n",
              "      <td>59.435499</td>\n",
              "      <td>38.946898</td>\n",
              "      <td>37.576950</td>\n",
              "      <td>1.369948</td>\n",
              "      <td>76.523849</td>\n",
              "      <td>4.047217</td>\n",
              "      <td>2.068656</td>\n",
              "      <td>85.995172</td>\n",
              "      <td>37.051364</td>\n",
              "      <td>44.437573</td>\n",
              "      <td>31.678540</td>\n",
              "      <td>5.053266</td>\n",
              "      <td>1.795977</td>\n",
              "      <td>86.474830</td>\n",
              "      <td>39.921044</td>\n",
              "      <td>45.112433</td>\n",
              "      <td>35.381825</td>\n",
              "      <td>0.988825</td>\n",
              "      <td>0.444862</td>\n",
              "      <td>0.810836</td>\n",
              "      <td>0.542516</td>\n",
              "      <td>0.667897</td>\n",
              "      <td>0.658622</td>\n",
              "      <td>1.001402</td>\n",
              "      <td>0.439276</td>\n",
              "      <td>59.687772</td>\n",
              "      <td>0.529688</td>\n",
              "      <td>0.417762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.696616</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.169087</td>\n",
              "      <td>0.146312</td>\n",
              "      <td>0.017217</td>\n",
              "      <td>0.024715</td>\n",
              "      <td>0.078889</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>0.038447</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.342323</td>\n",
              "      <td>0.060042</td>\n",
              "      <td>0.301857</td>\n",
              "      <td>74.766472</td>\n",
              "      <td>33.667006</td>\n",
              "      <td>41.099466</td>\n",
              "      <td>108.433478</td>\n",
              "      <td>44.113166</td>\n",
              "      <td>29.555688</td>\n",
              "      <td>14.557478</td>\n",
              "      <td>73.668854</td>\n",
              "      <td>68.044433</td>\n",
              "      <td>42.402254</td>\n",
              "      <td>25.642180</td>\n",
              "      <td>110.446687</td>\n",
              "      <td>95.119483</td>\n",
              "      <td>56.497295</td>\n",
              "      <td>38.622189</td>\n",
              "      <td>151.616778</td>\n",
              "      <td>62.400161</td>\n",
              "      <td>54.006420</td>\n",
              "      <td>8.393741</td>\n",
              "      <td>116.406581</td>\n",
              "      <td>104.351296</td>\n",
              "      <td>74.683000</td>\n",
              "      <td>29.668296</td>\n",
              "      <td>179.034296</td>\n",
              "      <td>3.439162</td>\n",
              "      <td>2.374293</td>\n",
              "      <td>116.377066</td>\n",
              "      <td>29.226454</td>\n",
              "      <td>81.122596</td>\n",
              "      <td>38.389404</td>\n",
              "      <td>3.774469</td>\n",
              "      <td>2.431975</td>\n",
              "      <td>117.490035</td>\n",
              "      <td>41.194225</td>\n",
              "      <td>114.484831</td>\n",
              "      <td>69.298856</td>\n",
              "      <td>0.977561</td>\n",
              "      <td>0.712606</td>\n",
              "      <td>0.830197</td>\n",
              "      <td>0.839097</td>\n",
              "      <td>0.534820</td>\n",
              "      <td>1.302525</td>\n",
              "      <td>1.020938</td>\n",
              "      <td>0.682329</td>\n",
              "      <td>196.767870</td>\n",
              "      <td>0.453209</td>\n",
              "      <td>0.590185</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026749</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.453958</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>0.220804</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cervix_area   os_area  walls_area  ...  experts::4  experts::5  consensus\n",
              "0       0.344647  0.003080    0.047522  ...         1.0         1.0        1.0\n",
              "1       0.165329  0.000000    0.048236  ...         0.0         0.0        0.0\n",
              "2       0.457010  0.001681    0.242888  ...         0.0         0.0        0.0\n",
              "3       0.513244  0.005711    0.213781  ...         1.0         1.0        1.0\n",
              "4       0.390319  0.009454    0.272884  ...         1.0         1.0        1.0\n",
              "..           ...       ...         ...  ...         ...         ...        ...\n",
              "282     0.610160  0.002726    0.298345  ...         0.0         0.0        0.0\n",
              "283     0.439891  0.006005    0.000000  ...         0.0         0.0        1.0\n",
              "284     0.696616  0.006693    0.169087  ...         1.0         0.0        1.0\n",
              "285     1.000000  0.000000    0.000000  ...         0.0         0.0        0.0\n",
              "286     1.000000  0.007517    0.000000  ...         0.0         0.0        0.0\n",
              "\n",
              "[287 rows x 69 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_deFZG7YvKWh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1.   Το dataset περιγράφει μετρήσεις κολποσκοπήσεων και την κατάσταση (ετικέτα) των αντίστοιχων κόλπων, όπως προκύπτει από τις εκτιμήσεις καθενός από 6 ειδικούς και την εκτίμηση της πλειοψηφίας. Οι μετρήσεις και οι εκτιμήσεις είναι διαφορετικές για τις διαφορετικές μεθόδους με τις οποίες πραγματοποιούνται οι κολποσκοπήσεις: Hinselmann, Green, Schiller.\n",
        "\n",
        "\n",
        "\n",
        "2.    Έχουμε συνολικά 287 δείγματα, 92 από τη μέθοδο Schiller, 98 από τη μέθοδο Green και 97 από τη μέθοδο Hinselmann. Σε κάθε δείγμα περιέχονται μετρήσεις για 62 χαρακτηριστικά-παρατηρήσεις από τα οποία προέκυψαν οι εκτιμήσεις. Όλα τα χαρακτηριστικά είναι διατεταγμένα.Οι μετρήσεις είναι αριθμητικά δεδομένα και αφορούν τιμές όπως τις επιφάνειες περιοχών του κόπλου.\n",
        "\n",
        "3.    Υπάρχουν επικεφαλίδες στην πρώτη γραμμή πάνω από τα χαρακτηριστικά και τις ετικέτες, οι οποίες θα πρέπει να αφαιρεθούν. Δεν υπάρχει στήλη για την αρίθμηση των γραμμών.\n",
        "\n",
        "4.    Η τιμή της κατάστασης μπορεί να πάρει δύο τιμές, 0 για κακή και 1 για καλή. Όπως υποδεικνύεται από τις FAQ το πρόβλημα θα αναλυθεί ως binary classification λαμβάνοντας υπόψην μόνο τις εκτιμήσεις της πλειοψηφίας. Έτσι οι στήλες των ετικετών που αφορούν μεμονωμένα τον κάθε ειδικό αφαιρούνται και μένει μόνο η τελευταία στήλη με τις εκτιμήσεις της πλειοψηφίας. Η στήλη αυτή είναι η τελευταία (θέση 69 στον αρχικό πίνακα). Παρατηρούμε οτι όλες οι ισοψηφίες (3-3) επιλύονται θεωρώντας την κατάσταση ως καλή, γεγονός που ίσως μας αναγκάσει να αυξήσουμε αργότερα το πλήθος των δειγμάτων με ετικέτα 0 ή να διαγράψουμε δείγματα με ετικέτα 1.\n",
        "\n",
        "    \n",
        "5.    Συνενώνουμε τα αρχεία που αφορούν τις 3 διαφορετικές μεθόδους. Μετά τη συνένωση οι επικεφαλίδες προστέθηκαν ως γραμμές, με αποτέλεσμα να έχουμε 3 φορές την ίδια γραμμή με τις επικεφαλίδες, τις οποίες και αφαιρούμε.\n",
        "\n",
        "6.    Δεν υπάρχουν απουσιάζουσες τιμές.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0110eLi0kT7",
        "colab_type": "text"
      },
      "source": [
        "Ας τυπώσουμε τον αριθμό, τα ονόματα και τους τύπους του κάθε attribute, για να τα εξετάσουμε καλύτερα:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ1Qrlcl0h_t",
        "colab_type": "code",
        "outputId": "b7c2b0e1-e20b-4cfb-f6c3-c01fdaff8bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"We have \",len(df.columns), \" attributes.\")\n",
        "for i in range(0, len(df.columns)):\n",
        "    print('{:<10}{:<40}{:<10}{:<20}'.format(str(i+1), str(df.columns[i]),\"type: \", str(df.dtypes[df.columns[i]])))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have  69  attributes.\n",
            "1         cervix_area                             type:     float64             \n",
            "2         os_area                                 type:     float64             \n",
            "3         walls_area                              type:     float64             \n",
            "4         speculum_area                           type:     float64             \n",
            "5         artifacts_area                          type:     float64             \n",
            "6         cervix_artifacts_area                   type:     float64             \n",
            "7         os_artifacts_area                       type:     float64             \n",
            "8         walls_artifacts_area                    type:     float64             \n",
            "9         speculum_artifacts_area                 type:     float64             \n",
            "10        cervix_specularities_area               type:     float64             \n",
            "11        os_specularities_area                   type:     float64             \n",
            "12        walls_specularities_area                type:     float64             \n",
            "13        speculum_specularities_area             type:     float64             \n",
            "14        specularities_area                      type:     float64             \n",
            "15        area_h_max_diff                         type:     float64             \n",
            "16        rgb_cervix_r_mean                       type:     float64             \n",
            "17        rgb_cervix_r_std                        type:     float64             \n",
            "18        rgb_cervix_r_mean_minus_std             type:     float64             \n",
            "19        rgb_cervix_r_mean_plus_std              type:     float64             \n",
            "20        rgb_cervix_g_mean                       type:     float64             \n",
            "21        rgb_cervix_g_std                        type:     float64             \n",
            "22        rgb_cervix_g_mean_minus_std             type:     float64             \n",
            "23        rgb_cervix_g_mean_plus_std              type:     float64             \n",
            "24        rgb_cervix_b_mean                       type:     float64             \n",
            "25        rgb_cervix_b_std                        type:     float64             \n",
            "26        rgb_cervix_b_mean_minus_std             type:     float64             \n",
            "27        rgb_cervix_b_mean_plus_std              type:     float64             \n",
            "28        rgb_total_r_mean                        type:     float64             \n",
            "29        rgb_total_r_std                         type:     float64             \n",
            "30        rgb_total_r_mean_minus_std              type:     float64             \n",
            "31        rgb_total_r_mean_plus_std               type:     float64             \n",
            "32        rgb_total_g_mean                        type:     float64             \n",
            "33        rgb_total_g_std                         type:     float64             \n",
            "34        rgb_total_g_mean_minus_std              type:     float64             \n",
            "35        rgb_total_g_mean_plus_std               type:     float64             \n",
            "36        rgb_total_b_mean                        type:     float64             \n",
            "37        rgb_total_b_std                         type:     float64             \n",
            "38        rgb_total_b_mean_minus_std              type:     float64             \n",
            "39        rgb_total_b_mean_plus_std               type:     float64             \n",
            "40        hsv_cervix_h_mean                       type:     float64             \n",
            "41        hsv_cervix_h_std                        type:     float64             \n",
            "42        hsv_cervix_s_mean                       type:     float64             \n",
            "43        hsv_cervix_s_std                        type:     float64             \n",
            "44        hsv_cervix_v_mean                       type:     float64             \n",
            "45        hsv_cervix_v_std                        type:     float64             \n",
            "46        hsv_total_h_mean                        type:     float64             \n",
            "47        hsv_total_h_std                         type:     float64             \n",
            "48        hsv_total_s_mean                        type:     float64             \n",
            "49        hsv_total_s_std                         type:     float64             \n",
            "50        hsv_total_v_mean                        type:     float64             \n",
            "51        hsv_total_v_std                         type:     float64             \n",
            "52        fit_cervix_hull_rate                    type:     float64             \n",
            "53        fit_cervix_hull_total                   type:     float64             \n",
            "54        fit_cervix_bbox_rate                    type:     float64             \n",
            "55        fit_cervix_bbox_total                   type:     float64             \n",
            "56        fit_circle_rate                         type:     float64             \n",
            "57        fit_circle_total                        type:     float64             \n",
            "58        fit_ellipse_rate                        type:     float64             \n",
            "59        fit_ellipse_total                       type:     float64             \n",
            "60        fit_ellipse_goodness                    type:     float64             \n",
            "61        dist_to_center_cervix                   type:     float64             \n",
            "62        dist_to_center_os                       type:     float64             \n",
            "63        experts::0                              type:     float64             \n",
            "64        experts::1                              type:     float64             \n",
            "65        experts::2                              type:     float64             \n",
            "66        experts::3                              type:     float64             \n",
            "67        experts::4                              type:     float64             \n",
            "68        experts::5                              type:     float64             \n",
            "69        consensus                               type:     float64             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSAYGUotwWE_",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι έχουμε 5 experts, αλλά εμείς θα δουλέψουμε μόνο με την συνολική άποψη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab4WaECKwgnl",
        "colab_type": "code",
        "outputId": "d0afdb20-84b8-44b5-f37f-5dcfa30afd3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df = df.drop(df.columns[[62, 63, 64, 65, 66, 67]], axis=1)\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cervix_area</th>\n",
              "      <th>os_area</th>\n",
              "      <th>walls_area</th>\n",
              "      <th>speculum_area</th>\n",
              "      <th>artifacts_area</th>\n",
              "      <th>cervix_artifacts_area</th>\n",
              "      <th>os_artifacts_area</th>\n",
              "      <th>walls_artifacts_area</th>\n",
              "      <th>speculum_artifacts_area</th>\n",
              "      <th>cervix_specularities_area</th>\n",
              "      <th>os_specularities_area</th>\n",
              "      <th>walls_specularities_area</th>\n",
              "      <th>speculum_specularities_area</th>\n",
              "      <th>specularities_area</th>\n",
              "      <th>area_h_max_diff</th>\n",
              "      <th>rgb_cervix_r_mean</th>\n",
              "      <th>rgb_cervix_r_std</th>\n",
              "      <th>rgb_cervix_r_mean_minus_std</th>\n",
              "      <th>rgb_cervix_r_mean_plus_std</th>\n",
              "      <th>rgb_cervix_g_mean</th>\n",
              "      <th>rgb_cervix_g_std</th>\n",
              "      <th>rgb_cervix_g_mean_minus_std</th>\n",
              "      <th>rgb_cervix_g_mean_plus_std</th>\n",
              "      <th>rgb_cervix_b_mean</th>\n",
              "      <th>rgb_cervix_b_std</th>\n",
              "      <th>rgb_cervix_b_mean_minus_std</th>\n",
              "      <th>rgb_cervix_b_mean_plus_std</th>\n",
              "      <th>rgb_total_r_mean</th>\n",
              "      <th>rgb_total_r_std</th>\n",
              "      <th>rgb_total_r_mean_minus_std</th>\n",
              "      <th>rgb_total_r_mean_plus_std</th>\n",
              "      <th>rgb_total_g_mean</th>\n",
              "      <th>rgb_total_g_std</th>\n",
              "      <th>rgb_total_g_mean_minus_std</th>\n",
              "      <th>rgb_total_g_mean_plus_std</th>\n",
              "      <th>rgb_total_b_mean</th>\n",
              "      <th>rgb_total_b_std</th>\n",
              "      <th>rgb_total_b_mean_minus_std</th>\n",
              "      <th>rgb_total_b_mean_plus_std</th>\n",
              "      <th>hsv_cervix_h_mean</th>\n",
              "      <th>hsv_cervix_h_std</th>\n",
              "      <th>hsv_cervix_s_mean</th>\n",
              "      <th>hsv_cervix_s_std</th>\n",
              "      <th>hsv_cervix_v_mean</th>\n",
              "      <th>hsv_cervix_v_std</th>\n",
              "      <th>hsv_total_h_mean</th>\n",
              "      <th>hsv_total_h_std</th>\n",
              "      <th>hsv_total_s_mean</th>\n",
              "      <th>hsv_total_s_std</th>\n",
              "      <th>hsv_total_v_mean</th>\n",
              "      <th>hsv_total_v_std</th>\n",
              "      <th>fit_cervix_hull_rate</th>\n",
              "      <th>fit_cervix_hull_total</th>\n",
              "      <th>fit_cervix_bbox_rate</th>\n",
              "      <th>fit_cervix_bbox_total</th>\n",
              "      <th>fit_circle_rate</th>\n",
              "      <th>fit_circle_total</th>\n",
              "      <th>fit_ellipse_rate</th>\n",
              "      <th>fit_ellipse_total</th>\n",
              "      <th>fit_ellipse_goodness</th>\n",
              "      <th>dist_to_center_cervix</th>\n",
              "      <th>dist_to_center_os</th>\n",
              "      <th>consensus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.344647</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>0.047522</td>\n",
              "      <td>0.288216</td>\n",
              "      <td>0.178585</td>\n",
              "      <td>0.016564</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043500</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.085833</td>\n",
              "      <td>0.024907</td>\n",
              "      <td>0.263560</td>\n",
              "      <td>37.594458</td>\n",
              "      <td>15.785021</td>\n",
              "      <td>21.809437</td>\n",
              "      <td>53.379479</td>\n",
              "      <td>109.918445</td>\n",
              "      <td>38.735421</td>\n",
              "      <td>71.183024</td>\n",
              "      <td>148.653865</td>\n",
              "      <td>55.029618</td>\n",
              "      <td>22.160330</td>\n",
              "      <td>32.869287</td>\n",
              "      <td>77.189948</td>\n",
              "      <td>38.561367</td>\n",
              "      <td>38.119059</td>\n",
              "      <td>0.442308</td>\n",
              "      <td>76.680426</td>\n",
              "      <td>95.109755</td>\n",
              "      <td>51.565052</td>\n",
              "      <td>43.544702</td>\n",
              "      <td>146.674807</td>\n",
              "      <td>48.808474</td>\n",
              "      <td>40.765228</td>\n",
              "      <td>8.043247</td>\n",
              "      <td>89.573702</td>\n",
              "      <td>5.014628</td>\n",
              "      <td>2.991944</td>\n",
              "      <td>167.952780</td>\n",
              "      <td>25.813163</td>\n",
              "      <td>109.919447</td>\n",
              "      <td>38.733741</td>\n",
              "      <td>5.090801</td>\n",
              "      <td>2.936650</td>\n",
              "      <td>159.486916</td>\n",
              "      <td>38.437294</td>\n",
              "      <td>95.123889</td>\n",
              "      <td>51.583029</td>\n",
              "      <td>0.923067</td>\n",
              "      <td>0.373371</td>\n",
              "      <td>0.844454</td>\n",
              "      <td>0.408130</td>\n",
              "      <td>0.603399</td>\n",
              "      <td>0.571175</td>\n",
              "      <td>0.962995</td>\n",
              "      <td>0.357890</td>\n",
              "      <td>85.474311</td>\n",
              "      <td>0.265933</td>\n",
              "      <td>0.346294</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.165329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048236</td>\n",
              "      <td>0.504736</td>\n",
              "      <td>0.502783</td>\n",
              "      <td>0.007012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097405</td>\n",
              "      <td>0.973837</td>\n",
              "      <td>0.004055</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054999</td>\n",
              "      <td>0.028431</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.505882</td>\n",
              "      <td>24.361877</td>\n",
              "      <td>35.144005</td>\n",
              "      <td>83.867760</td>\n",
              "      <td>122.366075</td>\n",
              "      <td>44.742407</td>\n",
              "      <td>77.623669</td>\n",
              "      <td>167.108482</td>\n",
              "      <td>78.058434</td>\n",
              "      <td>30.818729</td>\n",
              "      <td>47.239706</td>\n",
              "      <td>108.877163</td>\n",
              "      <td>54.932467</td>\n",
              "      <td>39.447415</td>\n",
              "      <td>15.485052</td>\n",
              "      <td>94.379883</td>\n",
              "      <td>101.680459</td>\n",
              "      <td>46.028852</td>\n",
              "      <td>55.651607</td>\n",
              "      <td>147.709311</td>\n",
              "      <td>63.218931</td>\n",
              "      <td>43.925912</td>\n",
              "      <td>19.293019</td>\n",
              "      <td>107.144843</td>\n",
              "      <td>4.944382</td>\n",
              "      <td>2.965108</td>\n",
              "      <td>130.260492</td>\n",
              "      <td>24.143867</td>\n",
              "      <td>122.366647</td>\n",
              "      <td>44.743932</td>\n",
              "      <td>5.080063</td>\n",
              "      <td>2.894163</td>\n",
              "      <td>128.251978</td>\n",
              "      <td>33.000693</td>\n",
              "      <td>101.725519</td>\n",
              "      <td>46.093510</td>\n",
              "      <td>0.850861</td>\n",
              "      <td>0.194308</td>\n",
              "      <td>0.646645</td>\n",
              "      <td>0.255673</td>\n",
              "      <td>0.497315</td>\n",
              "      <td>0.332444</td>\n",
              "      <td>0.894625</td>\n",
              "      <td>0.184803</td>\n",
              "      <td>124.794129</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.283059</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.457010</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.242888</td>\n",
              "      <td>0.212859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001756</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083055</td>\n",
              "      <td>0.018591</td>\n",
              "      <td>0.269798</td>\n",
              "      <td>39.353851</td>\n",
              "      <td>19.417332</td>\n",
              "      <td>19.936519</td>\n",
              "      <td>58.771183</td>\n",
              "      <td>109.543386</td>\n",
              "      <td>49.753490</td>\n",
              "      <td>59.789896</td>\n",
              "      <td>159.296876</td>\n",
              "      <td>54.642888</td>\n",
              "      <td>27.781965</td>\n",
              "      <td>26.860923</td>\n",
              "      <td>82.424853</td>\n",
              "      <td>41.242230</td>\n",
              "      <td>34.196356</td>\n",
              "      <td>7.045875</td>\n",
              "      <td>75.438586</td>\n",
              "      <td>109.592342</td>\n",
              "      <td>57.576640</td>\n",
              "      <td>52.015702</td>\n",
              "      <td>167.168982</td>\n",
              "      <td>53.470241</td>\n",
              "      <td>38.344391</td>\n",
              "      <td>15.125850</td>\n",
              "      <td>91.814632</td>\n",
              "      <td>5.049946</td>\n",
              "      <td>2.983966</td>\n",
              "      <td>163.576979</td>\n",
              "      <td>24.973042</td>\n",
              "      <td>109.544201</td>\n",
              "      <td>49.753659</td>\n",
              "      <td>5.078936</td>\n",
              "      <td>2.968023</td>\n",
              "      <td>162.268659</td>\n",
              "      <td>33.590792</td>\n",
              "      <td>109.597127</td>\n",
              "      <td>57.584515</td>\n",
              "      <td>0.918514</td>\n",
              "      <td>0.497554</td>\n",
              "      <td>0.747443</td>\n",
              "      <td>0.611432</td>\n",
              "      <td>0.633925</td>\n",
              "      <td>0.720923</td>\n",
              "      <td>0.920287</td>\n",
              "      <td>0.496596</td>\n",
              "      <td>94.948697</td>\n",
              "      <td>0.518740</td>\n",
              "      <td>0.419375</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.513244</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.213781</td>\n",
              "      <td>0.251819</td>\n",
              "      <td>0.079795</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017594</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.107022</td>\n",
              "      <td>46.322391</td>\n",
              "      <td>17.711957</td>\n",
              "      <td>28.610434</td>\n",
              "      <td>64.034349</td>\n",
              "      <td>116.075087</td>\n",
              "      <td>43.593124</td>\n",
              "      <td>72.481962</td>\n",
              "      <td>159.668211</td>\n",
              "      <td>51.430923</td>\n",
              "      <td>18.573016</td>\n",
              "      <td>32.857907</td>\n",
              "      <td>70.003940</td>\n",
              "      <td>40.365565</td>\n",
              "      <td>17.259087</td>\n",
              "      <td>23.106478</td>\n",
              "      <td>57.624652</td>\n",
              "      <td>102.641859</td>\n",
              "      <td>38.606995</td>\n",
              "      <td>64.034863</td>\n",
              "      <td>141.248854</td>\n",
              "      <td>50.805205</td>\n",
              "      <td>18.072101</td>\n",
              "      <td>32.733104</td>\n",
              "      <td>68.877306</td>\n",
              "      <td>5.177654</td>\n",
              "      <td>2.969214</td>\n",
              "      <td>156.242754</td>\n",
              "      <td>25.499379</td>\n",
              "      <td>116.081770</td>\n",
              "      <td>43.582671</td>\n",
              "      <td>5.071879</td>\n",
              "      <td>2.909002</td>\n",
              "      <td>158.343946</td>\n",
              "      <td>28.273928</td>\n",
              "      <td>102.648278</td>\n",
              "      <td>38.598300</td>\n",
              "      <td>0.951710</td>\n",
              "      <td>0.539286</td>\n",
              "      <td>0.855409</td>\n",
              "      <td>0.599998</td>\n",
              "      <td>0.618140</td>\n",
              "      <td>0.830304</td>\n",
              "      <td>0.964611</td>\n",
              "      <td>0.532073</td>\n",
              "      <td>74.221670</td>\n",
              "      <td>0.347202</td>\n",
              "      <td>0.361672</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.390319</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.272884</td>\n",
              "      <td>0.373487</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.071442</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>0.442831</td>\n",
              "      <td>37.552979</td>\n",
              "      <td>14.454975</td>\n",
              "      <td>23.098004</td>\n",
              "      <td>52.007953</td>\n",
              "      <td>101.044906</td>\n",
              "      <td>37.171973</td>\n",
              "      <td>63.872932</td>\n",
              "      <td>138.216879</td>\n",
              "      <td>53.971671</td>\n",
              "      <td>17.591669</td>\n",
              "      <td>36.380003</td>\n",
              "      <td>71.563340</td>\n",
              "      <td>40.717229</td>\n",
              "      <td>38.810699</td>\n",
              "      <td>1.906530</td>\n",
              "      <td>79.527927</td>\n",
              "      <td>97.446185</td>\n",
              "      <td>48.106253</td>\n",
              "      <td>49.339932</td>\n",
              "      <td>145.552438</td>\n",
              "      <td>51.733004</td>\n",
              "      <td>39.866956</td>\n",
              "      <td>11.866047</td>\n",
              "      <td>91.599960</td>\n",
              "      <td>4.978534</td>\n",
              "      <td>2.964685</td>\n",
              "      <td>158.160578</td>\n",
              "      <td>30.308891</td>\n",
              "      <td>101.050711</td>\n",
              "      <td>37.163383</td>\n",
              "      <td>5.051587</td>\n",
              "      <td>2.923540</td>\n",
              "      <td>157.131407</td>\n",
              "      <td>39.439642</td>\n",
              "      <td>97.457304</td>\n",
              "      <td>48.125747</td>\n",
              "      <td>0.955996</td>\n",
              "      <td>0.408286</td>\n",
              "      <td>0.882990</td>\n",
              "      <td>0.442043</td>\n",
              "      <td>0.623938</td>\n",
              "      <td>0.625574</td>\n",
              "      <td>0.957604</td>\n",
              "      <td>0.407600</td>\n",
              "      <td>61.546536</td>\n",
              "      <td>0.437852</td>\n",
              "      <td>0.673196</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.610160</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.298345</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028185</td>\n",
              "      <td>0.046193</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015983</td>\n",
              "      <td>0.104929</td>\n",
              "      <td>73.465862</td>\n",
              "      <td>33.856824</td>\n",
              "      <td>39.609038</td>\n",
              "      <td>107.322686</td>\n",
              "      <td>44.657984</td>\n",
              "      <td>35.514453</td>\n",
              "      <td>9.143531</td>\n",
              "      <td>80.172437</td>\n",
              "      <td>97.007389</td>\n",
              "      <td>67.135578</td>\n",
              "      <td>29.871812</td>\n",
              "      <td>164.142967</td>\n",
              "      <td>80.241955</td>\n",
              "      <td>32.698142</td>\n",
              "      <td>47.543813</td>\n",
              "      <td>112.940096</td>\n",
              "      <td>49.969255</td>\n",
              "      <td>32.728615</td>\n",
              "      <td>17.240641</td>\n",
              "      <td>82.697870</td>\n",
              "      <td>103.504549</td>\n",
              "      <td>66.143863</td>\n",
              "      <td>37.360686</td>\n",
              "      <td>169.648412</td>\n",
              "      <td>4.376708</td>\n",
              "      <td>2.426611</td>\n",
              "      <td>149.832364</td>\n",
              "      <td>34.523784</td>\n",
              "      <td>107.238951</td>\n",
              "      <td>58.226891</td>\n",
              "      <td>4.243756</td>\n",
              "      <td>2.459024</td>\n",
              "      <td>141.193904</td>\n",
              "      <td>33.987154</td>\n",
              "      <td>112.817594</td>\n",
              "      <td>57.899252</td>\n",
              "      <td>0.951236</td>\n",
              "      <td>0.641439</td>\n",
              "      <td>0.681541</td>\n",
              "      <td>0.895266</td>\n",
              "      <td>0.559964</td>\n",
              "      <td>1.089641</td>\n",
              "      <td>0.916873</td>\n",
              "      <td>0.665480</td>\n",
              "      <td>141.328331</td>\n",
              "      <td>0.356888</td>\n",
              "      <td>0.353911</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.439891</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014012</td>\n",
              "      <td>0.031853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011085</td>\n",
              "      <td>0.458246</td>\n",
              "      <td>36.279936</td>\n",
              "      <td>23.325050</td>\n",
              "      <td>12.954885</td>\n",
              "      <td>59.604986</td>\n",
              "      <td>31.549222</td>\n",
              "      <td>23.099706</td>\n",
              "      <td>8.449515</td>\n",
              "      <td>54.648928</td>\n",
              "      <td>39.162020</td>\n",
              "      <td>33.679494</td>\n",
              "      <td>5.482527</td>\n",
              "      <td>72.841514</td>\n",
              "      <td>37.256992</td>\n",
              "      <td>27.997780</td>\n",
              "      <td>9.259212</td>\n",
              "      <td>65.254772</td>\n",
              "      <td>32.188663</td>\n",
              "      <td>27.246836</td>\n",
              "      <td>4.941828</td>\n",
              "      <td>59.435499</td>\n",
              "      <td>38.946898</td>\n",
              "      <td>37.576950</td>\n",
              "      <td>1.369948</td>\n",
              "      <td>76.523849</td>\n",
              "      <td>4.047217</td>\n",
              "      <td>2.068656</td>\n",
              "      <td>85.995172</td>\n",
              "      <td>37.051364</td>\n",
              "      <td>44.437573</td>\n",
              "      <td>31.678540</td>\n",
              "      <td>5.053266</td>\n",
              "      <td>1.795977</td>\n",
              "      <td>86.474830</td>\n",
              "      <td>39.921044</td>\n",
              "      <td>45.112433</td>\n",
              "      <td>35.381825</td>\n",
              "      <td>0.988825</td>\n",
              "      <td>0.444862</td>\n",
              "      <td>0.810836</td>\n",
              "      <td>0.542516</td>\n",
              "      <td>0.667897</td>\n",
              "      <td>0.658622</td>\n",
              "      <td>1.001402</td>\n",
              "      <td>0.439276</td>\n",
              "      <td>59.687772</td>\n",
              "      <td>0.529688</td>\n",
              "      <td>0.417762</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>0.696616</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.169087</td>\n",
              "      <td>0.146312</td>\n",
              "      <td>0.017217</td>\n",
              "      <td>0.024715</td>\n",
              "      <td>0.078889</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>0.038447</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.342323</td>\n",
              "      <td>0.060042</td>\n",
              "      <td>0.301857</td>\n",
              "      <td>74.766472</td>\n",
              "      <td>33.667006</td>\n",
              "      <td>41.099466</td>\n",
              "      <td>108.433478</td>\n",
              "      <td>44.113166</td>\n",
              "      <td>29.555688</td>\n",
              "      <td>14.557478</td>\n",
              "      <td>73.668854</td>\n",
              "      <td>68.044433</td>\n",
              "      <td>42.402254</td>\n",
              "      <td>25.642180</td>\n",
              "      <td>110.446687</td>\n",
              "      <td>95.119483</td>\n",
              "      <td>56.497295</td>\n",
              "      <td>38.622189</td>\n",
              "      <td>151.616778</td>\n",
              "      <td>62.400161</td>\n",
              "      <td>54.006420</td>\n",
              "      <td>8.393741</td>\n",
              "      <td>116.406581</td>\n",
              "      <td>104.351296</td>\n",
              "      <td>74.683000</td>\n",
              "      <td>29.668296</td>\n",
              "      <td>179.034296</td>\n",
              "      <td>3.439162</td>\n",
              "      <td>2.374293</td>\n",
              "      <td>116.377066</td>\n",
              "      <td>29.226454</td>\n",
              "      <td>81.122596</td>\n",
              "      <td>38.389404</td>\n",
              "      <td>3.774469</td>\n",
              "      <td>2.431975</td>\n",
              "      <td>117.490035</td>\n",
              "      <td>41.194225</td>\n",
              "      <td>114.484831</td>\n",
              "      <td>69.298856</td>\n",
              "      <td>0.977561</td>\n",
              "      <td>0.712606</td>\n",
              "      <td>0.830197</td>\n",
              "      <td>0.839097</td>\n",
              "      <td>0.534820</td>\n",
              "      <td>1.302525</td>\n",
              "      <td>1.020938</td>\n",
              "      <td>0.682329</td>\n",
              "      <td>196.767870</td>\n",
              "      <td>0.453209</td>\n",
              "      <td>0.590185</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.005578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>87.554770</td>\n",
              "      <td>18.588586</td>\n",
              "      <td>68.966184</td>\n",
              "      <td>106.143357</td>\n",
              "      <td>52.509842</td>\n",
              "      <td>15.476167</td>\n",
              "      <td>37.033676</td>\n",
              "      <td>67.986009</td>\n",
              "      <td>90.324298</td>\n",
              "      <td>41.026944</td>\n",
              "      <td>49.297354</td>\n",
              "      <td>131.351243</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>3.829996</td>\n",
              "      <td>2.506680</td>\n",
              "      <td>118.684142</td>\n",
              "      <td>31.088448</td>\n",
              "      <td>102.074904</td>\n",
              "      <td>33.479112</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007517</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026749</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.453958</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>37.259466</td>\n",
              "      <td>29.718387</td>\n",
              "      <td>7.541079</td>\n",
              "      <td>66.977852</td>\n",
              "      <td>39.275293</td>\n",
              "      <td>27.840955</td>\n",
              "      <td>11.434338</td>\n",
              "      <td>67.116248</td>\n",
              "      <td>39.854648</td>\n",
              "      <td>31.458597</td>\n",
              "      <td>8.396051</td>\n",
              "      <td>71.313246</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>5.063514</td>\n",
              "      <td>2.037802</td>\n",
              "      <td>72.306876</td>\n",
              "      <td>39.600756</td>\n",
              "      <td>45.857621</td>\n",
              "      <td>32.562950</td>\n",
              "      <td>1.007190</td>\n",
              "      <td>0.992861</td>\n",
              "      <td>1.003585</td>\n",
              "      <td>0.996427</td>\n",
              "      <td>0.556931</td>\n",
              "      <td>1.795553</td>\n",
              "      <td>1.062704</td>\n",
              "      <td>0.940996</td>\n",
              "      <td>356.792378</td>\n",
              "      <td>0.220804</td>\n",
              "      <td>0.402563</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 63 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     cervix_area   os_area  ...  dist_to_center_os  consensus\n",
              "0       0.344647  0.003080  ...           0.346294        1.0\n",
              "1       0.165329  0.000000  ...           0.283059        0.0\n",
              "2       0.457010  0.001681  ...           0.419375        0.0\n",
              "3       0.513244  0.005711  ...           0.361672        1.0\n",
              "4       0.390319  0.009454  ...           0.673196        1.0\n",
              "..           ...       ...  ...                ...        ...\n",
              "282     0.610160  0.002726  ...           0.353911        0.0\n",
              "283     0.439891  0.006005  ...           0.417762        1.0\n",
              "284     0.696616  0.006693  ...           0.590185        1.0\n",
              "285     1.000000  0.000000  ...           0.402563        0.0\n",
              "286     1.000000  0.007517  ...           0.402563        0.0\n",
              "\n",
              "[287 rows x 63 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFW2ZAJs6pIz",
        "colab_type": "text"
      },
      "source": [
        "Ας μετρήσουμε τώρα, πόσοι είναι υγιείς και πόσοι όχι, και θα το αποθηκεύσουμε αυτό σαν τα labels μας (βεβαίως, θα πρέπει μετά να αφαιρέσουμε αυτήν την κολώνα χαρακτηριστικών, πριν κάνουμε training).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2757HtVM6vna",
        "colab_type": "code",
        "outputId": "5116b996-df8f-4d4a-f1c7-ae9c9467af45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels_df = df.iloc[:, [62]]\n",
        "labels = labels_df.values.reshape(287,)\n",
        "positive = 0\n",
        "negative = 0\n",
        "for i in range(0, len(labels)):\n",
        "    positive += labels[i]==1\n",
        "    negative += labels[i]==0\n",
        "print(\"positive = \", positive, \" negative: \", negative)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive =  216  negative:  71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe8KtmADxNwS",
        "colab_type": "text"
      },
      "source": [
        "Βλεπουμε οτι εχουμε unbalanced data set, καθως η κλαση των positive(υγειων) ειναι κατα πολυ μεγαλυτερη σε μεγεθος απο την κλαση των negative. (75-25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2l2kgIc_qXj",
        "colab_type": "text"
      },
      "source": [
        "Θα πρέπει αρχικά να κάνουμε split τα δεδομένα μας, όπως λέει η εκφώνηση 80-20. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBRy3W1k8o82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train, test, train_labels, test_labels = train_test_split(df, labels, test_size=0.2, random_state=78)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCBzUF3cxwp8",
        "colab_type": "text"
      },
      "source": [
        "Ας επαληθεύσουμε ότι πάλι ο λόγος των positive(υγειων) με των negative(άρρωστων) είναι πάλι skewed:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF_OBTmhx9IA",
        "colab_type": "code",
        "outputId": "a69a1145-ea66-4e67-df40-0a008af92c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "positive = 0\n",
        "negative = 0\n",
        "for i in range(0, len(train_labels)):\n",
        "    positive += train_labels[i]==1\n",
        "    negative += train_labels[i]==0\n",
        "print(\"positive = \", positive, \" negative: \", negative)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive =  174  negative:  55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PTz3kzyO5S",
        "colab_type": "text"
      },
      "source": [
        "Ας γράψουμε τώρα τις συναρτήσεις μετασχηματισμών και δημιουργίας των classifiers, που θα τις χρησιμοποιήσουμε εντός του pipeline. Έχουμε γράψει συναρτήσεις για selection δεδομένων, resampling (κάτι που θα χρειαστεί σίγουρα, καθώς δεν έχουμε ισορροπημένο dataset), standardization και τέλος για την δημιοργία των κατηγοριοποιητών. Όλες αυτές οι μέθοδοι είναι αρκετά γενικευμένοι και μπορούν να πάρουν σαν παραμέτρους διαφορετικές τεχνικές που ίσως να θέλουμε να εφαρμόσουμε στην κάθε περίπτωση."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miepy3x5hvpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies a transformation (selection) on a dataset, \n",
        "    based on the type of the selector. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no transformation happens. The selector argument\n",
        "    is a string and can be one of two things: either \"PCA\" or \n",
        "    \"Variance Threshold\". In either case, there can be arguments, and in \"PCA\"\n",
        "    the arguments can be an int of the number of principal components, and in\n",
        "    \"Variance Threshold\", it can be a float. Returns the transformed dataset.\n",
        "\"\"\"\n",
        "def Apply_selector(data, selector = None, arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    \n",
        "    if selector == \"PCA\":    \n",
        "        if arguments:\n",
        "            number_of_components = arguments\n",
        "            pca = PCA(n_components = number_of_components)\n",
        "            pca.fit(train)\n",
        "            train_reduced = pca.transform(train)\n",
        "            test_reduced = pca.transform(test)\n",
        "        else:\n",
        "            pca = PCA()\n",
        "            pca.fit(train)\n",
        "            train_reduced = pca.transform(train)\n",
        "            test_reduced = pca.transform(test)\n",
        "    elif selector == \"Variance Threshold\":\n",
        "        if arguments:\n",
        "            t = arguments\n",
        "            sel = VarianceThreshold(threshold=t)\n",
        "            train_reduced = sel.fit_transform(train)\n",
        "            test_reduced = sel.transform(test)\n",
        "        else:\n",
        "            sel = VarianceThreshold()\n",
        "            train_reduced = sel.fit_transform(train)\n",
        "            test_reduced = sel.transform(test)\n",
        "    else:\n",
        "        (train_reduced, train_labels, test_reduced, test_labels) = data\n",
        "    return (train_reduced, train_labels, test_reduced, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05vv4ikkh3Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies resampling on a dataset, based on the \n",
        "    type of sampling method. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no sampling happens. The sampling argument\n",
        "    is a string and can be one of three things: \"Over\", \"Under\" or \n",
        "    \"OverUnder\". In any case, there can be arguments, and in Under or Over or\n",
        "    OverUnder it can be a float with the ratio of samples between classes. \n",
        "    Returns the resampled dataset.\n",
        "\"\"\"\n",
        "def Apply_sampling(data, sampling = None, arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    if sampling == \"Over\":    \n",
        "        if arguments:\n",
        "            ratio = arguments\n",
        "            ros = RandomOverSampler(sampling_strategy=ratio)\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "        else:\n",
        "            ros = RandomOverSampler()\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "        return (train_oversampled, train_labels_oversampled, test, test_labels)\n",
        "    elif sampling == \"Under\":\n",
        "        if arguments:\n",
        "            ratio = arguments\n",
        "            rus = RandomUnderSampler(sampling_strategy=ratio)\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train,train_labels)\n",
        "        else:\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train,train_labels)\n",
        "        return (train_undersampled, train_labels_undersampled, test, test_labels)\n",
        "    elif sampling==\"OverUnder\":\n",
        "        if arguments:    \n",
        "            ratio = arguments\n",
        "            ros = RandomOverSampler(sampling_strategy=ratio)\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train_oversampled,train_labels_oversampled)\n",
        "        else:\n",
        "            ros = RandomOverSampler()\n",
        "            train_oversampled, train_labels_oversampled = ros.fit_sample(train,train_labels)\n",
        "            rus = RandomUnderSampler()\n",
        "            train_undersampled, train_labels_undersampled = rus.fit_sample(train_oversampled,train_labels_oversampled)\n",
        "        return (train_undersampled, train_labels_undersampled, test, test_labels)\n",
        "    else:\n",
        "        return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qw5kMxliJud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats as st\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that applies a transformation (standardization) on a dataset, \n",
        "    based on the type of the standardizer. Takes 3 arguments, one is mandatory and \n",
        "    is the dataset, which is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The other two can be\n",
        "    None, and when they are, no transformation happens. The selector argument\n",
        "    is a string and can be one of two things: either \"zscore\" or \n",
        "    \"minmax\". Argument arguments is always None. Returns the transformed dataset.\n",
        "\"\"\"\n",
        "def Apply_standardizer(data, standardizer = None,  arguments = None):\n",
        "    (train, train_labels, test, test_labels) = data\n",
        "    if standardizer==\"zscore\":\n",
        "        std_train = st.zscore(train)\n",
        "    elif standardizer==\"minmax\":\n",
        "        std_train = (train - np.min(train) )/ (np.max(train) - np.min(train))\n",
        "    else:\n",
        "        std_train = train\n",
        "    return (std_train, train_labels, test, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2doolScyIJqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\"\"\"\n",
        "    This is a function that creates a classifier object. It does not train, \n",
        "    it takes the name of the classifier, along with the arguments we want \n",
        "    to pass to the instance creator for the classifier and returns the \n",
        "    classifier object.\n",
        "\"\"\"\n",
        "def Create_classifier(classifier,  arguments = None):\n",
        "    if classifier==\"kNN\":\n",
        "        if arguments and arguments!=-1:\n",
        "            clf = neighbors.KNeighborsClassifier(n_jobs = -1, n_neighbors = arguments)\n",
        "        else:\n",
        "            clf = neighbors.KNeighborsClassifier(n_jobs = -1)\n",
        "    else:\n",
        "        if arguments:\n",
        "            strat = arguments\n",
        "            clf = DummyClassifier(strategy=strat)\n",
        "        else:\n",
        "            clf = DummyClassifier()\n",
        "    return clf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVhGuPcy6eh",
        "colab_type": "text"
      },
      "source": [
        "Εδώ ορίζουμε την pipeline, η οποία αντιστοιχίζει τα steps της προεπεξεργασίας δεδομένων και της δημιοργίας των classifiers με τις παραπάνω μεθόδους. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMX219HCiTQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    This is an implementation for the function imblearn.pipeline.Pipeline().\n",
        "    The way this function works is, it takes as input two arguments. The first\n",
        "    is a list. This list contains the steps that need to be executed in the \n",
        "    pipeline, IN ORDER. Always the last step is the classifier. Each step is\n",
        "    a tuple, which contains the name of the step, and the arguments needed to \n",
        "    execute the step, like so: (\"name\", arguments). The second argument of the \n",
        "    function is a tuple of data. It needs to be in the form:\n",
        "    (train_data, train_labels, test_data, test_labels). The function outputs a\n",
        "    tuple, which contains the processed data, ready for fitting the classier, \n",
        "    and the classifier object, as specified by the arguments, like so:\n",
        "    returns: (processed_data, classifier_object)\n",
        "\"\"\"\n",
        "def Pipeline(steps, data):\n",
        "    steps_dict = {\"selector\": Apply_selector,\n",
        "                  \"sampling\": Apply_sampling,\n",
        "                  \"standardizer\": Apply_standardizer,\n",
        "                  \"kNN\": Create_classifier,\n",
        "                  \"dummy\": Create_classifier}\n",
        "\n",
        "    for step in steps:\n",
        "        if step[0]!=\"kNN\" and step[0]!=\"dummy\":\n",
        "            data = steps_dict[step[0]](data,step[1])\n",
        "        else:\n",
        "            return ( data, Create_classifier(step[0], arguments = step[1]) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvUNGmO9zFDD",
        "colab_type": "text"
      },
      "source": [
        "Η συνάρτηση model_tuning ουσιαστικά υλοποιεί την gridsearchcv, και δημιουργεί για κάθε παράμετρο, για κάθε αλληλουχία steps, για κάθε dataset ένα pipe, και στην συνέχεια κάνει train τα δεδομένα και βγάζει ένα training accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RktfiSkMmjfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    This is an implementation for the function gridsearchcv. It takes 3\n",
        "    arguments, a tuple (model_name, model_parameters), the data to be fitted\n",
        "    and trained, and the specific parameters. The data parameter needs to be \n",
        "    in the form: (train_data, train_labels, test_data, test_labels). Lastly, \n",
        "    the Steps argument is a list, that has the steps for the data preprocessing,\n",
        "    as described in the Pipeline Function. For this exercise, we are dealing \n",
        "    only with dummy classifiers and kNN. The model_parameters if not None, is \n",
        "    a list of parameters (strings for dummy, ints for kNN).\n",
        "    Returns (classifier_object, best_parameter, accuracy_list)\n",
        "\"\"\"\n",
        "def model_tuning(model, folded_data,  f1, preprocessing_steps = None):\n",
        "   \n",
        "    #if the classifier is a dummy:\n",
        "    if model[0]==\"dummy\":\n",
        "        #and it has parameters (strategy):\n",
        "        if model[1]!=None:\n",
        "\n",
        "\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "\n",
        "\n",
        "            #if type(model[1])!= \"list\" then we only have one parameter\n",
        "            if type(model[1])!='list':\n",
        "                best_parameter = model[1]\n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], model[1])]\n",
        "                else:\n",
        "                    steps = [(model[0], model[1])]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                \n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(f1_score(test_labels, preds, average = f1))\n",
        "\n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "\n",
        "\n",
        "\n",
        "            else:\n",
        "                #for each parameter, run the \n",
        "                for parameter in model[1]:\n",
        "        \n",
        "                    #if there are preprocessing steps, add the model_creation \n",
        "                    #to the pipeline:\n",
        "                    if preprocessing_steps:\n",
        "                        steps = preprocessing_steps + [(model[0], parameter)]\n",
        "                    else:\n",
        "                        steps = [(model[0], parameter)]\n",
        "                    #create the pipeline, returns the processed data, the \n",
        "                    #classifier ready for training and evaluating:\n",
        "\n",
        "                    #initialize a list with the accuracies for the folded data:\n",
        "                    accuracy_list = []\n",
        "                    #train for each of the 10 folds, begin from scratch every time,\n",
        "                    #and then average out the results.\n",
        "                    for data in folded_data:\n",
        "                        \n",
        "                        #process the data, create the classifier\n",
        "                        (processed_data, clf) = Pipeline(steps, data)\n",
        "                        (train, train_labels, test, test_labels) = processed_data\n",
        "                        #fit the classifier\n",
        "                        clf = clf.fit(train, train_labels)\n",
        "                        #make the predictions based on the training\n",
        "                        preds = clf.predict(test)\n",
        "                        accuracy_list.append(f1_score(test_labels, preds, average = f1))\n",
        "                    \n",
        "                    \n",
        "                    \n",
        "                    #store the average accuracy\n",
        "                    avg_accuracy_list.append(sum(accuracy_list)/len(accuracy_list))\n",
        "                    \n",
        "                    #if the average accuracy of the 10 folds is the best one yet, \n",
        "                    #keep the parameter\n",
        "                    if (sum(accuracy_list)/len(accuracy_list)) >= max(avg_accuracy_list):\n",
        "                        best_parameter = parameter\n",
        "            \n",
        "        \n",
        "        \n",
        "        #if the dummy classifier has no parameters:                    \n",
        "        else:\n",
        "                #initialize the lists with the possible best parameters, and the average\n",
        "                #accuracies for each of the model parameter\n",
        "                avg_accuracy_list = []\n",
        "\n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], model[1])]\n",
        "                else:\n",
        "                    steps = [(model[0], model[1])]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                \n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(f1_score(test_labels, preds, average = f1))\n",
        "                \n",
        "                #since no parameter list has been given, best_param = param\n",
        "                best_parameter = None\n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "    \n",
        "    \n",
        "    #if the classifier is a kNN:\n",
        "    elif model[0] == \"kNN\":\n",
        "        #if the kNN classifier has no parameters:    \n",
        "        if model[1]== -1:\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "\n",
        "            #if there are preprocessing steps, add the model_creation \n",
        "            #to the pipeline:\n",
        "            if preprocessing_steps:\n",
        "                steps = preprocessing_steps + [(model[0], model[1])]\n",
        "            else:\n",
        "                steps = [(model[0], model[1])]\n",
        "            #create the pipeline, returns the processed data, the \n",
        "            #classifier ready for training and evaluating:\n",
        "\n",
        "            \n",
        "            #initialize a list with the accuracies for the folded data:\n",
        "            accuracy_list = []\n",
        "                \n",
        "            #train for each of the 10 folds, begin from scratch every time,\n",
        "            #and then average out the results.\n",
        "            for data in folded_data:\n",
        "                \n",
        "                #process the data, create the classifier\n",
        "                (processed_data, clf) = Pipeline(steps, data)\n",
        "                (train, train_labels, test, test_labels) = processed_data\n",
        "                #fit the classifier\n",
        "                clf = clf.fit(train, train_labels)\n",
        "                #make the predictions based on the training\n",
        "                preds = clf.predict(test)\n",
        "                accuracy_list.append(f1_score(test_labels, preds, average = f1))\n",
        "            \n",
        "            #since no parameter list has been given, best_param = None\n",
        "            best_parameter = None\n",
        "            #store the average accuracy\n",
        "            avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "\n",
        "        #if it has parameters (number_of_neighbors):    \n",
        "        else:\n",
        "            #initialize the lists with the possible best parameters, and the average\n",
        "            #accuracies for each of the model parameter\n",
        "            avg_accuracy_list = []\n",
        "        \n",
        "            #if type(model[1])!= \"list\" then we only have one parameter\n",
        "            if type(model[1])!='list':\n",
        "                #initialize the lists with the possible best parameters, and the average\n",
        "                #accuracies for each of the model parameter\n",
        "                avg_accuracy_list = []\n",
        "                best_parameter = model[1]\n",
        "                #if there are preprocessing steps, add the model_creation \n",
        "                #to the pipeline:\n",
        "                if preprocessing_steps:\n",
        "                    steps = preprocessing_steps + [(model[0], model[1])]\n",
        "                else:\n",
        "                    steps = [(model[0], model[1])]\n",
        "                #create the pipeline, returns the processed data, the \n",
        "                #classifier ready for training and evaluating:\n",
        "\n",
        "                \n",
        "                #initialize a list with the accuracies for the folded data:\n",
        "                accuracy_list = []\n",
        "\n",
        "                #train for each of the 10 folds, begin from scratch every time,\n",
        "                #and then average out the results.\n",
        "                for data in folded_data:\n",
        "                    \n",
        "                    #process the data, create the classifier\n",
        "                    (processed_data, clf) = Pipeline(steps, data)\n",
        "                    (train, train_labels, test, test_labels) = processed_data\n",
        "                    #fit the classifier\n",
        "                    clf = clf.fit(train, train_labels)\n",
        "                    #make the predictions based on the training\n",
        "                    preds = clf.predict(test)\n",
        "                    accuracy_list.append(f1_score(test_labels, preds, average = f1))\n",
        "\n",
        "                #store the average accuracy\n",
        "                avg_accuracy_list = sum(accuracy_list)/len(accuracy_list)\n",
        "                #for each parameter, run the \n",
        "\n",
        "\n",
        "            else:\n",
        "                for parameter in model[1]:\n",
        "        \n",
        "                    #if there are preprocessing steps, add the model_creation \n",
        "                    #to the pipeline:\n",
        "                    if preprocessing_steps:\n",
        "                        steps = preprocessing_steps + [(model[0], parameter)]\n",
        "                    else:\n",
        "                        steps = [(model[0], parameter)]\n",
        "                    #create the pipeline, returns the processed data, the \n",
        "                    #classifier ready for training and evaluating:\n",
        "\n",
        "                    \n",
        "                    #initialize a list with the accuracies for the folded data:\n",
        "                    accuracy_list = []\n",
        "\n",
        "                    #train for each of the 10 folds, begin from scratch every time,\n",
        "                    #and then average out the results.\n",
        "                    for data in folded_data:\n",
        "                        \n",
        "                        #process the data, create the classifier\n",
        "                        (processed_data, clf) = Pipeline(steps, data)\n",
        "                        (train, train_labels, test, test_labels) = processed_data\n",
        "                        #fit the classifier\n",
        "                        clf = clf.fit(train, train_labels)\n",
        "                        #make the predictions based on the training\n",
        "                        preds = clf.predict(test)\n",
        "                        accuracy_list.append(f1_score(test_labels, preds, average = f1))\n",
        "                    #store the average accuracy\n",
        "                    avg_accuracy_list.append(sum(accuracy_list)/len(accuracy_list))\n",
        "\n",
        "\n",
        "                    #if the average accuracy of the 10 folds is the best one yet, \n",
        "                    #keep the parameter\n",
        "                    if (sum(accuracy_list)/len(accuracy_list)) >= max(avg_accuracy_list):\n",
        "                        best_parameter = parameter\n",
        "                    \n",
        "    return (clf, best_parameter, avg_accuracy_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8P93LtzWsy",
        "colab_type": "text"
      },
      "source": [
        "Η συνάρτηση αυτή είναι η υλοποίηση της Kfold(), και σπάει σε διπλώματα το dataset. Συγκεκριμένα, το σπάει σε 10 διαφορετικά dataset, προκειμένου να αξιοποιήσουμε καλύτερα τον σχετικά μικρό όγκο δεδομένων μας. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRmPXV7pibkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "\"\"\"This function is an implementation for KFold(). It takes one argument that \n",
        "is a tuple in the form (train, train_labels, test, test_labels), and returns\n",
        "a list of k (10) tuples in the same form. It shuffles the train and train \n",
        "labels, to randomize the data, before splitting. \"\"\"\n",
        "def ten_fold(data):\n",
        "    #In our exercise, the number of folds is 10:\n",
        "    k = 10\n",
        "    #Reading the data, no need to read the test and test_labels data.\n",
        "    train, train_labels, _, _ = data\n",
        "    #Randomize the data:\n",
        "    train, train_labels = shuffle(train, train_labels)\n",
        "    #train is a pandas dataframe, need to make it a numpy array:\n",
        "    train = np.asarray(train)\n",
        "    #This is the size of the test set each time:\n",
        "    fold_size = int(len(train)/k)\n",
        "    #Create the list to be returned, fill it:\n",
        "    folds = []\n",
        "    for i in range(0,10):\n",
        "        train1 = []\n",
        "        train_labels1 = []\n",
        "        test = []\n",
        "        test_labels = []\n",
        "        for j in range(0, len(train)):\n",
        "            if j>= i*fold_size and j<(i+1)*fold_size:\n",
        "                test.append(train[j])\n",
        "                test_labels.append(train_labels[j])\n",
        "            else:\n",
        "                train1.append(train[j])\n",
        "                train_labels1.append(train_labels[j])\n",
        "        folds.append((train1, train_labels1, test, test_labels))\n",
        "    return folds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzzeMryyznY7",
        "colab_type": "text"
      },
      "source": [
        "Ας ξαναδιαβάσουμε από την αρχή τα δεδομένα μας, και ας τα χωρίσουμε πάλι σε training data και test data πριν προχωρήσουμε στο Γ. Πρέπει όμως πριν, να αφαιρέσουμε την στήλη που περιέχει τις ετικέτες για την υγεία των ασθενών."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ2oqrmDedlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"all.csv\")\n",
        "df = df.drop(df.columns[[62, 63, 64, 65, 66, 67]], axis=1)\n",
        "labels_df = df.iloc[:, [62]]\n",
        "labels = labels_df.values.reshape(287,)\n",
        "\n",
        "\n",
        "df = df.drop(df.columns[62], axis=1)\n",
        "train, test, train_labels, test_labels = train_test_split(df, labels, test_size=0.2, random_state=78)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjNCI0V_z-lz",
        "colab_type": "text"
      },
      "source": [
        "#Γ \n",
        "---\n",
        "\n",
        "Για το section αυτό, αρχικά πρέπει να εκπαιδεύσουμε τους classifiers (dummy και kNN) στο train dataset, με deafult τιμές. Καλόυμε την συνάρτηση model_tuning, και για τα δύο μοντέλα, χωρίς επιπλέον παραμέτρους, και θα λάβουμε πίσω τους classifiers, την καλύτερη παράμετρο (εδώ None, αφού δεν τους έχουμε δώσει κάποια παράμετρο για training) και το average training accuracy πάνω σε 10 folds. \n",
        "\n",
        "Δεν θα κάνουμε κάποια preprocessing steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqq8OnaOlBuY",
        "colab_type": "code",
        "outputId": "ce4680be-4898-42ca-e343-e47f19eb93e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Group the splitted dataset into a tuple:\n",
        "data = (train, train_labels, test,  test_labels)\n",
        "\n",
        "dummy_default = DummyClassifier()\n",
        "dummy_default.fit(train, train_labels)\n",
        "\n",
        "kNN_default = KNeighborsClassifier()\n",
        "kNN_default.fit(train, train_labels)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV0XlTdz2N1T",
        "colab_type": "text"
      },
      "source": [
        "Τώρα θα κάνουμε εκτίμηση στο test set και για τους δύο, θα βρούμε το confusion matrix τους, και θα τυπώσουμε το f1-micro και f1-macro average:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPXFLHN2NZN",
        "colab_type": "code",
        "outputId": "856d08e7-a222-41b1-eca8-caa77a2b4ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "preds1 = dummy_default.predict(test)\n",
        "preds2 = kNN_default.predict(test)\n",
        "\n",
        "#print the accuracies:\n",
        "print(\"The average testing accuracy of the default dummy classifier is: \", accuracy_score(test_labels, preds1))\n",
        "print(\"The average testing accuracy of the default kNN classifier is: \", accuracy_score(test_labels, preds2))\n",
        "\n",
        "#plot the confusion matrices:\n",
        "disp1 = plot_confusion_matrix(dummy_default, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "disp1 = plot_confusion_matrix(kNN_default, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#print the classification reports, where f1 macro average is shown:\n",
        "print(classification_report(test_labels, preds1, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "print(classification_report(test_labels, preds2, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "\n",
        "\n",
        "#print the f1 micro scores:\n",
        "print(\"F1 micro score for the default dummy classifier is: \", f1_score(test_labels, preds1, average = 'micro'))\n",
        "print(\"F1 micro score for the default kNN classifier is: \", f1_score(test_labels, preds2, average = 'micro'))\n",
        "\n",
        "#print the f1 macro scores:\n",
        "print(\"F1 macro score for the default dummy classifier is: \", f1_score(test_labels, preds1, average = 'macro'))\n",
        "print(\"F1 macro score for the default kNN classifier is: \", f1_score(test_labels, preds2, average = 'macro'))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average testing accuracy of the default dummy classifier is:  0.6379310344827587\n",
            "The average testing accuracy of the default kNN classifier is:  0.7586206896551724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEGCAYAAAAt9v2AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n18wsY/8NWp8CijT0tdoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEKCAYAAABaND37AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7gAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.31      0.25      0.28        16\n",
            "     healthy       0.73      0.79      0.76        42\n",
            "\n",
            "    accuracy                           0.64        58\n",
            "   macro avg       0.52      0.52      0.52        58\n",
            "weighted avg       0.62      0.64      0.63        58\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.67      0.25      0.36        16\n",
            "     healthy       0.77      0.95      0.85        42\n",
            "\n",
            "    accuracy                           0.76        58\n",
            "   macro avg       0.72      0.60      0.61        58\n",
            "weighted avg       0.74      0.76      0.72        58\n",
            "\n",
            "F1 micro score for the default dummy classifier is:  0.6379310344827587\n",
            "F1 micro score for the default kNN classifier is:  0.7586206896551724\n",
            "F1 macro score for the default dummy classifier is:  0.5172413793103448\n",
            "F1 macro score for the default kNN classifier is:  0.6073500967117988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rdSr2lzEhBJ",
        "colab_type": "text"
      },
      "source": [
        "Τυπώνουμε δύο bar plots, που δείχνουν τις τιμές των δύο classifiers μας (default_dummy, default_kNN) για τα averaged metrics f1 micro και f1 macro:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxoVbCF4E4Kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "7d41e15a-6ade-4f9f-d958-f9eed0faaad1"
      },
      "source": [
        "n_groups = 2\n",
        "f1_micro = (f1_score(test_labels, preds1, average = 'micro'), f1_score(test_labels, preds2, average = 'micro'))\n",
        "f1_macro = (f1_score(test_labels, preds1, average = 'macro'), f1_score(test_labels, preds2, average = 'macro'))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(n_groups)\n",
        "bar_width = 0.35\n",
        "opacity = 0.8\n",
        "\n",
        "rects1 = plt.bar(index, f1_micro, bar_width,\n",
        "alpha=opacity,\n",
        "color='b',\n",
        "label='F1 micro')\n",
        "\n",
        "rects2 = plt.bar(index + bar_width, f1_macro, bar_width,\n",
        "alpha=opacity,\n",
        "color='g',\n",
        "label='F1 macro')\n",
        "\n",
        "plt.xlabel('Classifier')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Scores by classifier')\n",
        "plt.xticks(index + bar_width, ('default Dummy', 'default kNN'))\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZWZmueSAMjOzXPr/S3apmsQ3FOsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTEz5VvLd20r",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι και οι δύο classifiers έχουν μέτρια απόδοση. Αυτό είναι αποτέλεσμα πολλών παραγόντων.  \n",
        "\n",
        "\n",
        "Αρχικά, δεν έχουμε επεξεργαστεί τα δεδομένα, κάτι που είναι πολύ σημαντικό. Τα δεδομένα μας δεν είναι καθόλου ισορροπημένα, καθώς ο αριθμός των δειγμάτων της μιας κλάσης είναι εξαιρετικά μεγαλύτερος της άλλης (τα δεδομένα είναι split μεταξύ των κλάσεων σε 75% positive - 25% negative). \n",
        "\n",
        "\n",
        "Πέρα από την ανισορροπία των δεδομένων, δεν έχει γίνει καθόλου επιλογή, και καθόλου standardization. Αυτό μπορεί να είναι εξαιρετικά κακό για τον classifier kNN, αλλά για τον dummy classifier, λόγω της απλότητάς του, να μην τον επηρεάζει. \n",
        "\n",
        "\n",
        "By default, ο default dummy classifier ακολουθεί μια στρατηγική που (σύμφωνα με το documentation) ονομάζεται \"stratified\", \"γεννά προβλέψεις λαμβάνοντας υπόψιν την κατανομή των δεδομένων\". Βλέπουμε ότι ο κατηγοριοποιητής dummy έχει accuracy περίπου 68%, κάτι που μας υποδεικνύει ότι τον επηρεάζει κάπως η κλάση με την μεγαλύτερη εμφανισιμότητα, δηλαδή τα θετικά. \n",
        "\n",
        "\n",
        "Από την άλλη, ο default kNN classifier έχει σαν όρισμα τιμή για κοντινότερους γείτονες n_neighbors = 5. Αυτή η τιμή είναι πιθανώς πολύ μικρή, και σε συνδυασμό με τα καθόλου καλά δεδομένα που του παρέχουμε, δεν εμφανίζει καλύτερη απόδοση από τον dummy classifier. \n",
        "\n",
        "\n",
        "Σαν να μην φτάναν όλα αυτά, λόγω της διάτμησης των δεδομένων με την μέθοδο kfold, τα δεδομένα που κάθε φορά λαμβάνει σαν training ο kNN classifier είναι σχετικά λίγα και το k δεν μπορεί να φτάσει σε μεγάλες τιμές (όχι ότι εκεί θα βρισκόταν απαραίτητα το βέλτιστο, αλλά καλό θα ήταν να βλέπαμε την συμπεριφορά του. \n",
        "\n",
        "\n",
        "Βλέπουμε λοιπόν ότι, τόσο οι F1 micro όσο και οι F1 macro μέσες τιμές είναι σαφώς καλύτερες στον kNN classifier. Φυσικά, επιδέχονται σημαντικής βελτίωσης.\n",
        "\n",
        "Τέλος, για τους πίνακες σύγχυσης, βλέπουμε ότι ο kNN classifier έχει σημαντικά πιο συγκεντρωμένα τα δεδομένα στην κύρια διαγώνιο σε σχέση με τον dummy, που του έχουν \"ξεφύγει\" αρκετά στην άλλη διαγώνιο, και είναι σχεδόν ισόποσα τα νούμερα. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ndY0gk7pR0",
        "colab_type": "text"
      },
      "source": [
        "#Δ\n",
        "---\n",
        "\n",
        "Για το section αυτό, θα δοκιμάσουμε κάθε πιθανό συνδυασμό προεπεξεργασίας δεδομένων, μαζί με κάθε δυνατό συνδυασμό παραμέτρων για τα μοντέλα μας, και αποφασίσουμε για τον βέλτιστο συνδυασμό με βάση τα F1 scores τους"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17rEFr4Cv-uV",
        "colab_type": "text"
      },
      "source": [
        "Ξεκινάμε με τον dummy classifier. Για τον dummy classifier, μπορουμε να επηρεάσουμε το preprocessing των δεδομένων (τι standardization, sampling, selection θα κάνουμε στα δεδομένα), καθώς και την τεχνική του dummy classifier ('most_frequent', 'stratified', 'uniform', 'prior')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V238HWfk6byA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "0946f04d-7f4e-4795-85df-174b565ad1ec"
      },
      "source": [
        "train, test, train_labels, test_labels = train_test_split(df, labels, test_size=0.2, random_state=78)\n",
        "data = (train, train_labels, test,  test_labels)\n",
        "\n",
        "\n",
        "best_dummy_f1_micro = 0\n",
        "best_dummy_f1_macro = 0\n",
        "\n",
        "folds = ten_fold(data)\n",
        "\n",
        "for parameter in [None, 'most_frequent', 'stratified', 'uniform', 'prior']:\n",
        "    for standardizer in [None, \"zscore\", \"minmax\"]:\n",
        "        for selector in [None, \"PCA\", \"Variance Threshold\"]:\n",
        "            if standardizer==\"zscore\" and selector==\"Variance Threshold\":\n",
        "                continue\n",
        "            for sampler in [None, \"Over\", \"Under\", \"OverUnder\"]:\n",
        "                prep = []\n",
        "                if standardizer:\n",
        "                    prep+=[(\"standardizer\", standardizer)]\n",
        "                if selector:\n",
        "                    prep+=[(\"selector\", selector)]\n",
        "                if sampler:\n",
        "                    prep+=[(\"sampling\", sampler)]\n",
        "                \n",
        "                #Start measuring the time:\n",
        "                start_time = time.time()\n",
        "                dummy, best_parameter, avg_f1_scores = model_tuning((\"dummy\", parameter), folded_data=folds, f1 = \"micro\", preprocessing_steps = prep)\n",
        "                duration = time.time() - start_time                \n",
        "                #print(\"F1 micro score: \", avg_f1_scores)\n",
        "                \n",
        "                #print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                \n",
        "                if avg_f1_scores > best_dummy_f1_micro:\n",
        "                    best_dummy_f1_micro = avg_f1_scores\n",
        "                    best_f1_micro_dummy = (dummy, best_parameter, standardizer, selector, sampler, duration)\n",
        "                    print(\"==============================================================================================================\")\n",
        "                    print(\"Best F1 micro score so far: \", avg_f1_scores)\n",
        "                    print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                    print(\"==============================================================================================================\")\n",
        "\n",
        "\n",
        "for parameter in [None, 'most_frequent', 'stratified', 'uniform', 'prior']:\n",
        "    for standardizer in [None, \"zscore\", \"minmax\"]:\n",
        "        for selector in [None, \"PCA\", \"Variance Threshold\"]:\n",
        "            if standardizer==\"zscore\" and selector==\"Variance Threshold\":\n",
        "                continue\n",
        "            for sampler in [None, \"Over\", \"Under\", \"OverUnder\"]:\n",
        "                prep = []\n",
        "                if standardizer:\n",
        "                    prep+=[(\"standardizer\", standardizer)]\n",
        "                if selector:\n",
        "                    prep+=[(\"selector\", selector)]\n",
        "                if sampler:\n",
        "                    prep+=[(\"sampling\", sampler)]\n",
        "                \n",
        "                #Start measuring the time:\n",
        "                start_time = time.time()\n",
        "                dummy, best_parameter, avg_f1_scores = model_tuning((\"dummy\", parameter), folded_data=folds, f1 = \"macro\", preprocessing_steps = prep)\n",
        "                duration = time.time() - start_time \n",
        "                \n",
        "                #print(\"F1 macro score: \", avg_f1_scores)\n",
        "                \n",
        "                #print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                \n",
        "                if avg_f1_scores > best_dummy_f1_macro:\n",
        "                    best_dummy_f1_macro = avg_f1_scores\n",
        "                    best_f1_macro = (dummy, best_parameter, standardizer, selector, sampler, duration)\n",
        "                    print(\"==============================================================================================================\")\n",
        "                    print(\"Best F1 macro score so far: \", avg_f1_scores)\n",
        "                    print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                    print(\"==============================================================================================================\")\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.6045454545454545\n",
            "Achieved with: {parameter =  None , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.6090909090909091\n",
            "Achieved with: {parameter =  None , standardizer =  None , selector =  PCA , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.6409090909090909\n",
            "Achieved with: {parameter =  None , standardizer =  None , selector =  Variance Threshold , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.6636363636363637\n",
            "Achieved with: {parameter =  None , standardizer =  minmax , selector =  PCA , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.6727272727272727\n",
            "Achieved with: {parameter =  None , standardizer =  minmax , selector =  Variance Threshold , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.7545454545454546\n",
            "Achieved with: {parameter =  most_frequent , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5119613095334701\n",
            "Achieved with: {parameter =  None , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5481529358154745\n",
            "Achieved with: {parameter =  None , standardizer =  None , selector =  Variance Threshold , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5527778394713879\n",
            "Achieved with: {parameter =  None , standardizer =  minmax , selector =  Variance Threshold , sampling =  None }\n",
            "==============================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqcQmTTRLBvV",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι με βάση το μετρικό F1-micro, ο καλύτερος συνδυασμός είναι με στρατηγική για τον classifier την most_frequent, κανένα standardization δεδομένων, κανένα selection δεδομένων, και κανένα sampling δεδομένων.\n",
        "\n",
        "Ακόμα, βλέπουμε ότι με βάση το μετρικό F1-macro, ο καλύτερος συνδυασμός είναι με στρατηγική για τον classifier την stratified, standardization δεδομένων να γίνεται με βάση το zscore, κανένα selection δεδομένων, και κανένα sampling δεδομένων.\n",
        "\n",
        "\n",
        "Ειναι ***σημαντικό*** να υπενθυμίσουμε τον μικρό αριθμό δεδομένων που έχουμε, που οδηγεί σε πολλούς διαφορετικούς συνδυασμούς να έχουν τα ίδια αποτελέσματα. Γενικά (από δικές μας δοκιμές), καταλήγουμε ότι πολλοί συνδυασμοί παραμέτρων που περιέχουν τεχνικές preprocessing έχουν το ίδιο αποτέλεσμα με συνδυασμούς που κάνουν λίγο/καθόλου preprocessing δεδομένων, αλλά επειδή τα δεδομένα είναι ακριβώς τόσο λίγα, δεν μπορεί να φανεί η αξία του preprocessing δεδομένων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mNG8GQ0MYPW",
        "colab_type": "text"
      },
      "source": [
        "Συνεχίζουμε με την βελτιστοποίηση του kNN classifier. Σε αυτόν μπορούμε να επηρεάσουμε τον τρόπο που προεπεξεργαζόμαστε τα δεδομένα (όπως αναφέρθηκε πριν), αλλά πλέον σαν παράμετρο του classifier δίνουμε τον αριθμό των k κοντινότερων γειτόνων. Επιλέξαμε να εξετάζουμε k μονά και από το 1 μέχρι το 31, αφενός γιατί στα k_folds τα folds είναι μεγέθους 22, και η συνάρτηση KNNClassifier δεν λειτουργεί πολύ καλά με k μεγαλύτερο των δειγμάτων, αλλά και αφετέρου επειδή είδαμε (από δικά μας πειράματα) ότι για k μεγαλύτερο του 30, η απόδοση πέφτει σημαντικά."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_ikIWGLzTK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62f1777c-c672-48dc-e4ed-b15a40827898"
      },
      "source": [
        "best_knn_f1_micro = 0\n",
        "best_knn_f1_macro = 0\n",
        "\n",
        "\n",
        "for parameter in range(1, 31, 2):\n",
        "    for standardizer in [None, \"zscore\", \"minmax\"]:\n",
        "        for selector in [None, \"PCA\", \"Variance Threshold\"]:\n",
        "            if standardizer==\"zscore\" and selector==\"Variance Threshold\":\n",
        "                continue\n",
        "            for sampler in [None, \"Over\", \"Under\", \"OverUnder\"]:\n",
        "                prep = []\n",
        "                if standardizer:\n",
        "                    prep+=[(\"standardizer\", standardizer)]\n",
        "                if selector:\n",
        "                    prep+=[(\"selector\", selector)]\n",
        "                if sampler:\n",
        "                    prep+=[(\"sampling\", sampler)]\n",
        "                \n",
        "                #Start measuring the time:\n",
        "                start_time = time.time()\n",
        "                knn, best_parameter, avg_f1_scores = model_tuning((\"kNN\", parameter), folded_data=folds, f1 = \"micro\", preprocessing_steps = prep)\n",
        "                duration = time.time() - start_time   \n",
        "                \n",
        "                #print(\"F1 micro score: \", avg_f1_scores)\n",
        "                \n",
        "                #print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                \n",
        "                if avg_f1_scores > best_knn_f1_micro:\n",
        "                    best_knn_f1_micro = avg_f1_scores\n",
        "                    best_f1_micro_knn = (knn, best_parameter, standardizer, selector, sampler, duration)\n",
        "                    print(\"==============================================================================================================\")\n",
        "                    print(\"Best F1 micro score so far: \", avg_f1_scores)\n",
        "                    print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                    print(\"==============================================================================================================\")\n",
        "\n",
        "\n",
        "for parameter in  range(1, 31, 2):\n",
        "    for standardizer in [None, \"zscore\", \"minmax\"]:\n",
        "        for selector in [None, \"PCA\", \"Variance Threshold\"]:\n",
        "            if standardizer==\"zscore\" and selector==\"Variance Threshold\":\n",
        "                continue\n",
        "            for sampler in [None, \"Over\", \"Under\", \"OverUnder\"]:\n",
        "                prep = []\n",
        "                if standardizer:\n",
        "                    prep+=[(\"standardizer\", standardizer)]\n",
        "                if selector:\n",
        "                    prep+=[(\"selector\", selector)]\n",
        "                if sampler:\n",
        "                    prep+=[(\"sampling\", sampler)]\n",
        "                \n",
        "                #Start measuring the time:\n",
        "                start_time = time.time()\n",
        "                knn, best_parameter, avg_f1_scores = model_tuning((\"kNN\", parameter), folded_data=folds, f1 = \"macro\", preprocessing_steps = prep)\n",
        "                duration = time.time() - start_time                   \n",
        "                #print(\"F1 macro score: \", avg_f1_scores)\n",
        "                \n",
        "                #print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                \n",
        "                if avg_f1_scores > best_knn_f1_macro:\n",
        "                    best_knn_f1_macro = avg_f1_scores\n",
        "                    best_f1_macro_knn = (knn, best_parameter, standardizer, selector, sampler, duration)\n",
        "                    print(\"==============================================================================================================\")\n",
        "                    print(\"Best F1 macro score so far: \", avg_f1_scores)\n",
        "                    print(\"Achieved with: {parameter = \", best_parameter, \", standardizer = \", standardizer,\", selector = \", selector, \", sampling = \", sampler,\"}\")\n",
        "                    print(\"==============================================================================================================\")\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.6772727272727272\n",
            "Achieved with: {parameter =  1 , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.6863636363636364\n",
            "Achieved with: {parameter =  1 , standardizer =  minmax , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.718181818181818\n",
            "Achieved with: {parameter =  3 , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.7545454545454546\n",
            "Achieved with: {parameter =  5 , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.7681818181818182\n",
            "Achieved with: {parameter =  7 , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 micro score so far:  0.781818181818182\n",
            "Achieved with: {parameter =  17 , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5557692727351546\n",
            "Achieved with: {parameter =  1 , standardizer =  None , selector =  None , sampling =  None }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5573348150502225\n",
            "Achieved with: {parameter =  3 , standardizer =  None , selector =  None , sampling =  OverUnder }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5577130576395282\n",
            "Achieved with: {parameter =  7 , standardizer =  minmax , selector =  None , sampling =  Under }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.559662445560783\n",
            "Achieved with: {parameter =  9 , standardizer =  None , selector =  None , sampling =  OverUnder }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5661170549749968\n",
            "Achieved with: {parameter =  9 , standardizer =  minmax , selector =  Variance Threshold , sampling =  Under }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5688136594457947\n",
            "Achieved with: {parameter =  13 , standardizer =  minmax , selector =  PCA , sampling =  Under }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.572236788126993\n",
            "Achieved with: {parameter =  19 , standardizer =  None , selector =  Variance Threshold , sampling =  Under }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5732921117036143\n",
            "Achieved with: {parameter =  21 , standardizer =  None , selector =  None , sampling =  Under }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.5998250793263653\n",
            "Achieved with: {parameter =  21 , standardizer =  None , selector =  PCA , sampling =  Under }\n",
            "==============================================================================================================\n",
            "==============================================================================================================\n",
            "Best F1 macro score so far:  0.6019961936262745\n",
            "Achieved with: {parameter =  23 , standardizer =  None , selector =  None , sampling =  Under }\n",
            "==============================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbKfQhKmMVkZ",
        "colab_type": "text"
      },
      "source": [
        "Βλέπουμε ότι με βάση το μετρικό F1-micro, ο καλύτερος συνδυασμός είναι με παράμετρο n_neighbors για τον classifier k = 17, κανένα standardization δεδομένων, κανένα selection δεδομένων, και κανένα sampling δεδομένων.\n",
        "\n",
        "Ακόμα, βλέπουμε ότι με βάση το μετρικό F1-macro, ο καλύτερος συνδυασμός είναι με παράμετρο n_neighbors για τον classifier k = 23, κανένα standardization δεδομένων, κανένα selection δεδομένων, και κανένα sampling δεδομένων.\n",
        "\n",
        "\n",
        "Ειναι ***σημαντικό*** να υπενθυμίσουμε τον μικρό αριθμό δεδομένων που έχουμε, που οδηγεί σε πολλούς διαφορετικούς συνδυασμούς να έχουν τα ίδια αποτελέσματα. Γενικά (από δικές μας δοκιμές), καταλήγουμε ότι πολλοί συνδυασμοί παραμέτρων που περιέχουν τεχνικές preprocessing έχουν το ίδιο αποτέλεσμα με συνδυασμούς που κάνουν λίγο/καθόλου preprocessing δεδομένων, αλλά επειδή τα δεδομένα είναι ακριβώς τόσο λίγα, δεν μπορεί να φανεί η αξία του preprocessing δεδομένων."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODHVP_DHOPp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = (train, train_labels, test,  test_labels)\n",
        "\n",
        "(dummy_micro, best_parameter1, standardizer1, selector1, sampler1, duration1) = best_f1_micro_dummy \n",
        "(dummy_macro, best_parameter2, standardizer2, selector2, sampler2, duration2) = best_f1_macro \n",
        "\n",
        "\n",
        "(knn_micro, best_parameter3, standardizer3, selector3, sampler3, duration3) = best_f1_micro_knn\n",
        "(knn_macro, best_parameter4, standardizer4, selector4, sampler4, duration4) = best_f1_macro_knn \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzgylsFZSgUQ",
        "colab_type": "text"
      },
      "source": [
        "Τώρα θα κάνουμε εκτίμηση στο test set και για τους 4, θα βρούμε το confusion matrix τους, και θα τυπώσουμε το f1-micro και f1-macro average:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1CxJKokOZZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9b40eef-20f5-432b-c4ed-d8bd53163884"
      },
      "source": [
        "preds3 = dummy_micro.predict(test)\n",
        "preds4 = dummy_macro.predict(test)\n",
        "preds5 = knn_micro.predict(test)\n",
        "preds6 = knn_macro.predict(test)\n",
        "\n",
        "#print the accuracies:\n",
        "print(\"The average testing accuracy of the micro dummy classifier is: \", accuracy_score(test_labels, preds3))\n",
        "print(\"The average testing accuracy of the macro dummy classifier is: \", accuracy_score(test_labels, preds4))\n",
        "print(\"The average testing accuracy of the micro kNN classifier is: \", accuracy_score(test_labels, preds5))\n",
        "print(\"The average testing accuracy of the macro kNN classifier is: \", accuracy_score(test_labels, preds6))\n",
        "\n",
        "#plot the confusion matrices:\n",
        "disp1 = plot_confusion_matrix(dummy_micro, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "disp1 = plot_confusion_matrix(dummy_macro, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "disp1 = plot_confusion_matrix(knn_micro, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "disp1 = plot_confusion_matrix(knn_macro, test, test_labels,\n",
        "                                 display_labels=[\"unhealthy\", \"healthy\"],\n",
        "                                 cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "#print the classification reports, where f1 macro average is shown:\n",
        "print(classification_report(test_labels, preds3, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "print(classification_report(test_labels, preds4, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "print(classification_report(test_labels, preds5, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "print(classification_report(test_labels, preds6, target_names=[\"unhealthy\", \"healthy\"]))\n",
        "\n",
        "\n",
        "#print the micro dummy scores:\n",
        "print(\"F1 micro score for the  micro dummy classifier is: \", f1_score(test_labels, preds3, average = 'micro'))\n",
        "print(\"F1 macro score for the  micro dummy classifier is: \", f1_score(test_labels, preds3, average = 'macro'))\n",
        "\n",
        "#print the macro dummy scores:\n",
        "print(\"F1 micro score for the  macro dummy classifier is: \", f1_score(test_labels, preds4, average = 'micro'))\n",
        "print(\"F1 macro score for the  macro dummy classifier is: \", f1_score(test_labels, preds4, average = 'macro'))\n",
        "\n",
        "#print the micro knn scores:\n",
        "print(\"F1 micro score for the  micro kNN classifier is: \", f1_score(test_labels, preds5, average = 'micro'))\n",
        "print(\"F1 macro score for the  micro kNN classifier is: \", f1_score(test_labels, preds5, average = 'macro'))\n",
        "\n",
        "#print the macro knn scores:\n",
        "print(\"F1 micro score for the  macro kNN classifier is: \", f1_score(test_labels, preds6, average = 'micro'))\n",
        "print(\"F1 macro score for the  macro kNN classifier is: \", f1_score(test_labels, preds6, average = 'macro'))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average testing accuracy of the micro dummy classifier is:  0.7241379310344828\n",
            "The average testing accuracy of the macro dummy classifier is:  0.6379310344827587\n",
            "The average testing accuracy of the micro kNN classifier is:  0.7586206896551724\n",
            "The average testing accuracy of the macro kNN classifier is:  0.7068965517241379\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEGCAYAAAAt9v2AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrrsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEGCAYAAAAt9v2AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEKCAYAAABaND37AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7gAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAEGCAYAAAAt9v2AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnHjNzHLmxGtmljMnXjOznP1/oRg8gUSW55IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.00      0.00      0.00        16\n",
            "     healthy       0.72      1.00      0.84        42\n",
            "\n",
            "    accuracy                           0.72        58\n",
            "   macro avg       0.36      0.50      0.42        58\n",
            "weighted avg       0.52      0.72      0.61        58\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.33      0.31      0.32        16\n",
            "     healthy       0.74      0.76      0.75        42\n",
            "\n",
            "    accuracy                           0.64        58\n",
            "   macro avg       0.54      0.54      0.54        58\n",
            "weighted avg       0.63      0.64      0.63        58\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.67      0.25      0.36        16\n",
            "     healthy       0.77      0.95      0.85        42\n",
            "\n",
            "    accuracy                           0.76        58\n",
            "   macro avg       0.72      0.60      0.61        58\n",
            "weighted avg       0.74      0.76      0.72        58\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   unhealthy       0.47      0.50      0.48        16\n",
            "     healthy       0.80      0.79      0.80        42\n",
            "\n",
            "    accuracy                           0.71        58\n",
            "   macro avg       0.64      0.64      0.64        58\n",
            "weighted avg       0.71      0.71      0.71        58\n",
            "\n",
            "F1 micro score for the  micro dummy classifier is:  0.7241379310344829\n",
            "F1 macro score for the  micro dummy classifier is:  0.42000000000000004\n",
            "F1 micro score for the  macro dummy classifier is:  0.6379310344827587\n",
            "F1 macro score for the  macro dummy classifier is:  0.5377609108159392\n",
            "F1 micro score for the  micro kNN classifier is:  0.7586206896551724\n",
            "F1 macro score for the  micro kNN classifier is:  0.6073500967117988\n",
            "F1 micro score for the  macro kNN classifier is:  0.7068965517241379\n",
            "F1 macro score for the  macro kNN classifier is:  0.6400146038700256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yk1a5T2Sn0N",
        "colab_type": "text"
      },
      "source": [
        "Παρακάτω φαίνονται οι χρόνοι εκτέλεσης. Παρατηρούμε τον μεγάλο χρόνο για τον kNN, που οφείλεται, στην πρόβλεψη λόγω του υπολογισμού των αποστάσεων."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtE-VQbxSxKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f3ac6b69-52cd-4bf8-adb1-cf71d973ea16"
      },
      "source": [
        "print(\"micro dummy train and predict time: \", duration1, \" sec\")\n",
        "print(\"macro dummy train and predict time: \", duration2, \" sec\")\n",
        "print(\"micro knn train and predict time: \", duration3, \" sec\")\n",
        "print(\"macro knn train and predict time: \", duration4, \" sec\")\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "micro dummy train and predict time:  0.011401176452636719  sec\n",
            "macro dummy train and predict time:  0.03397345542907715  sec\n",
            "micro knn train and predict time:  1.0483341217041016  sec\n",
            "macro knn train and predict time:  1.0571017265319824  sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZVL01YWV6vY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e1f741e2-6ffe-4873-8585-8a1bac686e02"
      },
      "source": [
        "n_groups = 4\n",
        "f1_micro = (f1_score(test_labels, preds3, average = 'micro'), \n",
        "            f1_score(test_labels, preds4, average = 'micro'), \n",
        "            f1_score(test_labels, preds5, average = 'micro'), \n",
        "            f1_score(test_labels, preds6, average = 'micro'))\n",
        "\n",
        "f1_macro = (f1_score(test_labels, preds3, average = 'macro'), \n",
        "            f1_score(test_labels, preds4, average = 'macro'), \n",
        "            f1_score(test_labels, preds5, average = 'macro'), \n",
        "            f1_score(test_labels, preds6, average = 'macro'))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(n_groups)\n",
        "bar_width = 0.3\n",
        "opacity = 0.7\n",
        "\n",
        "rects1 = plt.bar(index, f1_micro, bar_width,\n",
        "alpha=opacity,\n",
        "color='b',\n",
        "label='F1 micro ')\n",
        "\n",
        "rects2 = plt.bar(index + bar_width, f1_macro, bar_width,\n",
        "alpha=opacity,\n",
        "color='g',\n",
        "label='F1 macro')\n",
        "\n",
        "plt.xlabel('Classifier')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Scores by classifier')\n",
        "plt.xticks(index + bar_width, (\"Micro Dummy\", \"Macro Dummy\", \"Micro kNN\", \"Macro kNN\"))\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nixR+yAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpAaW4LhWkJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "7bccc2ac-95be-4799-89c0-98e8e660fd6a"
      },
      "source": [
        "n_groups = 2\n",
        "before_micro = (f1_score(test_labels, preds1, average = 'micro'), \n",
        "                f1_score(test_labels, preds3, average = 'micro'))\n",
        "\n",
        "after_micro = (f1_score(test_labels, preds2, average = 'micro'),\n",
        "               f1_score(test_labels, preds5, average = 'micro'))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(n_groups)\n",
        "bar_width = 0.3\n",
        "opacity = 0.7\n",
        "\n",
        "rects1 = plt.bar(index, before_micro, bar_width,\n",
        "alpha=opacity,\n",
        "color='b',\n",
        "label='dummy')\n",
        "\n",
        "rects2 = plt.bar(index + bar_width, after_micro, bar_width,\n",
        "alpha=opacity,\n",
        "color='g',\n",
        "label='kNN')\n",
        "\n",
        "plt.xlabel('Classifier')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('F1 micro Score improvement by classifier')\n",
        "plt.xticks(index + bar_width, (\"Before\", \"After\"))\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3de5xVdb3/8debuyJColIKOpRY4gVR\nzEumg3lPGe2ikmb+zMxT2ukCp06Q2cXH6XI0/Z3sgtUPb0leSqFQ7OIctaIGvIUQSmgxmhcQFQxU\n5PP7Y30Ht9sZZg8za/baM+/n4zGPWZfvWuuz1t5rf/b3u9b6bkUEZmZmRdOn2gGYmZm1xgnKzMwK\nyQnKzMwKyQnKzMwKyQnKzMwKyQnKzMwKyQnK2iXpIUn11Y6jGiSdLumOasfRk0i6SNK1OW/jLEn3\n5Lj+2yR9pGT865JWSnpS0i6S1krqm9f2ewsnqBoi6TFJ69Kbv+VvpzRvhqSlkjZKOqsrtxsRe0ZE\nY1euszWSDpX0B0nPS3pW0u8lHZD3djcnIq6LiKOrGUMtkVQnKST1q3YseYqI4yLiKgBJuwCfA8ZG\nxJsj4h8RsU1EvFrdKGufE1TtOTG9+Vv+nkjTHwA+Adxbxdg26egHlKRtgV8C/wNsB+wMfAV4qYvj\nKuy32iLHZpu1C7AqIp7u7Ip6emLvKCeoHiIiroiI3wLr2ysraaak76VmirWppvJmSZdJWi3pr5LG\nl5R/TNKRabivpC9K+pukNZIWShqV5oWkT0p6BHgkTTtEUlOqFTVJOqSNsHZP+3F9RLwaEesi4o6I\neLAkjo9JWpK2u1jSfmn6HpIaJT2XmiMnle3r9yXNlfQiMFHSQEn/Lekfkp6S9ANJW7VxrF7XVJT2\n8ROSHklxfE3S21LN7wVJN0gakMrWS2pOx2tlOo6ntxPbUElXS3pG0t8lTZfUJ8X8nKS9SpbfIdWo\nd0zjJ0i6P5X7g6R9yl7DqZIelPSipB9LGpHeA2sk/UbSm0rKH5TW8ZykB1TSxJuO9dfS+2aNpDsk\nbZ9m35X+P5feWwe38XoPkvSztPy9ksaldU+VdHPZa/B/JV3exuszStLP0/FaJem7bZS7XNKK9Bot\nlPTuknnvlLQgzXtK0qVp+iBJ16b1PpfevyNKjsE5ys6LXwM7pf2dqbJaZHpNfyzpn5IeV9Yc2DfN\nOysdx+9IWgVc1Mbx6p0iwn818gc8BhzZTpl7gLPaKTMTWAnsDwwCfgc8CpwJ9AW+DtzZ2naBqcBf\ngLcDAsYBw9O8IDtZtwO2Sv9XAx8G+gGT0/jwVmLaFlgFXAUcB7ypbP4HgceBA9J2dwN2BfoDy4Av\nAgOAI4A1wNtL9vV54F1kX8gGAd8BZqf4hgBzgP9q41idBdxTMh7ArSnePclqeL8F3goMBRYDH0ll\n64ENwKXAQOBw4MV2Yrs6rX8IUAc8DHw0lf8JcHFJLJ8Ebk/D44GngQPTa/iR9LoNLHkN5wMjyGqn\nT5PVtseXvAe+nMrunF6L41NcR6XxHdL8RuBvZF8qtkrj30jz6tIx6reZ999FwCvAB9LrN4Xs/dcf\neEs6RsNS2X4p1v1bWU9fspaD7wCD034c2sbrdgYwPK3vc8CTwKA074/Ah9PwNsBBafjjZO+NrdO2\n9ge2LTkG55S8zs0l23rdMQB+Afwwxbgj8Gfg4yVxbgAuSLFtVe3PmSL9VT0A/3Xgxco+ZNYCz6W/\nW1opU2mCurJk/AJgScn43sBzZdttSVBLgYY21hvAESXjHwb+XFbmj23FB+yRYmtOJ+1sYESaNw/4\n91aWeXf6sOlTMu164KKSfb26ZJ7SB+DbSqYdDDzaRkzlH3QBvKtkfCHw+ZLxS4DL0nB92o/BJfNv\nAL7URmx9gZfJrmW0TPs40JiGjwT+VjLv98CZafj7wNfKYl8KHF7yGp5eMu9m4Ptl74Fb0vDngWvK\n1jWP1xJvIzC9ZN4neC1R1lFZgppfMt4H+Cfw7jR+G/CxNHwCsLiN9RwMPNPatspft1bmrwbGpeG7\nyJqTty8rczbwB2CfVpZvpIIERfaF4CVKEg/ZF7U7S+L8x+bO19785ya+2nNSRAxLfyd1Yj1PlQyv\na2V8mzaWG0X27bktK0qGdwL+Xjb/72Tf0N8gIpZExFkRMRLYKy1/WTvb3QlYEREbN7ON0ph2IPtG\nvDA12zwH3J6mV6ojx251RLxYFttObcS2PVkt4u9l5Vv25U5ga0kHSqoD9iX7dg5ZbfJzLfuU9mtU\n2bYqjXtX4INl6zqUrHbT4smS4X/R9vulLZv2O712zSWxXkVW4yH9v6aNdYwC/h4RG9rbmKQpypqH\nn0/7M5TseAN8lKw2+NfUjHdCmn4NWWKeJekJSd+S1L/yXQReq+X/s+RY/pCsJtViRatLmhOUddgK\n4G2bmV/aPf4TZCdoqV3Imuo2KyL+SlbDaLnm0tZ2nwBGSSp9L5dvozSmlWQfxnuWJPqhEdHRD9hK\nvUnS4LLYnigZL4/tFV5/zDbtS2R3hd1A9g18MvDLiFiTyq0ga/4bVvK3dURcvwUxryCrQZWua3BE\nfKOCZSv9eYRRLQPptRvJa8flFmCfdL3tBOC6zcS5i9q5sSBdb/oP4BSypuNhZE2rAoiIRyJiMlnS\n+CZwk6TBEfFKRHwlIsYCh6RYzqxw/0pjfImsdtZyLLeNiD1LyvgnJdrgBNVDSBogaRDZSdc/XeDN\n4/X9EfA1SWOU2UfS8DbKzgV2l/QhSf0knQqMJbtbrzz+d0j6nKSRaXwU2Yfw/JLtTpG0f9rubpJ2\nBf5E9g3+PyT1TxfzTwRmtRZQ+rZ+JfAdvXZzwc6Sjtmio1GZr6TX591kH3I3thFbSwK6WNKQtH+f\nBUqfGfopcCpwehpucSVwXqpdSdJgSe+VNGQL4r0WOFHSMcpuihmk7IaPkRUs+wywkeya3ObsL+l9\nKbl8muxDfD5ARKwHbiLbvz9HxD/aWMefyZoGv5H2d5Ckd7VSbghZU+szQD9JF5JdQwRA0hmSdkjv\njefS5I2SJkraO93Q8ALZl4eNdEBE/BO4A7hE0rbKbnh5m6TDO7Ke3soJque4g6xmcAgwIw0flsN2\nLiX7EL2D7KT9MdmF8jeIiFVkH8ifI7vI/h/ACRGxspXia8gu8P9J2R1t84FFaVki4kbgYrIPrTVk\n37K3i4iXyRLScWQ1kO+RXZf562b24fNkN1bMl/QC8Buymz7y8CTZ9Y4nyGoC57UT2wVk18iWk11P\n/CnZzREARMSf0vydyK7VtExfAHwM+G7a3jKy6xsdFhErgAayG0+eIasFTKWCz4uI+BfZ6/T71KR1\nUBtFbyVLtC030bwvIl4pmX8V2bXQtpr3WhL6iWQ3zPyDrJnw1FaKziNrxn2YrMl0Pa9vVjsWeEjS\nWuBy4LSIWAe8mSxRvgAsAf53c/FsxplkN/AsJtvfm3h9c6m1QelCnZl1sVSbuzZdU7MOUPbw61+B\nN0fEC9WOx6rDNSgzK5TUNP1ZYJaTU++WW4KS9BNJT0ta1MZ8KXsAb5myhwf3yysWM6sN6YaSF8ie\nvfpylcOxKsutiU/SYWTP7FwdEXu1Mv94svb248muPVweEQfmEoyZmdWc3GpQEXEX8OxmijSQJa+I\niPnAMEm+cGhmZkD2pHO17Mzr76RpTtP+WV5Q0rnAuQBbbbXV/qNGjSovYjVk48aN9Onjy59mXaEn\nnE8PP/zwyoh4w8PyNdFzbkTMILt1mgkTJsSCBQuqHJF1RmNjI/X19dUOw6xH6Annk6TyHmeA6t7F\n9zglT5OTPUnebg8DZmbWO1QzQc0Gzkx38x0EPJ+eujYzM8uviU/S9WS9/G4vqZnsltH+ABHxA7Ju\ncI4ne+L9X8D/ySsWMzOrPbklqNT54ubmB9nv2ZiZ1bxXXnmF5uZm1q9v9zdDu9TQoUNZsmRJt25z\nSw0aNIiRI0fSv39lncLXxE0SZmZF19zczJAhQ6irq0NSt213zZo1DBmyJX0Cd6+IYNWqVTQ3NzN6\n9OiKlqntexPNzApi/fr1DB8+vFuTUy2RxPDhwztUw3SCMjPrIk5Om9fR4+MEZWZmheRrUGZmOTjx\nxK5d35w5HSt/0UUXsc022zBlypSuDaQbuQZlZmaF5ARlZtZDXHzxxey+++4ceuihLF26FID6+npa\nuodbuXIldXV1AMycOZOTTjqJo446irq6Or773e9y6aWXMn78eA466CCeffbZTct/5jOfYcKECeyx\nxx40NTXxvve9jzFjxjB9+nQALrzwQi677LJNcUybNo3LL7+80/vjBGVm1gMsXLiQWbNmcf/99zN3\n7lyampraXWbRokX8/Oc/p6mpiWnTprH11ltz3333cfDBB3P11VdvKjdgwAAWLFjAeeedR0NDA1dc\ncQWLFi1i5syZrFq1irPPPntT+Y0bNzJr1izOOOOMTu+Tr0GZmfUAd999NyeffDJbb701AJMmTWp3\nmYkTJzJkyBCGDBnC0KFDOTFdONt777158MEHN5VrWdfee+/NnnvuyVvekv0y0lvf+lZWrFjBvvvu\ny/Dhw7nvvvt46qmnGD9+PMOHD+/0PjlBmZn1YP369WPjxo0Ab3gGaeDAgZuG+/Tps2m8T58+bNiw\n4Q3lSsuUlzvnnHOYOXMmTz75JGeffXaXxO4mPjOzHuCwww7jlltuYd26daxZs4Y56ba/uro6Fi5c\nCMBNN92U2/ZPPvlkbr/9dpqamjjmmGO6ZJ2uQZmZ5aCjt4V31n777cepp57KuHHj2HHHHTnggAMA\nmDJlCqeccgozZszgve99b27bHzBgABMnTmTYsGH07du3S9aprM/W2uEfLKx9PeEH1szKLVmyhD32\n2KPbt1uUvvg2btzIfvvtx4033siYMWPaLNfacZK0MCImlJd1E5+ZmXXK4sWL2W233XjPe96z2eTU\nUW7iMzOzThk7dizLly/v8vW6BmVmZoXkBGVmZoXkBGVmZoXkBGVmZoXkmyTMzHJw4vVd+3sbcya3\n/2DVY489xgknnMCiRYs2TWtsbGTixInMnj17U1dGJ5xwAlOmTKG+vp76+nrWrl27qUPZBQsWMGXK\nFBobG7s0/i3hGpSZWQ83cuRILr744jbnP/3009x2223dGFFlnKDMzHqg5cuXM378eJqamhg3bhxD\nhw7l17/+datlp06dutkEVi1OUGZmPczSpUt5//vfz8yZMzd1eTRt2jS+/vWvt1r+4IMPZsCAAdx5\n553dGWa7nKDMzHqQZ555hoaGBq677jrGjRu3afphhx0GwD333NPqctOnT28zgVWLE5SZWQ8ydOhQ\ndtlll1YT0eZqUUcccQTr1q1j/vz5eYdYMScoM7MeZMCAAfziF7/g6quv5qc//enr5h199NGsXr36\ndT9GWGr69Ol861vf6o4wK+LbzM3MclDJbeF5GTx4ML/85S856qij+NKXvvS6edOmTaOhoaHV5Y4/\n/nh22GGH7gixIk5QZmY9RF1d3aZnoIYNG0ZTUxPw+p9/nzRpEqU/s1T+vFPLjxsWgZv4zMyskJyg\nzMyskJygzMy6SK39Qnl36+jxcYIyM+sCgwYNYtWqVU5SbYgIVq1axaBBgypexjdJmJl1gZEjR9Lc\n3MwzzzzTrdtdv359hz70q2nQoEGMHDmy4vJOUGZmXaB///6MHj2627fb2NjI+PHju3273cFNfGZm\nVki5JihJx0paKmmZpC+0Mn8XSXdKuk/Sg5KOzzMeMzOrHbklKEl9gSuA44CxwGRJY8uKTQduiIjx\nwGnA9/KKx8zMakueNah3AssiYnlEvAzMAsr71whg2zQ8FHgix3jMzKyG5HmTxM7AipLxZuDAsjIX\nAXdIugAYDBzZ2ooknQucCzBixIhC/BSxbbm1a9f6NTTrIj35fKr2XXyTgZkRcYmkg4FrJO0VERtL\nC0XEDGAGwIQJE6K+vr77I7Uu09jYiF9Ds67Rk8+nPJv4HgdGlYyPTNNKfRS4ASAi/ggMArbPMSYz\nM6sReSaoJmCMpNGSBpDdBDG7rMw/gPcASNqDLEF171NuZmZWSLklqIjYAJwPzAOWkN2t95Ckr0pq\n6fv9c8DHJD0AXA+cFe4nxMzMyPkaVETMBeaWTbuwZHgx8K48YzAzs9rkniTMzKyQnKDMzKyQnKDM\nzKyQqv0clFXBidefWNXtNwxs4JLrL6nKtudMnlOV7VrP1ZvPJ8j3nHINyszMCskJyszMCskJyszM\nCskJyszMCskJyszMCskJyszMCskJyszMCsnPQZlZp5xY3ceAqu9D1Q6g53INyszMCskJyszMCskJ\nyszMCskJyszMCskJyszMCskJyszMCskJyszMCskJyszMCskJyszMCskJyszMCskJyszMCskJyszM\nCskJyszMCskJyszMCskJyszMCskJyszMCskJyszMCskJyszMCqlX/uS7f6K62gGYmbXPNSgzMysk\nJygzMyskJygzMyskJygzMyukXBOUpGMlLZW0TNIX2ihziqTFkh6S9NM84zEzs9qR2118kvoCVwBH\nAc1Ak6TZEbG4pMwY4D+Bd0XEakk75hWPmZnVljxrUO8ElkXE8oh4GZgFNJSV+RhwRUSsBoiIp3OM\nx8zMakiez0HtDKwoGW8GDiwrszuApN8DfYGLIuL28hVJOhc4F2DEiBE0NjZ2KrCG8jTZ2wys7gEY\n1mcYDVWKobPvHXsjn0+993yCfM+paj+o2w8YA9QDI4G7JO0dEc+VFoqIGcAMgAkTJkR9fX2nNnrJ\nJZ1avPZ9qLoHoGFgA7e+dGtVtj2nfk5VttuT+XzqvecT5HtO5dnE9zgwqmR8ZJpWqhmYHRGvRMSj\nwMNkCcvMzHq5PBNUEzBG0mhJA4DTgNllZW4hqz0haXuyJr/lOcZkZmY1IrcEFREbgPOBecAS4IaI\neEjSVyVNSsXmAaskLQbuBKZGxKq8YjIzs9qR6zWoiJgLzC2bdmHJcACfTX9mZmabuCcJMzMrJCco\nMzMrJCcoMzMrpIoSlKQPShqShqdL+rmk/fINzczMerNKa1Bfiog1kg4FjgR+DHw/v7DMzKy3qzRB\nvZr+vxeYERG/AgbkE5KZmVnlCepxST8ETgXmShrYgWXNzMw6rNIkcwrZQ7XHpH7ytgOm5haVmZn1\nehUlqIj4F/A0cGiatAF4JK+gzMzMKr2L78vA58l+XBCgP3BtXkGZmZlV2sR3MjAJeBEgIp4AhuQV\nlJmZWaUJ6uXUb14ASBqcX0hmZmaVJ6gb0l18wyR9DPgNcGV+YZmZWW9XUW/mEfHfko4CXgDeDlwY\nEb/ONTIzM+vV2k1QkvoCv4mIiYCTkpmZdYt2m/gi4lVgo6Sh3RCPmZkZUPkPFq4F/iLp16Q7+QAi\n4lO5RGVmZr1epQnq5+nPzMysW1R6k8RVkgYAu6dJSyPilfzCMjOz3q6iBCWpHrgKeAwQMErSRyLi\nrvxCMzOz3qzSJr5LgKMjYimApN2B64H98wrMzMx6t0of1O3fkpwAIuJhsv74zMzMclFpDWqBpB/x\nWgexpwML8gnJzMys8gT1b8AngZbbyu8GvpdLRGZmZlSeoPoBl0fEpbCpd4mBuUVlZma9XqXXoH4L\nbFUyvhVZh7FmZma5qDRBDYqItS0jaXjrfEIyMzOrPEG9KGm/lhFJE4B1+YRkZmZW+TWoTwM3Snoi\njb8FODWfkMzMzNqpQUk6QNKbI6IJeAfwM+AV4Hbg0W6Iz8zMeqn2mvh+CLychg8GvghcAawGZuQY\nl5mZ9XLtNfH1jYhn0/CpwIyIuBm4WdL9+YZmZma9WXs1qL6SWpLYe4Dflcyr9PqVmZlZh7WXZK4H\n/lfSSrK79u4GkLQb8HzOsZmZWS+22QQVERdL+i3ZXXt3RESkWX2AC/IOzszMeq92n4OKiPkR8YuI\nKP2p94cj4t72lpV0rKSlkpZJ+sJmyr1fUqTnq8zMzCp+ULfDUn99VwDHAWOByZLGtlJuCPDvwJ/y\nisXMzGpPbgkKeCewLCKWR8TLwCygoZVyXwO+CazPMRYzM6sxed6JtzOwomS8GTiwtEDqPmlURPxK\n0tS2ViTpXOBcgBEjRtDY2NipwBpaS5O9ycDqHoBhfYbRUKUYOvvesTfy+dR7zyfI95yq2q3ikvoA\nlwJntVc2ImaQHgyeMGFC1NfXd2rbl1zSqcVr34eqewAaBjZw60u3VmXbc+rnVGW7PZnPp957PkG+\n51SeTXyPA6NKxkemaS2GAHsBjZIeAw4CZvtGCTMzg3wTVBMwRtJoSQOA04DZLTMj4vmI2D4i6iKi\nDpgPTIoI/5S8mZnll6AiYgNwPjAPWALcEBEPSfqqpEl5bdfMzHqGXK9BRcRcYG7ZtAvbKFufZyxm\nZlZb8mziMzMz22JOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZm\nVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhO\nUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZm\nVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVkhOUGZmVki5JihJx0paKmmZpC+0\nMv+zkhZLelDSbyXtmmc8ZmZWO3JLUJL6AlcAxwFjgcmSxpYVuw+YEBH7ADcB38orHjMzqy151qDe\nCSyLiOUR8TIwC2goLRARd0bEv9LofGBkjvGYmVkN6ZfjuncGVpSMNwMHbqb8R4HbWpsh6VzgXIAR\nI0bQ2NjYqcAaGtov06MNrO4BGNZnGA1ViqGz7x17I59Pvfd8gnzPqTwTVMUknQFMAA5vbX5EzABm\nAEyYMCHq6+s7tb1LLunU4rXvQ9U9AA0DG7j1pVursu059XOqst2ezOdT7z2fIN9zKs8E9TgwqmR8\nZJr2OpKOBKYBh0fESznGY2ZmNSTPa1BNwBhJoyUNAE4DZpcWkDQe+CEwKSKezjEWMzOrMbklqIjY\nAJwPzAOWADdExEOSvippUir2bWAb4EZJ90ua3cbqzMysl8n1GlREzAXmlk27sGT4yDy3b2Zmtcs9\nSZiZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZ\nWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5\nQZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZ\nWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSHlmqAkHStpqaRlkr7QyvyBkn6W\n5v9JUl2e8ZiZWe3ILUFJ6gtcARwHjAUmSxpbVuyjwOqI2A34DvDNvOIxM7PakmcN6p3AsohYHhEv\nA7OAhrIyDcBVafgm4D2SlGNMZmZWIxQR+axY+gBwbESck8Y/DBwYEeeXlFmUyjSn8b+lMivL1nUu\ncG4afTuwNJegrbtsD6xst5SZVaInnE+7RsQO5RP7VSOSjoqIGcCMasdhXUPSgoiYUO04zHqCnnw+\n5dnE9zgwqmR8ZJrWahlJ/YChwKocYzIzsxqRZ4JqAsZIGi1pAHAaMLuszGzgI2n4A8DvIq82RzMz\nqym5NfFFxAZJ5wPzgL7ATyLiIUlfBRZExGzgx8A1kpYBz5IlMev53Fxr1nV67PmU200SZmZmneGe\nJMzMrJCcoMzMrJCcoKzDJL0q6X5JD0i6V9IhFSzzKUlLJF3XHTGa1TpJJ0kKSe8omfZtSQ+l/ye1\n0jtPj+JrUNZhktZGxDZp+BjgixFxeDvL/BU4suWh7Aq20S8iNnQ+WrPaJOlnwE5kdzd/OU17Htgu\nIl6VNBP4ZUTc1IF11tR55RqUdda2wOqWEUlTJTVJelDSV9K0HwBvBW6T9BlJ20m6JZWZL2mfVO4i\nSddI+j3Z3Z190zfFlvV9vBo7aNbdJG0DHErWX+lpadpsYBtgoaQvA5OAb6fWjLelv9slLZR0d0vN\nS9JMST+Q9CfgW9XZoy1TEz1JWOFsJel+YBDwFuAIAElHA2PI+mEUMFvSYRFxnqRjgYkRsVLS/wD3\nRcRJko4Argb2TeseCxwaEetSF1fPR8QBkgYCv5d0R0Q82q17a9b9GoDbI+JhSask7R8Rk1Lrxb4A\nkkZTUoOS9FvgvIh4RNKBwPdI5yZZRwmHRMSrVdiXLeYEZVtiXclJcjBwtaS9gKPT332p3DZkCeuu\nsuUPBd4PEBG/kzRc0rZp3uyIWJeGjwb2Sf06QtbTyBjACcp6usnA5Wl4Vhpf2FbhVOM6BLixpL/t\ngSVFbqy15AROUNZJEfFHSdsDO5DVmv4rIn7YiVW+WDIs4IKImNeZGM1qiaTtyGo+e0sKso4OQtLU\nzSzWB3iu5YtjK15sY3qh+RqUdUpq5+5L1ofiPODs9G0OSTtL2rGVxe4GTk9l6oGVEfFCK+XmAf8m\nqX8qu7ukwV2/F2aF8gHgmojYNSLqImIUWavBu8vKrQGGAKTz51FJHwRQZlx3Bp0H16BsS7Rcg4Ks\nlvOR1Hxwh6Q9gD+mZoa1wBnA02XLXwT8RNKDwL94rT/Gcj8C6oB70++EPQOc1IX7YVZEk3njj7fe\nnKaXmgVcKelTZEntdOD7kqYD/dP8B3KONVe+zdzMzArJTXxmZlZITlBmZlZITlBmZlZITlBmZlZI\nTlBmZlZITlBmnSTpzZJmSfpb6gdtbnpma1EXbuOrko5Mw+9OPVrfn541q7izULNa4tvMzTohPZ/1\nB+CqiPhBmjaOrBPd70fEXjls8wfAPRFx7RYsW1O9WVvv5hqUWedMBF5pSU4AEfEAsKJlXFJd6l36\n3tLfz5L0Fkl3pZrQolQz6pt6n14k6S+SPpPKzpT0AUnnAKcAX5N0XVr3olSm1d7fJdWn7c8GFnfb\nkTHrJPckYdY5e7GZTjyTp4GjImK9pDHA9cAE4EPAvIi4WFJfYGuyXt13bql5SRpWuqKI+JGkQ0m9\nWEuqK5n9UVrp/T3N2w/YyzxSWFkAAAEkSURBVD3BWy1xgjLLX3/gu5L2BV4Fdk/Tm8i6fOoP3BIR\n90taDrw1/STJr4A7Wl1j69rq/f1l4M9OTlZr3MRn1jkPAfu3U+YzwFPAOLKa0wCAiLgLOAx4HJgp\n6cyIWJ3KNQLnkfVHWKmW3t/3TX+jI6IlwdVkb9bWuzlBmXXO74CB6ccVAVD2C8GjSsoMBf4ZERuB\nD5P1/o6kXYGnIuJKskS0X/rpkj4RcTMwnaxprlLu/d16FDfxmXVCRISkk4HLJH0eWA88Bny6pNj3\ngJslnQnczmu1mXpgqqRXyHp+PxPYGfh/klq+PP5nB8Jx7+/Wo/g2czMzKyQ38ZmZWSE5QZmZWSE5\nQZmZWSE5QZmZWSE5QZmZWSE5QZmZWSE5QZmZWSH9f6ytpQuKOTY7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-baQVvuPXswA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "0f426c22-c35f-4c4f-c152-32e956212edc"
      },
      "source": [
        "n_groups = 2\n",
        "before_macro = (f1_score(test_labels, preds1, average = 'macro'), \n",
        "                f1_score(test_labels, preds2, average = 'macro'))\n",
        "\n",
        "after_macro = (f1_score(test_labels, preds4, average = 'macro'),\n",
        "               f1_score(test_labels, preds6, average = 'macro'))\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "index = np.arange(n_groups)\n",
        "bar_width = 0.3\n",
        "opacity = 0.7\n",
        "\n",
        "rects1 = plt.bar(index, before_macro, bar_width,\n",
        "alpha=opacity,\n",
        "color='b',\n",
        "label='F1 macro (before)')\n",
        "\n",
        "rects2 = plt.bar(index + bar_width, after_macro, bar_width,\n",
        "alpha=opacity,\n",
        "color='g',\n",
        "label='F1 macro (after)')\n",
        "\n",
        "plt.xlabel('Classifier')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('F1 macro Score improvement by classifier')\n",
        "plt.xticks(index + bar_width, (\"dummy\", \"kNN\"))\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZnlkgPKzMxyyQFlZma59L8Sn+yG9L8JJgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEek1R-Md7i2",
        "colab_type": "text"
      },
      "source": [
        "Πλέον βλέπουμε μια πιο ξεκάθαρη διαφορά μεταξύ του kNN και του dummy, τόσο σε μέγιστες τιμές F1 micro average όσο και σε F1 macro average. \n",
        "\n",
        "Στο πρώτο ραβδόγραμμα βλέπουμε ότι το kNN προηγείται τόσο σε απόδοση, όσο και σε συνέπεια μεταξύ των τιμών F1 micro average και σε F1 macro average. \n",
        "\n",
        "Αυτό μας δείχνει ότι είναι ένας σαφώς ανώτερος εκτιμητής, που όμως είναι πιο δύσκολο να εκπαιδευθεί, κάτι που αποδεικνύεται και από τους χρόνους (~100 φορές πιο αργό). \n",
        "\n",
        "Στα διαγράμματα διαφορών πριν και μετά της βελτιστοποίησης πήραμε για το μετά (στην εκάστοτε μετρική F1) τον καλύτερο δυνατό classifier μετά την βελτιστοποίηση, για να αναδείξουμε την μεγαλύτερη δυνατή βελτίωση. \n",
        "\n",
        "Όσον αφορά το F1 micro average, παρατηρούμε ότι ενώ ο dummy classifier έχει μια σημαντική βελτίωση, ο kNN δεν έχει τόσο μεγάλη βελτίωση. \n",
        "\n",
        "Επιπλέον, όσον αφορά το F1 macro average, βλέπουμε ότι ενώ και οι δύο classifiers έχουν μια μικρή αύξηση, ο kNN έχει μια πιο μεγάλη αύξηση, συγκρινόμενος με τον dummy. \n",
        "\n",
        "Μπορούμε να συμπεράνουμε ότι με μεγαλύτερο dataset, ίσως η διαφορά να κρύβεται στην προεπεξεργασία, που στην προκειμένη περίπτωση δεν είχε μεγάλο ρόλο, αφού λόγω του μικρού αριθμού δειγμάτων, η σημασία κάθε δείγματος είναι αρκετά μεγάλη,\n",
        "άρα η αφαίρεση δεδομένων δεν είχε λόγο να γίνεται. "
      ]
    }
  ]
}