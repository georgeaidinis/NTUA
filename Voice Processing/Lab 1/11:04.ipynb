{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mylast.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9_Jhh4FN2Hqv",
        "KlLLoZbp3tWO",
        "mmmQB8J08h6y",
        "Yy1kN1yI9uWR",
        "IRI8GJmQ_Fq5",
        "_6i9AWk5Teg3",
        "RuzupFrrggeN",
        "-_kxWWizgiHR",
        "LTVMz5ju2wKx",
        "5REbZDoF90H7",
        "qnbDRI1UV1P1",
        "1ouXqDv4ipnt",
        "_mBc-yzgWMQL",
        "sGEGAW4yuClW",
        "NAOXXVKl9jcG"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeaidinis/NTUA/blob/master/Voice%20Processing/Lab%201/11%3A04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM0mb3ATwibH",
        "colab_type": "text"
      },
      "source": [
        "> Αϊδίνης Γιώργος  (el16031)\n",
        "\n",
        "> Τσιλιβής Θοδωρής (el16032)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 1η Εργαστηριακή Άσκηση - Μέρος πρώτο: Ορθογράφος"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCBjt0YXxvDZ",
        "colab_type": "text"
      },
      "source": [
        "Πριν αρχίσουμε τα βήματα για την προετοιμασία του εργαστηρίου, πρέπει να εγκαταστήσουμε κάποια εργαλεία που θα χρησιμοποιήσουμε στην εργασία αυτή. Πρώτα είναι η βιβλιοθήκη OpenFST (1.6.1), για την οποία χρησιμοποιούμε το έτοιμο bash script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqpOG9a7HVEU",
        "colab_type": "code",
        "outputId": "f09225d4-2fc9-41bd-ff10-9e7782e41e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "download(\"https://raw.githubusercontent.com/georgepar/python-lab/master/install_openfst.sh\",\"install_openfst.sh\")\n",
        "print(\"All the files are downloaded\")"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hY00fgBuCMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod +x install_openfst.sh\n",
        "!./install_openfst.sh &> /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcwet47Oyboe",
        "colab_type": "text"
      },
      "source": [
        "Ακόμα, θα πρέπει να εγκαταστήσουμε τις βιβλιοθήκες του NLTK (Natural Language Toolkit) καθώς και του graphviz, που μας επιτρέπει να σχεδιάζουμε τα FST που δημιουργούμε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjbkzLreyQgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "!sudo apt install python-pydot python-pydot-ng graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Jhh4FN2Hqv",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 1ο\n",
        "\n",
        "Για το Βήμα αυτό, επιλέξαμε και τα δύο προτεινόμενα βιβλία από το Project Guternberg, για να έχουμε περισσότερα δεδομένα.\n",
        "\n",
        "Το τελικό αρχείο, ονομάζεται corpora.txt . \n",
        "\n",
        "Ιδανικά, θα θέλαμε να είχαμε όσο το δυνατόν περισσοτερα δεδομενα, καθως αυτο θα μας επετρεπε να εχουμε ευρυτερο λεξιλογιο, δηλαδη να αναγνωριζει το συστημα μας περισσοτερες λεξεις. Ακομα, θα μπορουσαμε να εχουμε στην διαθεση μας διαφορετικες γραφες των ιδιων λεξεων, που ειναι χρησιμο σε εξειδικευμενους ορθογραφους."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBPwKdZ9DdVq",
        "colab_type": "code",
        "outputId": "29b5af7d-feb4-4705-bcb0-ee30e41bfcb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "download(\"http://www.gutenberg.org/files/1661/1661-0.txt\",\"corpora.txt\")\n",
        "print(\"All the files are downloaded\")"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pINLD3af4CCM",
        "colab_type": "text"
      },
      "source": [
        "Προσδιοριζουμε την κωδικοποιηση χαρακτηρων σε UTF-8, για να αναγνωρισθουν τυχον \"περιεργοι\" χαρακτηρες.\n",
        "\n",
        "Ακομα, ειναι χρησιμο να κανουμε import τωρα ο,τι βιβλιοθηκες της python θα χρησιμοποιησουμε αργοτερα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY7KEix24A7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "import itertools\n",
        "from nltk.tokenize.regexp import regexp_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlLLoZbp3tWO",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 2ο\n",
        "\n",
        "\n",
        "\n",
        "> α)  Γραφουμε μια συναρτηση που την ονομαζουμε identity_preprocess: (αρκετα απλο)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYDfatHm4up3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Reads a string and returns it. Returns string.\"\"\"\n",
        "def identity_preprocess(stringaki):\n",
        "\treturn stringaki\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JWABj-q5Dn0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> β) Γραφουμε μια συναρτηση που να δεχεται δυο ορισματα, το ονομα αρχειου, και μια συναρτηση της επιλογης μας και να διαβαζει το αρχειο γραμμη - γραμμη καλωντας την συναρτηση που εχουμε δωσει σαν ορισμα σε καθε γραμμη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYUlckQY5esE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes 2 arguments, one being the filepath and the second\n",
        "being the name of the method that reads the string, defaulting\n",
        "to the above method, 'identity_preprocess'. Returns a list of\n",
        "strings, that are the whole text of the file.\"\"\"\n",
        "def read_from_file(filepath, myfunc = identity_preprocess):\n",
        "\tlines = []\n",
        "\twith open(filepath) as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tlines.append(myfunc(line))\n",
        "\treturn lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGCff1_15jiw",
        "colab_type": "text"
      },
      "source": [
        "> γ) Γραφουμε εδω 2 συναρτησεις, η μια ειναι η συναρτηση clean_text που \"καθαριζει\" το κειμενο που της δινουμε απο οποιον χαρακτηρα δεν ειναι γραμμα, και στην συνεχεια, την tokenize, που παιρνει ενα τετοιο καθαρο κειμενο και το χωριζει σε μερη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOQVa4T6Su9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes one string and returns a string that has only lowercase\n",
        "letters, that means exluding any non-alphanumeric sumbols, numbers\n",
        "punctuation.\"\"\"\n",
        "def clean_text(text):\n",
        "\t#replace new line and carriage return\n",
        "\ttext = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "\t#replace the numbers, symbols and punctuation with space\n",
        "\tpunc_list = '~`!@#$£½%^&*()_+-—“”’‘\\'=}{|:à\"?>æ<âç,œ./è;é][}' + '1234567890'\n",
        "\tt = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
        "\ttext = text.translate(t)\n",
        "\t#make everything lowercase\n",
        "\ttext = text.lower()\n",
        "\treturn text\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes a text and splits it between whitespaces or \\n. Returns\n",
        "a list of strings\"\"\"\n",
        "def tokenize(text):\n",
        "\ttext = clean_text(text)\n",
        "\ttext = text.split()\n",
        "\treturn text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wiwjnso6Zbp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> δ) μια καλη συναρτηση που βρηκαμε οτι εχει καλα αποτελεσματα ειναι η εξης:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_IOTEIT6kVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"Uses the imported library from nltk to return a list of strings\n",
        "created by using regular expressions. It is one of the fastest \n",
        "tokenizers.\"\"\"\n",
        "def Tokenize(text):\n",
        "\t#text = clean_text(text)\n",
        "\treturn regexp_tokenize(text, pattern = '\\s+', gaps = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jllEoKJy6ot9",
        "colab_type": "text"
      },
      "source": [
        "Στις προσπαθειες μας, βρηκαμε οτι η ετοιμη συναρτηση ενω ειναι παρα πολυ γρηγορη, (πολυ πιο γρηγορη απο την δικη μας, ακομα και για τον σχετικα μικρο αριθμο δεδομενων μας), επιτρεπει λιγο λιγοτερες λεξεις (~170k vs ~173k) να \"περασουν\" ομως καταληγουμε με πολυ μεγαλο λεξιλογιο (κατι που θα φανει μετεπειτα): (παραθετουμε και την συναρτηση flatten, που αν και δεν ζητειται, ειναι χρησιμη καθως μετατρεπει μια λιστα λιστων σε μια λιστα)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNMTsZw7c9C",
        "colab_type": "code",
        "outputId": "5bd13fe9-2247-4887-86dd-85d864471b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "\"\"\"Takes a list of lists and returns a list. Elements are kept in\n",
        "order.\"\"\"\n",
        "def flatten(l):\n",
        "\treturn list(itertools.chain(*l))\n",
        " \n",
        "\n",
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "text1 = []\n",
        "for line in lines:\n",
        "    text.append(Tokenize(line))\n",
        "    text1.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "text1 = flatten(text1)\n",
        "print(len(text))\n",
        "print(len(text1))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "107602\n",
            "109021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmmQB8J08h6y",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 3ο\n",
        "\n",
        "\n",
        "\n",
        "> α)  Για να βρουμε το λεξιλογιο μας, φτιαχνουμε την συναρτηση unique_tokens, που παιρνει μια λιστα απο strings και αφαιρει τις πολλαπλες εμφανισεις:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSaBEEp9PgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes a list of strings (tokens) and reduces it to only unique\n",
        "elements. (gets rid of the duplicates). Returns a list of strings\n",
        "that are unique and sorted.\"\"\"\n",
        "def unique_tokens(tokens):\n",
        "\tlexicon = list(set(tokens))\n",
        "\tlexicon.sort()\n",
        "\treturn lexicon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GRjySeS9UEP",
        "colab_type": "text"
      },
      "source": [
        "> β)  Για να βρουμε το αλφαβητο του corpus, φτιαχνουμε την συναρτηση alphabet, που παιρνει μια λιστα απο strings επιστρεφει μια λιστα απο χαρακτηρες, χωρις πολλαπλες εμφανισεις:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qod5JWP4-efW",
        "colab_type": "text"
      },
      "source": [
        "Εκτελωντας τις παρακατω εντολες, μπορουμε να δουμε οτι τα λεξιλογια που δημιουργουνται απο τις διαφορετικες μεθοδους tokenization ειναι κατα πολυ διαφορετικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTPTmijl-u-f",
        "colab_type": "code",
        "outputId": "7f8ed113-ab1c-4cae-9d54-db5362ce9bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(Tokenize(line))\n",
        "text = flatten(text)\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"NLTK tokenize(): \",len(Lexicon))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"Our tokenize(): \",len(Lexicon))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK tokenize():  15221\n",
            "Our tokenize():  8082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYu1H3tI9mnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes a string and returns a list of the characters \n",
        "that were used to make the strings.\"\"\"\n",
        "def split_word(word):\n",
        "\treturn [char for char in word]\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes a list of strings as an input and returns a list of chars.\n",
        "The characters that are returned are used to make the elements of \n",
        "the input strings, but they are returned duplicate-free and sorted.\"\"\"\n",
        "def alphabet(words):\n",
        "\tchars = []\n",
        "\tfor word in words:\n",
        "\t\tchars.append(split_word(word))\n",
        "\talpbt = list(set(flatten(chars)))\n",
        "\talpbt.sort()\n",
        "\treturn alpbt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy1kN1yI9uWR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 4ο\n",
        "\n",
        "\n",
        "Για να δημιουργησουμε το αρχειο chars.syms που μας χρειαζεται για μετα, φιταχνουμε δυο συναρτησεις, μια για να κανει το (προηγουμενως δημιουργημενο) αλφαβητο indexed, δηλαδη καθε χαρακτηρας να αντιστοιχιζεται σε εναν φυσικο αριθμο, και το ε ('<epsilon>') να αντιστοιχιζεται στο 0. Ακομα, φτιαχνουμε μια συναρτηση που να παιρνει ενα τετοιο στοιχισμενο αλφαβητο και να το αποθηκευει σε ενα αρχειο της επιλογης μας (προεπιλεγμενο ονομα ειναι το \"chars.syms\"):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcfIp5xG-aSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes an alphabet (list of chars) and assigns each character \n",
        "an index. Index 0 is <epsilon> (ε). Returns a list that has n+1 \n",
        "tuples, each tuple being the character with its assigned index.\"\"\"\n",
        "def alphabet_indexing(alphabet):\n",
        "\tindexed_alphabet = []\n",
        "\tindexed_alphabet.append((\"<epsilon>\", 0))\n",
        "\ti = 1;\n",
        "\tfor letter in alphabet:\n",
        "\t\tindexed_alphabet.append((letter, str(i)))\n",
        "\t\ti+=1\n",
        "\treturn indexed_alphabet\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes two arguments. The first is an  alphabet and the \n",
        "second is the name of the file to write the output. The \n",
        "method takes the indexed alphabet and writes it to a file \n",
        "with the name given\"\"\"\n",
        "def symbols_file(alphabet, filename = \"chars.syms\"):\n",
        "\tf = open(filename, \"w+\")\n",
        "\tfor pair in alphabet:\n",
        "\t\tf.write(pair[0] + \"\\t\\t\" + str(pair[1]) + \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRI8GJmQ_Fq5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 5ο\n",
        "\n",
        "\n",
        "   α) Για to βημα 5ο, αρχικα υποθεσαμε οτι πρεπει να υλοποιησουμε εκτος των αλλων και την συναρτηση για την αποσταση Levenshtein, οποτε αν και (μετα απο ξεκαθαρισμα) δεν ειναι απαραιτητο, την παραθετουμε εδω για κριτικη, καθως και για να υπαρχει σε τυχον περιπτωση που την χρειαστουμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVaTHdqP_fOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This function computes the Levenshtein distance between two \n",
        "chars s,t. It uses the algorithm mentioned in the instructions\n",
        "of the lab, and is horribly inefficient. Takes two arguments, both\n",
        "chars and returns an integer that is the levenshtein distance of\n",
        "the two strings.\"\"\"\n",
        "def my_levenshtein(s,t):\n",
        "\t#same characters, no edit => Levenshtein cost 0\n",
        "\tif (s==t):\n",
        "\t\treturn 0\n",
        "\t#<epsilon> with some other character or any character with <epsilon>\n",
        "\t#either insertion or deletion => Levenshtein cost 1\n",
        "\telif (s=='<epsilon' and t!='<epsilon>') or (s!='<epsilon' and t=='<epsilon>'):\n",
        "\t\treturn 1\n",
        "\t#two different chars, substiturion => Levenshtein cost 1\n",
        "\telse:\n",
        "\t\treturn 1\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"This function computes the Levenshtein distance between two \n",
        "strings s,t. It uses the iterative algorithm, that creates a \n",
        "2d matrix of the characters of the strings, along with the \n",
        "values it needs to compute the sum of the weights of the insert\n",
        "/delete/substitute logic. It is a much faster algorithm than the\n",
        "recursive one. The method takes 3 arguments, string s, string t\n",
        "and a tuple of 3 integeres that are the cost of deletion, insertion\n",
        "and substitution respectively. Defaults to (1,1,1) (equal cost).\n",
        "I have taken the code for this method from:\n",
        "https://www.python-course.eu/levenshtein_distance.php \n",
        "Another method could have been the method given in\n",
        "the python package python-Levenshtein (0.12.0) that uses python to \n",
        "instruct C to compute the afforementioned array, with much faster \n",
        "execution times.\"\"\"\n",
        "def iterative_levenshtein(s, t, costs=(1, 1, 1)):\n",
        "    \"\"\" \n",
        "        iterative_levenshtein(s, t) -> ldist\n",
        "        ldist is the Levenshtein distance between the strings \n",
        "        s and t.\n",
        "        For all i and j, dist[i,j] will contain the Levenshtein \n",
        "        distance between the first i characters of s and the \n",
        "        first j characters of t\n",
        "        \n",
        "        costs: a tuple or a list with three integers (d, i, s)\n",
        "               where d defines the costs for a deletion\n",
        "                     i defines the costs for an insertion and\n",
        "                     s defines the costs for a substitution\n",
        "    \"\"\"\n",
        "    rows = len(s)+1\n",
        "    cols = len(t)+1\n",
        "    deletes, inserts, substitutes = costs\n",
        "    \n",
        "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
        "    # source prefixes can be transformed into empty strings \n",
        "    # by deletions:\n",
        "    for row in range(1, rows):\n",
        "        dist[row][0] = row * deletes\n",
        "    # target prefixes can be created from an empty source string\n",
        "    # by inserting the characters\n",
        "    for col in range(1, cols):\n",
        "        dist[0][col] = col * inserts\n",
        "        \n",
        "    for col in range(1, cols):\n",
        "        for row in range(1, rows):\n",
        "            if s[row-1] == t[col-1]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = substitutes\n",
        "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
        "                                 dist[row][col-1] + inserts,\n",
        "                                 dist[row-1][col-1] + cost) # substitution\n",
        "    \"\"\"for r in range(rows):\n",
        "        print(dist[r])\n",
        "    \"\"\" \n",
        "    return dist[row][col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbSdpft_rBN",
        "colab_type": "text"
      },
      "source": [
        "Προφανως η ασκηση δεν ανεφερε αυτο, οποτε για την δημιουργια του μετατροπεα μιας καταστασης, αρχικα δημιουργουμε τα αρχεια που χρειαζομαστε, (τα αρχεια για τα συμβολα ειναι ηδη ετοιμα, καθως ειναι ομοια με τα αρχεια του στοιχισμενου αλφαβητου που περιεχεται στο αρχειο chars.syms. Για τον σκοπο αυτο, φτιαξαμε μια μεθοδο για να τυπωνει τα περιεχομενα του fst τοσο στο stdout, οσο και στο αρχειο με ονομα \"text.txt\" που θα χρησιμοποιησουμε αργοτερα. O ακριβης αριθμος των ακμων στο fst ειναι (πληθος αλφαβητου)^2 -1, καθως θελουμε καθε γραμμα να εχει καποια ενεργεια με οποιοδηποτε αλλο γραμμα του αλφαβητου, εκτος απο το ε, που με τον εαυτο του δεν θελουμε να εχει. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8Xzx_HAmS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_fst(Alphabet, filename = \"text.txt\", print_lines = False):\n",
        "    f = open(filename, \"w+\")\n",
        "    if (print_lines):\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                else:\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "        print(\"0\")\n",
        "        f.write(\"0\")\n",
        "    else:\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "        f.write(\"0\")\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXRETrXFCcPN",
        "colab_type": "text"
      },
      "source": [
        "Για να δημιουργησουμε τα αρχεια, θα πρεπει να εκτελεσουμε τον παρακατω κωδικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSk8leWVCiwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "#print(len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "#print(len(Alphabet))\n",
        "Alphabet = alphabet_indexing(Alphabet)\n",
        "#print(Alphabet)\n",
        "symbols_file(Alphabet,)\n",
        "text_fst(Alphabet, \"onestate.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqq_r9--C4WJ",
        "colab_type": "text"
      },
      "source": [
        "Στην συνεχεια, για να δημιουργησουμε το fst, θα πρεπει να εκτελεστει το παρακτω shell script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo0hg4X5DAyx",
        "colab_type": "code",
        "outputId": "865f5e7f-7adb-4b1f-bd2e-66732da74786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms onestate.txt > onestate.fst\n",
        "!fstinfo onestate.fst"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       1\n",
            "# of arcs                                         728\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               26\n",
            "# of output epsilons                              26\n",
            "input label multiplicity                          26.9643\n",
            "output label multiplicity                         26.9643\n",
            "# of accessible states                            1\n",
            "# of coaccessible states                          1\n",
            "# of connected states                             1\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     1\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            y\n",
            "cyclic at initial state                           y\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95snmURjDkxd",
        "colab_type": "text"
      },
      "source": [
        "Η πρωτη εντολη κανει compile τις πληροφοριες του αρχειο που δημιουργησαμε προηγουμενως για το one state fst, και παιρνει σαν input και output symbols τους χαρακτηρες του chars.syms (που ειναι και η αλφαβητα μας).\n",
        "Η δευτερη εντολη σχεδιαζει το fst που δημιουργηθηκε με τις βιβλιοθηκες dot και ghraphviz, και το αποθηκευει στο αρχειο fst.jpg. Η τριτη εντολη ειναι για να δουμε οτι ολα πηγαν καλα, δηλαδη οτι δημιουργηθηκε και εχει τα καταλληλα χαρακτηρηστικα. Επειδη ο μετατροπεας μιας καταστασης για την περιπτωση μας ειναι εξαιρετικα μεγαλος και η εικονα (jpg) δεν μας δειχνει καθαρα το τι συμβαινει, θα κανουμε μια μικροτερη αποτυπωση, απλα και μονο για λογους κατανοησης της λειτουργιας του κωδικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj4prUwFL8Ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Alphabet = [('<epsilon>', 0), ('a', '1'), ('b', '2'), ('c', '3'), ('d', '4')]\n",
        "symbols_file(Alphabet, \"chars1.syms\")\n",
        "text_fst(Alphabet, \"text1.txt\", False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXeLoMsiNG8t",
        "colab_type": "text"
      },
      "source": [
        "Τα αρχεια αυτα εχουν 25 ακμες, οποτε ειναι αρκετα πιο ευκολο να διακρινουμε τι γινεται απο την εικονα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mlvpPqwNP1L",
        "colab_type": "code",
        "outputId": "51152c75-7d3a-4959-fc48-8bfad367c827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "!fstcompile -isymbols=chars1.syms -osymbols=chars1.syms text1.txt > fst1.fst\n",
        "!fstdraw --isymbols=chars1.syms --osymbols=chars1.syms -portrait fst1.fst  | dot -Tjpg >fst1.jpg\n",
        "!fstinfo fst1.fst"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       1\n",
            "# of arcs                                         24\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               4\n",
            "# of output epsilons                              4\n",
            "input label multiplicity                          4.83333\n",
            "output label multiplicity                         4.83333\n",
            "# of accessible states                            1\n",
            "# of coaccessible states                          1\n",
            "# of connected states                             1\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     1\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            y\n",
            "cyclic at initial state                           y\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hzCCm1NpLu",
        "colab_type": "text"
      },
      "source": [
        "Ο μετατροπεας εισοδου αν παρουμε το shortest path, αυτο σημαινει οτι δεν επιφερει καμια αλλαγη, δηλαδη η λεξη που παιρνει στην εισοδο και η λεξη που παιρνουμε στην εξοδο ειναι ιδιες."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfSj8QUkSMzc",
        "colab_type": "text"
      },
      "source": [
        "   β) Ιδανικα, αντι να εχουμε κανονες για τα βαρη, θα επρεπε αν ειχαμε πολυ περισσοτερα δεδομενα να εφαρμοζουμε πιθανοτικες μεθοδους για να προσδιορισουμε τα βαρη καλυτερα στο μοντελο μας."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6i9AWk5Teg3",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 6ο\n",
        "   α) Για να υλοποιησουμε τον αποδοχεα λεξεων του λεξικου, ακολουθουμε την λογικη που ειχαμε και πριν για το αλφαβητο, μονο που αυτην την φορα ο αποδοχεας ειναι παρα (μα παρα) πολυ μεγαλος (τουλαχιστον για τον υπολογιστη μας). Παρολα αυτα, η μεθοδος sixth δημιουργει ενα αρχειο με το προσδιορισμενο ονομα, που περιεχει τις ακμες για καθε λεξη, με μηδενικο βαρος."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XArzNCcdelqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sixth(Lexicon, Alphabet, filename = \"6a.txt\", print_lines = False):\n",
        "    for i in range(0, len(Alphabet)):\n",
        "        temp = Alphabet[i]\n",
        "        Alphabet[i] = [temp, i]\n",
        "    f = open(filename, \"w+\")\n",
        "    i = 2\n",
        "    if (not print_lines):\n",
        "        for word in Lexicon:\n",
        "            for j in range(0, len(word)):\n",
        "                if j==0:\n",
        "                    f.write(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                    i += 1\n",
        "                elif (j== len(word) -1):\n",
        "                    f.write( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    i +=1\n",
        "                else:\n",
        "                    f.write( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    i+=1\n",
        "        f.write(\"1\")\n",
        "    else:\n",
        "        for word in Lexicon:\n",
        "            for j in range(0, len(word)):\n",
        "                if j==0:\n",
        "                    f.write(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    print(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                    i += 1\n",
        "                elif (j== len(word) -1):\n",
        "                    f.write( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    print( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                    i +=1\n",
        "                else:\n",
        "                    f.write( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    print( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    i+=1\n",
        "        f.write(\"1\")\n",
        "        print(\"1\")\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr6xfnX6BdiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "#print(len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "sixth(Lexicon, Alphabet, \"acceptor.txt\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em0PndE8Zdnp",
        "colab_type": "text"
      },
      "source": [
        " Αφου φτιαξουμε το αρχειο, οπως πριν καλουμε απο το cli τις ιδιες εντολες με τον μετατροπεα μιας καταστασης (με αλλαγμενα τα ονοματα προφανως. Συστηνουμε (τουλαχιστον εμεις ετσι μπορουσαμε να δουμε τα αποτελεσματα) να εκτελεστει το απο κατω κελι αντι για το απο πανω, για και στην συνεχεια να δημιουργηθει το fst, γιατι μειωνει σημαντικα ( απο 11249 λεξεις σε μολις 10 ) το λεξιλογιο, οποτε οχι μονο ειναι γρηγορη η δημιουργια του fst, αλλα και μπορει να διακριθει ο ιδιος ο αποδοχεας."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LktRu0bDGGn",
        "colab_type": "code",
        "outputId": "c336137e-aa91-46d2-db15-35c3e247cd6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"Αρχικο μεγεθος λεξικου: \",len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "Lexicon1 = Lexicon[10:20]\n",
        "print(\"Μειωμενο μεγεθος λεξικου: \",len(Lexicon1))\n",
        "sixth(Lexicon1, Alphabet)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Αρχικο μεγεθος λεξικου:  8082\n",
            "Μειωμενο μεγεθος λεξικου:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LwAJwLRCt5L",
        "colab_type": "code",
        "outputId": "75479664-f204-4e28-a94f-d17d566ba6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms acceptor.txt > acceptor.fst\n",
        "!fstinfo acceptor.fst"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       48224\n",
            "# of arcs                                         56282\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          71.8434\n",
            "output label multiplicity                         71.8434\n",
            "# of accessible states                            48224\n",
            "# of coaccessible states                          48202\n",
            "# of connected states                             48202\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     48224\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          n\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      n\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGcbyqVNa0_B",
        "colab_type": "text"
      },
      "source": [
        "   β) Για το μερος β, αρκει να τρεξουμε τις παρακατω εντολες (τις εχουμε βαλει ξεχωριστα για να παρατηρησουμε τι κανει η καθε μια)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP9n8AZKbEnh",
        "colab_type": "code",
        "outputId": "9bbff4b9-71ca-498c-ff5a-79a0cde39f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!fstdeterminize acceptor.fst  > acceptor_determinized.fst\n",
        "!fstminimize acceptor_determinized.fst > acceptor_minimized.fst\n",
        "!fstinfo acceptor_minimized.fst"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       6031\n",
            "# of arcs                                         11566\n",
            "initial state                                     0\n",
            "# of final states                                 795\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            6031\n",
            "# of coaccessible states                          6031\n",
            "# of connected states                             6031\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     6031\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          n\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blOuFPvafApW",
        "colab_type": "text"
      },
      "source": [
        "Η συναρτηση fstrmepsilon αφαιρει τις μεταβασεις που εχουν ε στο fst μας, και προφανως δεν ειχε καν νοημα να τις εφαρμοσουμε, αφου δεν ειχαμε χρησιμοποιησει καν ε στην κατασκευη του.\n",
        "Η συναρτηση fstdeterminize αφαιρει τον μη ντετερμινισμο, και οπως φαινεται απο τις εικονες των διαγραμματων που παραγονται, αν και δεν εχει ελαχιστο αριθμο καταστασεων, ειναι αφενος σαφως μικροτερο διαγραμμα απο το αρχικο και αφετερου σαφως πιο κατανοητο.\n",
        "Τελος, η συναρτηση fstminimize δημιουργει το διαγραμμα με τον ελαχιστο αριθμο καταστασεων, οπως φαινεται και απο την φωτογραφια fst_minimized.jpg που εχει 24 καταστασεις (ενω η determinized 27, ενω η αρχικη 55!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuzupFrrggeN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 7ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyfwnLBpNfz-",
        "colab_type": "text"
      },
      "source": [
        "   α) Φτιαχνουμε το Levenshtein Transducer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24sy_jxd_Nyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstcompose onestate.fst acceptor_minimized.fst > Leven.fst\n",
        "!fstinfo Leven.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XlilBJXZnTb",
        "colab_type": "text"
      },
      "source": [
        "Αναλογα με τα βαρη του καθε ειδους edit, γινεται αναλογο prioritization στα output words. Για παραδειγμα, για ισοβαρη edits, ο min distance transducer μπορει να επιλεξει και λεξεις μεγαλυτερου μηκους, που εχουν πολλα κοινα γραμματα με την αρχικη, με τελικο βαρος ισο με αλλες λεξεις μικροτερου μηκους, που ομως εχουν περισσοτερα substitutions γραμματων. Απο την αλλη, αν ειχαμε μεγαλυτερο βαρος στο removal η στο insertion, τοτε θα ειχαμε περισσοτερα αποτελεσματα ιδιου μηκους με την λεξη εισοδου, αλλα με περισσοτερα substitutions. Συμπερασματικα, καταληγουμε οτι τα βαρη επηρεαζουν την μορφη των λεξεων εξοδου. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNp5vgd4NwuJ",
        "colab_type": "text"
      },
      "source": [
        " β) Φτιαχνουμε τα απαραιτητα αρχεια για την δημιουργια ενος acceptor για την λεξη \"cit\":\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9OgWWx8OXmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lexicon_cit = [\"cit\"]\n",
        "sixth(Lexicon_cit, Alphabet, \"cit.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlBWXN-QOy-C",
        "colab_type": "text"
      },
      "source": [
        "Φτιαχνουμε τον αποδοχεα για την λεξη \"cit\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlVgsc7oHyw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! fstcompile -isymbols=chars.syms -osymbols=chars.syms cit.txt > cit.fst\n",
        "! fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait cit.fst  | dot -Tjpg >cit.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncB7sxhrOOnE",
        "colab_type": "text"
      },
      "source": [
        "Κανουμε compose το FST για την αποσταση Levenshtein με τον αποδοχεα της λεξης \"cit\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfhSAwZfOPFH",
        "colab_type": "code",
        "outputId": "5773f501-b8bd-40b8-f1aa-69ffd6b19e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "! fstcompose cit.fst Leven.fst  > min_distance.fst\n",
        "! fstinfo min_distance.fst"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       24124\n",
            "# of arcs                                         99055\n",
            "initial state                                     0\n",
            "# of final states                                 795\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               46264\n",
            "# of output epsilons                              18093\n",
            "input label multiplicity                          4.02569\n",
            "output label multiplicity                         1.70058\n",
            "# of accessible states                            24124\n",
            "# of coaccessible states                          24124\n",
            "# of connected states                             24124\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     24124\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To5bzmAKPDU7",
        "colab_type": "text"
      },
      "source": [
        "Καλουμε την fstshortestpath για να βρουμε την ελαχιστη edit αποσταση (αποσταση Levenshtein)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT6XXmZJSNnK",
        "colab_type": "code",
        "outputId": "e4d77695-4fc2-45ee-8e68-686581aee7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!fstshortestpath min_distance.fst | fstrmepsilon > out.fst\n",
        "!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait out.fst  | dot -Tjpg >out.jpg\n",
        "!fstinfo out.fst"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       4\n",
            "# of arcs                                         3\n",
            "initial state                                     3\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            4\n",
            "# of coaccessible states                          4\n",
            "# of connected states                             4\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     4\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdaYjRTDZWu8",
        "colab_type": "text"
      },
      "source": [
        "Αν τρεξουμε τον παραπανω κωδικα, βλεπουμε οτι η προβλεψη του min edit spell checker ειναι η λεξη \"wit\" που ουσιαστικα εχει μονο ενα substitution ενος γραμματος. Αν βαλουμε το argument --nshorterst = Ν, οπου Ν ο αριθμος των προβλεψεων που θελουμε να δουμε, βλεπουμε οτι υπαρχουν πολλες επιλογες με το ιδιο αριθμο edits, οπως η λεξη \"sit\" και αλλες. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_kxWWizgiHR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 8ο\n",
        "\n",
        "  α) Για το βημα αυτο, εχουμε αποηθεκυσει τα δεδομενα για testing στο αρχειο evaluation_data.txt .\n",
        "  β) Για το ερωτημα αυτο, πρωτα δημιουργουμε ενα λεξικο που συνδεει τις σωστες λεξεις με τις λαθος, και απο αυτο \"τραβαμε\" τυχαια λαθος λεξεις και χρησιμοποιουμε τον min edit checker για να μας δωσει πιθανες απαντησεις λεξεων. Στην συνεχεια συγκρινουμε τις απαντησεις του με τις προκαθορισμενες απαντησεις, τυπωνοντας για καθε περιπτωση."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoP9BPA-GLqD",
        "colab_type": "code",
        "outputId": "f3184508-1aa9-4f60-9ad0-14d075d57808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "download(\"https://raw.githubusercontent.com/georgepar/python-lab/master/spell_checker_test_set\",\"evaluation_data.txt\")\n",
        "print(\"All the files are downloaded\")#If the downloaded file is a zip file than you can use below function to unzip it."
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_5EU0ha3H0D",
        "colab_type": "code",
        "outputId": "220dba90-7587-4bf7-81b7-51c8b4d5e2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "import random\n",
        "lines = read_from_file(\"evaluation_data.txt\")\n",
        "text = []\n",
        "dictionary = {}\n",
        "for line in lines:\n",
        "    text = []\n",
        "    text.append(tokenize(line))\n",
        "    for i in range (1,len(text[0])):\n",
        "      dictionary.update({text[0][i]:text[0][0]})\n",
        "for counter in range(0,20):\n",
        "  key = random.choice(list(dictionary.keys()))\n",
        "  value = dictionary[key]\n",
        "  Lexicon_test = [key]\n",
        "  f = open('new.txt', 'w+')\n",
        "  f.truncate(0)\n",
        "  f.close()\n",
        "  fileopen = open(\"new.txt\",\"w+\")\n",
        "  for i in range(0,len(key)):\n",
        "    fileopen.write(\"%s \" %str(i))\n",
        "    fileopen.write(\" %s \" %str(i+1))\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\"0\\n\")\n",
        "  fileopen.write(str(len(key)))\n",
        "  fileopen.close()\n",
        "  sixth(Lexicon_test, Alphabet, \"new.txt\")\n",
        "  !fstcompile -isymbols=chars.syms -osymbols=chars.syms new.txt > test.fst\n",
        "  !fstcompose test.fst Leven.fst  > min_distance.fst\n",
        "  print(\"Wrong word = \" ,key,\", actual word = \",value)\n",
        "  print(\"What we got from the transducer =\", end =\" \")  \n",
        "  !fstshortestpath min_distance.fst | fstrmepsilon | fsttopsort | fstprint -osymbols=chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n'\n",
        "  print()"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong word =  realy , actual word =  really\n",
            "What we got from the transducer = real\n",
            "Wrong word =  poertry , actual word =  poetry\n",
            "What we got from the transducer = poetry\n",
            "Wrong word =  offen , actual word =  often\n",
            "What we got from the transducer = offer\n",
            "Wrong word =  contended , actual word =  contented\n",
            "What we got from the transducer = continued\n",
            "Wrong word =  refressmunt , actual word =  refreshment\n",
            "What we got from the transducer = represent\n",
            "Wrong word =  managment , actual word =  management\n",
            "What we got from the transducer = management\n",
            "Wrong word =  experance , actual word =  experience\n",
            "What we got from the transducer = experience\n",
            "Wrong word =  benifits , actual word =  benefits\n",
            "What we got from the transducer = senility\n",
            "Wrong word =  wote , actual word =  wrote\n",
            "What we got from the transducer = note\n",
            "Wrong word =  stumache , actual word =  stomach\n",
            "What we got from the transducer = strange\n",
            "Wrong word =  intial , actual word =  initial\n",
            "What we got from the transducer = india\n",
            "Wrong word =  parralell , actual word =  parallel\n",
            "What we got from the transducer = parallel\n",
            "Wrong word =  magnificnet , actual word =  magnificent\n",
            "What we got from the transducer = magnificent\n",
            "Wrong word =  latist , actual word =  latest\n",
            "What we got from the transducer = waist\n",
            "Wrong word =  poetre , actual word =  poetry\n",
            "What we got from the transducer = poetry\n",
            "Wrong word =  specail , actual word =  special\n",
            "What we got from the transducer = special\n",
            "Wrong word =  accomodation , actual word =  accommodation\n",
            "What we got from the transducer = accumulation\n",
            "Wrong word =  scarecly , actual word =  scarcely\n",
            "What we got from the transducer = stately\n",
            "Wrong word =  neccesary , actual word =  necessary\n",
            "What we got from the transducer = necessary\n",
            "Wrong word =  curtans , actual word =  curtains\n",
            "What we got from the transducer = curtain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm1GLoTXgjci",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 9ο\n",
        "\n",
        "  α) Διαβαζουμε το κειμενο σε προτασεις:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478LJxqs81dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AouLMySO89CU",
        "colab_type": "text"
      },
      "source": [
        "  β) Κανουμε import την βιβλιοθηκη του Word2Vec, για να εκπαιδεύσουμε 100διαστατα word2vec embeddings με βάση τις προτάσεις του βηματος 9α."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSyHHegH6TsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "path = get_tmpfile(\"word2vec.model\")\n",
        "model = Word2Vec(text, size=100, window=5)\n",
        "model.save(\"word2vec.model\")\n",
        "model.train(text, total_examples=len(text), epochs=1000)\n",
        "voc = model.wv.index2word\n",
        "dim = model.wv.vector_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7vBjalPBIob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def to_embeddings_Matrix(model):  \n",
        "    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n",
        "    word2idx = {}\n",
        "    for i in range(len(model.wv.vocab)):\n",
        "        embedding_matrix[i] = model.wv[model.wv.index2word[i]]\n",
        "        word2idx[model.wv.index2word[i]] = i\n",
        "    return embedding_matrix, model.wv.index2word, word2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgei-di2-Gb4",
        "colab_type": "text"
      },
      "source": [
        "  γ) Βρισκουμε 10 τυχαιες λεξεις απο το λεξικο και παραγουμε τις σημασειολογικα κοντινοτερες τους:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXYCS4GG-Qzv",
        "colab_type": "code",
        "outputId": "0635580d-47bb-4e61-e85f-b99dffa39a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "text1 = flatten(text)\n",
        "Lexicon = unique_tokens(text1)\n",
        "i = 0\n",
        "tries = 0\n",
        "while i<10:\n",
        "    try:\n",
        "        word = random.choice(Lexicon)\n",
        "        result, similarity = model.wv.most_similar(word,topn=1)[0][0],  model.wv.most_similar(word,topn=1)[0][1]\n",
        "        i += 1\n",
        "        print(\"Initial word: \", word, \", Similar-context word (According to Word2Vec embedding):\", result, \"\\n Accuracy Achieved: \", similarity)\n",
        "    except:\n",
        "        pass\n",
        "    finally:\n",
        "        tries +=1\n",
        "    "
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial word:  writing , Similar-context word (According to Word2Vec embedding): lawyer \n",
            " Accuracy Achieved:  0.34325671195983887\n",
            "Initial word:  reward , Similar-context word (According to Word2Vec embedding): understand \n",
            " Accuracy Achieved:  0.3462287187576294\n",
            "Initial word:  acquaintance , Similar-context word (According to Word2Vec embedding): wedding \n",
            " Accuracy Achieved:  0.36892834305763245\n",
            "Initial word:  bent , Similar-context word (According to Word2Vec embedding): wrist \n",
            " Accuracy Achieved:  0.3685738444328308\n",
            "Initial word:  works , Similar-context word (According to Word2Vec embedding): scene \n",
            " Accuracy Achieved:  0.38312798738479614\n",
            "Initial word:  flight , Similar-context word (According to Word2Vec embedding): mission \n",
            " Accuracy Achieved:  0.38402414321899414\n",
            "Initial word:  died , Similar-context word (According to Word2Vec embedding): returned \n",
            " Accuracy Achieved:  0.41020041704177856\n",
            "Initial word:  does , Similar-context word (According to Word2Vec embedding): is \n",
            " Accuracy Achieved:  0.32770299911499023\n",
            "Initial word:  headed , Similar-context word (According to Word2Vec embedding): listen \n",
            " Accuracy Achieved:  0.2951596975326538\n",
            "Initial word:  drawer , Similar-context word (According to Word2Vec embedding): wood \n",
            " Accuracy Achieved:  0.35370272397994995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTVMz5ju2wKx",
        "colab_type": "text"
      },
      "source": [
        "# Βήμα 10ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQku2-LA-Ky2",
        "colab_type": "text"
      },
      "source": [
        "  α) Εξάγουμε την πιθανότητα εμφάνισης κάθε token (λέξης) και την αποθηκεύουμε σε ένα λεξικό με key το token και value την πιθανότητα εμφάνισής του."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2xIxWxj2K0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "Word_probability = {}\n",
        "Totalwords = 0\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "for i in range(0,len(text)):\n",
        "  for word in text[i]:\n",
        "      if not word:\n",
        "        continue\n",
        "      Totalwords+= 1\n",
        "      if Word_probability.get(word) == None:\n",
        "        Word_probability.update({word:1})\n",
        "      else:\n",
        "        Word_probability[word] += 1\n",
        "for key,value in Word_probability.items():\n",
        "  Word_probability[key] = Word_probability[key]/Totalwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7LICeKa-Vss",
        "colab_type": "text"
      },
      "source": [
        " β) Εξάγουμε την πιθανότητα εμφάνισης κάθε χαρακτήρα και την αποθηκεύουμε σε ένα λεξικό με key το token και value την πιθανότητα εμφάνισής του."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHHk-BkJ2Pgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "Letter_probability = {}\n",
        "Totalletters = 0\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "for i in range(0,len(text)):\n",
        "  for word in text[i]:\n",
        "    for letter in range(0,len(word)):\n",
        "      Totalletters+= 1\n",
        "      if Letter_probability.get(word[letter]) == None:\n",
        "        Letter_probability.update({word[letter]:1})\n",
        "      else:\n",
        "        Letter_probability.update({word[letter]:Letter_probability[word[letter]]+1})\n",
        "for key,value in Letter_probability.items():\n",
        "  Letter_probability[key] = Letter_probability[key]/Totalletters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REbZDoF90H7",
        "colab_type": "text"
      },
      "source": [
        "#Βήμα 11ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiWlXiiWCz23",
        "colab_type": "text"
      },
      "source": [
        "  α)Υπολογιζουμε την μεση τιμη των βαρων του word level, αλλα και του unigram level:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUfh99mO97dv",
        "colab_type": "code",
        "outputId": "37c26b23-9202-4f06-9633-af5e9f4c3a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "Wordw = 0\n",
        "Distwords = 0\n",
        "Letterw = 0\n",
        "Distletters = 0\n",
        "for (key,value) in Word_probability.items():\n",
        "  Wordw -= np.log(value)\n",
        "  Distwords += 1\n",
        "for (key,value) in Letter_probability.items():\n",
        "  Letterw -= np.log(value)\n",
        "  Distletters += 1\n",
        "Wordw = Wordw / Distwords\n",
        "print(Distwords)\n",
        "print(Wordw)\n",
        "Letterw = Letterw / Distletters\n",
        "print(Distletters)\n",
        "print(Letterw)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8082\n",
            "10.620257420623155\n",
            "26\n",
            "3.9143484580218573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ipu0tD6D7CZ",
        "colab_type": "text"
      },
      "source": [
        "  β) Αρχικα φτιαχνουμε μια συναρτηση για να τυπωνουμε σε αρχειο τις απαραιτητες πληροφοριες για να φτιαχτει το fst:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040RRQj7AIhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_fst(Alphabet,weight, filename = \"text.txt\", print_lines = False):\n",
        "    f = open(filename, \"w+\")\n",
        "    if (print_lines):\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight))\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight)+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight))\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight)+ \"\\n\")\n",
        "                else:\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight))\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight)+ \"\\n\")\n",
        "        print(\"0\")\n",
        "        f.write(\"0\")\n",
        "    else:\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight)+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight)+ \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(weight)+ \"\\n\")\n",
        "        f.write(\"0\")\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukg8W6PEEKFY",
        "colab_type": "text"
      },
      "source": [
        "Φτιαχνουμε τα δυο αρχεια που θα μας χρειαστουν για τους μετατροπεις:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhWng1JgCXog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "#print(len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "#print(len(Alphabet))\n",
        "Alphabet = alphabet_indexing(Alphabet)\n",
        "#print(Alphabet)\n",
        "symbols_file(Alphabet,)\n",
        "weighted_fst(Alphabet,Wordw, \"Wordw.txt\")\n",
        "weighted_fst(Alphabet,Letterw, \"Letterw.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nik0YYidEk7w",
        "colab_type": "text"
      },
      "source": [
        "Και τωρα κανουμε compile τους μετατροπεις:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRi9Kw45D1Qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms Wordw.txt > Wordw.fst\n",
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms Letterw.txt > Letterw.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0hj4pILo2z1",
        "colab_type": "text"
      },
      "source": [
        "Αν είχαμε στην διάθεση μας οτιδήποτε δεδομένα θέλουμε θα χρησιμοποιήσουμα ως βάρη την συχνότητα που συμβαίνουν τα λάθη. Πιο συγκεκριμένα θα θέλαμε να \"μετρήσουμε\" πόσο συχνά γίνεται λάθος μεταξύ συγκεκριμένων γραμμάτων, πόσο συχνά δεν γράφεται ένα συγκεκριμένο γράμμα και πόσο συχνά χρησιμοποιείται κάποιο περιττό γράμμα. Επίσης χρήσιμο θα ήταν να εισάγωγουμε παράμετρο στο βάρος ανταλλαγής γραμμάτων με βάση την φυσική απόσταση των γραμμάτων στο πληκτρολόγιο (π.χ. το 'α' είναι πιθανότερο να έχει γραφεί ως 'ζ' η 'σ') ή και την ομοηχία (π.χ. το 'ι' με το 'η' και το 'υ'). Ιδανικά θα θέλαμε να χρησιμοποιήσουμε και έναν πιο εξειδικευμένο μηχανισμό για τον υπολογισμό των edit distances, όπως τον Damerau–Levenshtein distance, που επιτρέπει και αντιμεταθέσεις εκτός από insertions, deletions, substitutions. Αυτή είναι μια χρήσιμη απόσταση, καθώς κάποιος μπορεί να πληκτρολογήσει με λάθος σειρά τα σωστά γράμματα (π.χ. Επεξεργασία και Επεξεγρασία). Ακόμα, ιδανικά θα βρίσκαμε μια πιο εξειδικευμένη συνάρτηση για την έυρεση των βαρών, όπως κάνει ο αλγόριθμος  Wagner–Fischer, που μελετάει τα προθέματα και επιθέματα όλων των λέξεων του corpus, πριν φτιάξει τα βάρη, και κρατάει επιπλέον πληροφορίες για τα επιθέματα και προθέματα και μπορεί πιο εύκολα να συνδέσει λέξεις μεταξύ τους, άρα και να βρει καλύτερα βάρη."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnbDRI1UV1P1",
        "colab_type": "text"
      },
      "source": [
        "# Βήμα 12ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8WJEc0CPPsI",
        "colab_type": "text"
      },
      "source": [
        "   α) Φτιαχνουμε 2 συναρτησεις για να δημιουργουμε τα αρχεια που ειναι απαραιτητα για τα  fst που μας ζητουνται:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLEo8bt6V5Qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def twelvtha(Lexicon, Alphabet, Word_probability, filename = \"12a.txt\"):\n",
        "    f = open(filename, \"w+\")\n",
        "    i = 2\n",
        "    for word in Lexicon:\n",
        "        for j in range(0, len(word)):\n",
        "            if j==0:\n",
        "                f.write(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" \" + str(-1*np.log(Word_probability[word])) + \"\\n\")\n",
        "                i += 1\n",
        "            elif (j== len(word) -1):\n",
        "                f.write( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                i +=1\n",
        "            else:\n",
        "                f.write( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                i+=1\n",
        "    f.write(\"1\")\n",
        "    f.close()\n",
        "\n",
        "def twelvthb(Lexicon, Alphabet, Letter_probability, filename = \"12b.txt\"):\n",
        "    f = open(filename, \"w+\")\n",
        "    i = 2\n",
        "    for word in Lexicon:\n",
        "        for j in range(0, len(word)):\n",
        "            if j==0:\n",
        "                f.write(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" \" + str(-1*np.log(Letter_probability[word[j]])/len(word)) + \"\\n\")\n",
        "                i += 1\n",
        "            elif (j== len(word) -1):\n",
        "                f.write( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" \" + str(-1*np.log(Letter_probability[word[j]])/len(word)) + \"\\n\")\n",
        "                i +=1\n",
        "            else:\n",
        "                f.write( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" \" + str(-1*np.log(Letter_probability[word[j]])/len(word)) + \"\\n\")\n",
        "                i+=1\n",
        "    f.write(\"1\")\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDUQyVgzPa0o",
        "colab_type": "text"
      },
      "source": [
        "*Στην συνάρτηση twelvthb το βάρος τοποθετείται σταθμισμένο με βάση το μέγεθος της εκάστοτε λέξης καθώς δίχως αυτή την κανονικοποίηση ο ορθογράφος θα προτιμούσε πάντα πολύ μικρές λέξεις*\n",
        "\n",
        "\n",
        "Δημιουργουμε τα αρχεια που χρειαζομαστε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H51_jYZTXhkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twelvtha(Lexicon, Alphabet,Word_probability,\"Wordlevel.txt\")\n",
        "twelvthb(Lexicon, Alphabet,Letter_probability,\"Unigram.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcmgviLNPg_W",
        "colab_type": "text"
      },
      "source": [
        "  β) Καλουμε τις αναλογες συναρτησεις για να βελτιστοποιησουμε τα μοντελα μας:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3JMoUZJhpT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7612161d-bfa2-400f-e84d-a9835b72214b"
      },
      "source": [
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms Wordlevel.txt | fstdeterminize | fstminimize | fstarcsort > Wordlevel.fst\n",
        "!fstinfo Wordlevel.fst\n",
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms Unigram.txt | fstdeterminize | fstminimize | fstarcsort > Unigram.fst\n",
        "!fstinfo Unigram.fst"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       7518\n",
            "# of arcs                                         13551\n",
            "initial state                                     0\n",
            "# of final states                                 1401\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            7518\n",
            "# of coaccessible states                          7518\n",
            "# of connected states                             7518\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     7518\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n",
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       7884\n",
            "# of arcs                                         13955\n",
            "initial state                                     0\n",
            "# of final states                                 1613\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            7884\n",
            "# of coaccessible states                          7884\n",
            "# of connected states                             7884\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     7884\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ouXqDv4ipnt",
        "colab_type": "text"
      },
      "source": [
        "#Βήμα 13ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXLNmcyP_3K",
        "colab_type": "text"
      },
      "source": [
        "α)β)Για να κατασκευασουμε τους ορθογραφους για τα word-level & unigram-level γλωσσικα μοντελα καλουμε την fstcompose με τον word-level μετατροπεα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew4HNQrZit4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! fstcompose Wordw.fst Wordlevel.fst> Wordlevellast.fst\n",
        "! fstcompose Letterw.fst Unigram.fst > Unigramlast.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cjhSYEGQYry",
        "colab_type": "text"
      },
      "source": [
        "γ) Αρχικα καλουμε την fstcompose με το μοντελο sample.fst, και για τους δυο ορθογραφους:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaiBE12LTQ4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lexicon_sample = [\"duckery\"]\n",
        "sixth(Lexicon_sample, Alphabet, \"sample.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1QUavDwTb8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! fstcompile -isymbols=chars.syms -osymbols=chars.syms sample.txt > sample.fst\n",
        "! fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait sample.fst  | dot -Tjpg >sample.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKJtpdInQk4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! fstcompose sample.fst Wordlevellast.fst > min_distance_Wordlevel.fst  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GeMwL01KdT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! fstcompose  sample.fst Unigramlast.fst > min_distance_Unigram.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55G1BO8RRJA4",
        "colab_type": "text"
      },
      "source": [
        "Στην συνεχεια βρισκουμε την λεξη με την μικροτερη αποσταση απο την λεξη \"cit\" και για τους δυο ορθογραφους:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AThXNyhwQ6Sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstshortestpath min_distance_Wordlevel.fst | fstrmepsilon > out_word.fst\n",
        "!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait out_word.fst  | dot -Tjpg >out_word.jpg\n",
        "!fstshortestpath min_distance_Unigram.fst | fstrmepsilon > out_unigram.fst\n",
        "!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait out_unigram.fst  | dot -Tjpg >out_unigram.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axGyF7jfUmJB",
        "colab_type": "text"
      },
      "source": [
        "Αυτοι οι ορθογραφοι προσπαθουν να βρουν την λεξη που με ελαχιστο edit cost ερχεται πλησιεστερα στην λεξη εισοδου σε επιπεδο λεξεων και χαρακτηρων. Με αλλα λογια, ο ενας προσπαθει να ταιριαξει ολοκληρη την λεξη με βαση τις υπολοιπες λεξεις του κειμενου, ενω ο αλλος προσπαθει να ταιριαξει την λεξη ταιριαζοντας χαρακτηρες, ετσι ωστε να δημιουργησει μια λεξη που υπαρχει στο κειμενο. Ενα παραδειγμα αμφισημης διορθωσης ειναι η λεξη-εισοδος \"duckery\", που με το μοντελο λεξεων διορθωνεται σε \"duke\", ενω με το μοντελο unigram διορθωνεται σε \"archery\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mBc-yzgWMQL",
        "colab_type": "text"
      },
      "source": [
        "#Βήμα 14ο\n",
        "\n",
        "  α) Το συνολο δεδομενων επαληθευσης ειναι στο αρχειο evaluation_data.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNtXrh1_hbDj",
        "colab_type": "text"
      },
      "source": [
        "β) Ο κωδικας για να ελεγχουμε για καθε λεξη του συνολου δεδομενων επαληθευσης ειναι ο εξης:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT9b_JdViDVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"evaluation_data.txt\")\n",
        "text = []\n",
        "dictionary = {}\n",
        "for line in lines:\n",
        "    text = []\n",
        "    text.append(tokenize(line))\n",
        "    for i in range (1,len(text[0])):\n",
        "      dictionary.update({text[0][i]:text[0][0]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN1jxiE-gbkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_contents = list(dictionary)\n",
        "for counter in range(0,20):\n",
        "  key = random.choice(dict_contents)\n",
        "  value = dictionary[key]\n",
        "  Lexicon_test = [key]\n",
        "  f = open('new.txt', 'w+')\n",
        "  f.truncate(0)\n",
        "  f.close()\n",
        "  fileopen = open(\"new.txt\",\"w+\")\n",
        "  for i in range(0,len(key)):\n",
        "    fileopen.write(\"%s \" %str(i))\n",
        "    fileopen.write(\" %s \" %str(i+1))\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\"0\\n\")\n",
        "  fileopen.write(str(len(key)))\n",
        "  fileopen.close()\n",
        "  sixth(Lexicon_test, Alphabet, \"new.txt\")\n",
        "  !fstcompile -isymbols=chars.syms -osymbols=chars.syms new.txt > test.fst\n",
        "  !fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait test.fst  | dot -Tjpg >test.jpg \n",
        "  !fstcompose test.fst Wordlevellast.fst > min_distance.fst\n",
        "  print(\"Wrong word = \" ,key,\", actual word = \",value)\n",
        "  print(\"What we got from the transducer =\", end =\" \")  \n",
        "  !fstshortestpath min_distance.fst | fstrmepsilon | fsttopsort | fstprint -osymbols=chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n'\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw-4CZA4iAgL",
        "colab_type": "code",
        "outputId": "2e8a70df-aa2c-4e35-8c40-67fc98db4060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "for counter in range(0,20):\n",
        "  key = random.choice(dict_contents)\n",
        "  value = dictionary[key]\n",
        "  Lexicon_test = [key]\n",
        "  f = open('new.txt', 'w+')\n",
        "  f.truncate(0)\n",
        "  f.close()\n",
        "  fileopen = open(\"new.txt\",\"w+\")\n",
        "  for i in range(0,len(key)):\n",
        "    fileopen.write(\"%s \" %str(i))\n",
        "    fileopen.write(\" %s \" %str(i+1))\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\"0\\n\")\n",
        "  fileopen.write(str(len(key)))\n",
        "  fileopen.close()\n",
        "  sixth(Lexicon_test, Alphabet, \"new.txt\")\n",
        "  !fstcompile -isymbols=chars.syms -osymbols=chars.syms new.txt > test.fst\n",
        "  !fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait test.fst  | dot -Tjpg >test.jpg \n",
        "  !fstcompose test.fst Wordlevellast.fst > min_distance.fst\n",
        "  print(\"Wrong word = \" ,key,\", actual word = \",value)\n",
        "  print(\"What we got from the transducer =\", end =\" \")  \n",
        "  !fstshortestpath min_distance.fst | fstrmepsilon | fsttopsort | fstprint -osymbols=chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n'\n",
        "  print()"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong word =  litriture , actual word =  literature\n",
            "What we got from the transducer = literature\n",
            "Wrong word =  inconvienient , actual word =  inconvenient\n",
            "What we got from the transducer = inconvenience\n",
            "Wrong word =  stumache , actual word =  stomach\n",
            "What we got from the transducer = such\n",
            "Wrong word =  arrainged , actual word =  arranged\n",
            "What we got from the transducer = arranged\n",
            "Wrong word =  viseted , actual word =  visited\n",
            "What we got from the transducer = visited\n",
            "Wrong word =  biscits , actual word =  biscuits\n",
            "What we got from the transducer = visits\n",
            "Wrong word =  transportibility , actual word =  transportability\n",
            "What we got from the transducer = responsibility\n",
            "Wrong word =  febuary , actual word =  february\n",
            "What we got from the transducer = february\n",
            "Wrong word =  beetween , actual word =  between\n",
            "What we got from the transducer = between\n",
            "Wrong word =  undersand , actual word =  understand\n",
            "What we got from the transducer = understand\n",
            "Wrong word =  oppesite , actual word =  opposite\n",
            "What we got from the transducer = opposite\n",
            "Wrong word =  recieve , actual word =  receive\n",
            "What we got from the transducer = relieve\n",
            "Wrong word =  biult , actual word =  built\n",
            "What we got from the transducer = but\n",
            "Wrong word =  anut , actual word =  aunt\n",
            "What we got from the transducer = nut\n",
            "Wrong word =  neccasary , actual word =  necessary\n",
            "What we got from the transducer = necessary\n",
            "Wrong word =  uneque , actual word =  unique\n",
            "What we got from the transducer = unique\n",
            "Wrong word =  extented , actual word =  extended\n",
            "What we got from the transducer = extended\n",
            "Wrong word =  magnifcent , actual word =  magnificent\n",
            "What we got from the transducer = magnificent\n",
            "Wrong word =  recieve , actual word =  receive\n",
            "What we got from the transducer = relieve\n",
            "Wrong word =  vistors , actual word =  visitors\n",
            "What we got from the transducer = visitors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iak9QHOKteoB",
        "colab_type": "text"
      },
      "source": [
        "Και οι δύο ορθογράφοι δουλεύουν ικανοποιητικά. Η unigram αντιμετώπιση τείνει να δυσκολεύεται λίγο σε μεγάλες λέξεις και γι αυτό εισάγαμε μια κανονικοποίηση στα βάρη.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKRpWPKKlP7b",
        "colab_type": "text"
      },
      "source": [
        "#Βήμα 16ο\n",
        "\n",
        "  α) Kατεβαζουμε τα δεδομενα αρχικα:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvWHqxiemAwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "download(\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\"aclImdb_v1.tar.gz\")\n",
        "print(\"All the files are downloaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjhdiNjkn2Mr",
        "colab_type": "text"
      },
      "source": [
        "Στην συνεχεια τα κανουμε uncompress:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdeYz72BnhgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf aclImdb_v1.tar.gz "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfIrjLr1oKFs",
        "colab_type": "text"
      },
      "source": [
        "  β) Επειτα τα επεξεργαζομαστε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM2Z4S45oLq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = './aclImdb/'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "pos_train_dir = os.path.join(train_dir, 'pos')\n",
        "neg_train_dir = os.path.join(train_dir, 'neg')\n",
        "pos_test_dir = os.path.join(test_dir, 'pos')\n",
        "neg_test_dir = os.path.join(test_dir, 'neg')\n",
        "\n",
        "# For memory limitations. These parameters fit in 8GB of RAM. (5000)\n",
        "# If you have 16G of RAM you can experiment with the full dataset / W2V (10000)\n",
        "MAX_NUM_SAMPLES = 100000\n",
        "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
        "# sorted from most frequent to least frequent.\n",
        "# It may yield much worse results for other embeddings corpora\n",
        "NUM_W2V_TO_LOAD = 2000000\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Fix numpy random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "try:\n",
        "    import glob2 as glob\n",
        "except ImportError:\n",
        "    import glob\n",
        "\n",
        "def strip_punctuation(s):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', ' ', s)\n",
        "\n",
        "def preprocess(s):\n",
        "    return re.sub('\\s+',' ', strip_punctuation(s).lower())\n",
        "\n",
        "def tokenize(s):\n",
        "    return s.split(' ')\n",
        "\n",
        "def preproc_tok(s):\n",
        "    return tokenize(preprocess(s))\n",
        "\n",
        "def read_samples(folder, preprocess=lambda x: x):\n",
        "    samples = glob.iglob(os.path.join(folder, '*.txt'))\n",
        "    data = []\n",
        "    for i, sample in enumerate(samples):\n",
        "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
        "            break\n",
        "        with open(sample, 'r') as fd:\n",
        "            x = [preprocess(l) for l in fd][0]\n",
        "            data.append(x)\n",
        "    return data\n",
        "\n",
        "def create_corpus(pos, neg):\n",
        "    corpus = np.array(pos + neg)\n",
        "    y = np.array([1 for _ in pos] + [0 for _ in neg])\n",
        "    indices = np.arange(y.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    return list(corpus[indices]), list(y[indices])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJw-BZXOrDr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#4 lists of 12500 length, containing strings\n",
        "#Each string is a positive or negative comment, of varying length\n",
        "positive_train_data = read_samples(pos_train_dir)\n",
        "negative_train_data = read_samples(neg_train_dir)\n",
        "positive_test_data = read_samples(pos_test_dir)\n",
        "negative_test_data = read_samples(neg_test_dir)\n",
        "whole_train_text = positive_train_data + negative_train_data\n",
        "whole_test_text = positive_test_data + negative_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGEGAW4yuClW",
        "colab_type": "text"
      },
      "source": [
        "#Βημα 17ο\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2ufbVzxPSUS",
        "colab_type": "text"
      },
      "source": [
        "α) Ο ρολος των βαρων TF-IDF στις BOW αναπαραστασεις ειναι για να κανει ουσιαστικα normalize τα βαρη των ορων που εμφανιζονται πολλες φορες. Με αλλα λογια, η αναλυση του Term Frequency εξεταζει ποσες φορες εχει εμφανιστει ενας ορος σε ενα sample, και η αναλυση Inverse Document Frequency εξεταζει (αναστροφα) το ποσες φορες εχει εμφανιστει ο ορος αυτος σε ολα τα sample που εχουμε. Ο ρολος τους δηλαδη ειναι να αποτιμουν βαρη για ορους που θελουμε να μαθουμε ποσο σχετικοι ειναι με ενα θεμα, αλλα αν εμφανιζονται παρα πολλες φορες σε ενα sample τοτε το βαρος τους δεν πρεπει να ειναι εξαιρετικα μεγαλο. Η αναλυση TF εξεταζει το ποσο σημαντικος ειναι ενας ορος στο συγκεκριμενο sample, ενω η αναλυση IDF εξεταζει ποσο \"κοινος\" ειναι σε ολα τα sample, αρα και ποσο μη σημαντικος (κοινοτυπος) ειναι ο ορος αυτος.\n",
        "\n",
        "Ενα παραδειγμα ειναι το εξης:\n",
        "\n",
        "Αν θελουμε να ψαξουμε για την σχετικοτητα του ορου \"pool\" σε αρθρα που αναφερονται σε κολυμβηση (\"swimming\"), τοτε η αναλυση TF μπορει να μας πει οτι ο ορος \"pool\" εμφανιζεται 5 φορες στο αρθρο x, και απο την αναλυση IDF, 20 φορες σε ολα τα αρθρα. Απο την αλλη, ο ορος \"the\" (που ειναι εξαιρετικα κοινος, καθως ειναι stop-word) μπορει να εμφανιζεται στο αρθρο x 100 φορες, και σε ολα τα αρθρα 1000. Αυτο μας λεει οτι λογω της πολυ μεγαλης αναστροφης συχνοτητας εμφανισης στα αρθρα, δεν εχει μεγαλη σημασεια, τουλαχιστον οχι οσο μεγαλη σημασεια εχει ο ορος \"pool\" \n",
        "(πχ  5/20  >  100/1000).\n",
        "\n",
        "Η τεχνικη αυτη ειναι πολυ χρησιμη και ευρεως διαδεδομενη, και συνδεεται με τον νομο του Ζιπφ. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSkIpxTq2kHP",
        "colab_type": "text"
      },
      "source": [
        "β) Αρχικα χρησιμοποιουμε το CountVectorizer για να δημιουργησουμε μια αφελη (μη σταθμισμενη) αναπαρασταση για το Bag Of Words μας:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orcBgRv62lMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer= CountVectorizer()\n",
        "\n",
        "random.shuffle(whole_train_text)\n",
        "random.shuffle(whole_test_text)\n",
        "\n",
        "vectorizer.fit_transform(whole_train_text)\n",
        "#print(vectorizer_train.vocabulary_)\n",
        "\n",
        "train_vector = vectorizer.fit_transform(whole_train_text)\n",
        "print(train_vector.shape)\n",
        "print(type(train_vector))\n",
        "print(train_vector.toarray())\n",
        "\n",
        "test_vector = vectorizer.transform(whole_test_text)\n",
        "print(test_vector.shape)\n",
        "print(type(test_vector))\n",
        "print(test_vector.toarray())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mmbyB-weUok",
        "colab_type": "text"
      },
      "source": [
        "γ) Εκπαιδευουμε εναν ταξινομητη χρησιμοποιωντας την τεχνικη Logistic Regression για να δουμε την ακριβεια:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhmzmSfMuV7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "labels_train = [0]*len(whole_train_text)\n",
        "for i in range(0, len(whole_train_text)):\n",
        "    if whole_train_text[i] in positive_train_data:\n",
        "        labels_train[i] = 1\n",
        "    else:\n",
        "        labels_train[i] = -1\n",
        "\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(train_vector, labels_train)\n",
        "\n",
        "\n",
        "\n",
        "labels_test = [0]*len(whole_test_text)\n",
        "for i in range(0, len(whole_test_text)):\n",
        "    if whole_test_text[i] in positive_test_data:\n",
        "        labels_test[i] = 1\n",
        "    else:\n",
        "        labels_test[i] = -1\n",
        "\n",
        "lr_prediction = logisticRegr.predict(test_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i35_J0LU9CX8",
        "colab_type": "text"
      },
      "source": [
        "Η ακριβεια ειναι:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj-rSB_53rCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = 0\n",
        "for i in range(0, len(lr_prediction)):\n",
        "    accuracy += (lr_prediction[i] == labels_test[i])\n",
        "print(\"Accuracy = \",100*accuracy/len(lr_prediction),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S5luB4ZeUYe",
        "colab_type": "text"
      },
      "source": [
        "δ) Τωρα, θα χρησιμοποιησουμε τον TfidVectorizer για να δημιουργησουμε μια καλυτερη αναπαρασταση για τα δεδομενα μας (σταθμισμενη)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4PQRg-feXuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "\n",
        "vectorizer_tfidf.fit_transform(whole_train_text)\n",
        "#print(vectorizer_train.vocabulary_)\n",
        "\n",
        "train_vector_tfidf = vectorizer_tfidf.fit_transform(whole_train_text)\n",
        "print(train_vector_tfidf.shape)\n",
        "print(type(train_vector_tfidf))\n",
        "print(train_vector_tfidf.toarray())\n",
        "\n",
        "test_vector_tfidf = vectorizer_tfidf.transform(whole_test_text)\n",
        "print(test_vector_tfidf.shape)\n",
        "print(type(test_vector_tfidf))\n",
        "print(test_vector_tfidf.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQOm6cx49PxY",
        "colab_type": "text"
      },
      "source": [
        "Και φτιαχνουμε παλι εναν ταξινομητη για να δουμε την ακριβεια:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVo_bttifJf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logisticRegr_tfidf = LogisticRegression()\n",
        "logisticRegr_tfidf.fit(train_vector_tfidf, labels_train)\n",
        "\n",
        "lr_prediction_tfidf = logisticRegr_tfidf.predict(test_vector_tfidf)\n",
        "accuracy = 0\n",
        "for i in range(0, len(lr_prediction_tfidf)):\n",
        "    accuracy += (lr_prediction_tfidf[i] == labels_test[i])\n",
        "print(\"Accuracy = \",100*accuracy/len(lr_prediction_tfidf),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Peo6GIW9Yf9",
        "colab_type": "text"
      },
      "source": [
        "Βλεπουμε οτι η σταθμισμενη αναπαρασταση ειναι πιο ακριβης απο την μη σταθμισμενη, γεγονος που επαληθευει την διαισθηση μας οτι ειναι χρησιμη τεχνικη."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAOXXVKl9jcG",
        "colab_type": "text"
      },
      "source": [
        "#Βημα 18ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu2fGhcn_TdE",
        "colab_type": "text"
      },
      "source": [
        "α)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2sRxCWv_VK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_train_data = read_samples(pos_train_dir)\n",
        "negative_train_data = read_samples(neg_train_dir)\n",
        "whole_train_text = positive_train_data + negative_train_data\n",
        "lines = whole_train_text\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text1 = flatten(text)\n",
        "Lexicon = unique_tokens(text1)\n",
        "words_in_voc = 0\n",
        "for i in range(0, len(voc)):\n",
        "    words_in_voc += (voc[i] in Lexicon)\n",
        "print(words_in_voc, \" of the vocabulary of IMDB texts are in the embedding, out of \", len(Lexicon), \", A \", 100*words_in_voc/len(Lexicon),\"% .\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX_uh5Ff_Vrq",
        "colab_type": "text"
      },
      "source": [
        "β)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaAylr-0_V9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7N7KJCs_WXd",
        "colab_type": "text"
      },
      "source": [
        "γ)Κατεβαζουμε τα προεκπαιδευμενα GoogleNews vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qh2uDFnF4hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget  -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jovV9-i_XZ4",
        "colab_type": "text"
      },
      "source": [
        "δ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZE83me1AkAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "model2 = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True, limit = 1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT2d7TdE_YKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.save(\"model2.model\")\n",
        "text1 = flatten(text)\n",
        "Lexicon = unique_tokens(text1)\n",
        "for i in range(0,10):\n",
        "    word = random.choice(model.wv.index2word)\n",
        "    while (word not in model2.wv.index2word):\n",
        "      word = random.choice(model.wv.index2word)\n",
        "    result, similarity = model2.wv.most_similar(word,topn=1)[0][0],  model2.wv.most_similar(word,topn=1)[0][1]\n",
        "    print(\"Initial word: \", word)\n",
        "    print(\"Similar-context word (According to Word2Vec embedding):\", result, \", Accuracy Achieved: \", similarity)\n",
        "    result, similarity = model.wv.most_similar(word,topn=1)[0][0],  model.wv.most_similar(word,topn=1)[0][1]\n",
        "    print(\"Similar-context word (According to our embedding):\", result, \", Accuracy Achieved: \", similarity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWqWfaIx_Y0_",
        "colab_type": "text"
      },
      "source": [
        "ε)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcwTRkDb_ZTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YrlTZZC_aAs",
        "colab_type": "text"
      },
      "source": [
        "ζ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWQrfB-F_agf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}