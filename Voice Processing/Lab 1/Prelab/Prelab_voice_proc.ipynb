{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prelab_voice_proc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeaidinis/NTUA/blob/master/Voice%20Processing/Lab%201/Prelab/Prelab_voice_proc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM0mb3ATwibH",
        "colab_type": "text"
      },
      "source": [
        "> Αϊδίνης Γιώργος  (el16031)\n",
        "\n",
        "> Τσιλιβής Θοδωρής (el16032)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 1η Εργαστηριακή Άσκηση - Μέρος πρώτο: Ορθογράφος"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCBjt0YXxvDZ",
        "colab_type": "text"
      },
      "source": [
        "Πριν αρχίσουμε τα βήματα για την προετοιμασία του εργαστηρίου, πρέπει να εγκαταστήσουμε κάποια εργαλεία που θα χρησιμοποιήσουμε στην εργασία αυτή. Πρώτα είναι η βιβλιοθήκη OpenFST (1.6.1), για την οποία χρησιμοποιούμε το έτοιμο bash script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hY00fgBuCMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod +x install_openfst.sh\n",
        "!./install_openfst.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcwet47Oyboe",
        "colab_type": "text"
      },
      "source": [
        "Ακόμα, θα πρέπει να εγκαταστήσουμε τις βιβλιοθήκες του NLTK (Natural Language Toolkit) καθώς και του graphviz, που μας επιτρέπει να σχεδιάζουμε τα FST που δημιουργούμε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjbkzLreyQgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "!sudo apt install python-pydot python-pydot-ng graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Jhh4FN2Hqv",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 1ο\n",
        "\n",
        "Για το Βήμα αυτό, επιλέξαμε και τα δύο προτεινόμενα βιβλία από το Project Guternberg, για να έχουμε περισσότερα δεδομένα.\n",
        "\n",
        "Το τελικό αρχείο, ονομάζεται corpora.txt . \n",
        "\n",
        "Ιδανικά, θα θέλαμε να είχαμε όσο το δυνατόν περισσοτερα δεδομενα, καθως αυτο θα μας επετρεπε να εχουμε ευρυτερο λεξιλογιο, δηλαδη να αναγνωριζει το συστημα μας περισσοτερες λεξεις. Ακομα, θα μπορουσαμε να εχουμε στην διαθεση μας διαφορετικες γραφες των ιδιων λεξεων, που ειναι χρησιμο σε εξειδικευμενους ορθογραφους."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pINLD3af4CCM",
        "colab_type": "text"
      },
      "source": [
        "Προσδιοριζουμε την κωδικοποιηση χαρακτηρων σε UTF-8, για να αναγνωρισθουν τυχον \"περιεργοι\" χαρακτηρες.\n",
        "\n",
        "Ακομα, ειναι χρησιμο να κανουμε import τωρα ο,τι βιβλιοθηκες της python θα χρησιμοποιησουμε αργοτερα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY7KEix24A7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "import itertools\n",
        "from nltk.tokenize.regexp import regexp_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlLLoZbp3tWO",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 2ο\n",
        "\n",
        "\n",
        "\n",
        "> α)  Γραφουμε μια συναρτηση που την ονομαζουμε identity_preprocess: (αρκετα απλο)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYDfatHm4up3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Reads a string and returns it. Returns string.\"\"\"\n",
        "def identity_preprocess(stringaki):\n",
        "\treturn stringaki\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JWABj-q5Dn0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> β) Γραφουμε μια συναρτηση που να δεχεται δυο ορισματα, το ονομα αρχειου, και μια συναρτηση της επιλογης μας και να διαβαζει το αρχειο γραμμη - γραμμη καλωντας την συναρτηση που εχουμε δωσει σαν ορισμα σε καθε γραμμη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYUlckQY5esE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes 2 arguments, one being the filepath and the second\n",
        "being the name of the method that reads the string, defaulting\n",
        "to the above method, 'identity_preprocess'. Returns a list of\n",
        "strings, that are the whole text of the file.\"\"\"\n",
        "def read_from_file(filepath, myfunc = identity_preprocess):\n",
        "\tlines = []\n",
        "\twith open(filepath) as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tlines.append(myfunc(line))\n",
        "\treturn lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGCff1_15jiw",
        "colab_type": "text"
      },
      "source": [
        "> γ) Γραφουμε εδω 2 συναρτησεις, η μια ειναι η συναρτηση clean_text που \"καθαριζει\" το κειμενο που της δινουμε απο οποιον χαρακτηρα δεν ειναι γραμμα, και στην συνεχεια, την tokenize, που παιρνει ενα τετοιο καθαρο κειμενο και το χωριζει σε μερη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOQVa4T6Su9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes one string and returns a string that has only lowercase\n",
        "letters, that means exluding any non-alphanumeric sumbols, numbers\n",
        "punctuation.\"\"\"\n",
        "def clean_text(text):\n",
        "\t#replace new line and carriage return\n",
        "\ttext = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "\t#replace the numbers, symbols and punctuation with space\n",
        "\tpunc_list = '~`!@#$£½%^&*()_+-—“”’‘\\'=}{|:à\"?>æ<âç,œ./è;é][}' + '1234567890'\n",
        "\tt = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
        "\ttext = text.translate(t)\n",
        "\t#make everything lowercase\n",
        "\ttext = text.lower()\n",
        "\treturn text\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes a text and splits it between whitespaces or \\n. Returns\n",
        "a list of strings\"\"\"\n",
        "def tokenize(text):\n",
        "\ttext = clean_text(text)\n",
        "\ttext = text.split()\n",
        "\treturn text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wiwjnso6Zbp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> δ) μια καλη συναρτηση που βρηκαμε οτι εχει καλα αποτελεσματα ειναι η εξης:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_IOTEIT6kVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"Uses the imported library from nltk to return a list of strings\n",
        "created by using regular expressions. It is one of the fastest \n",
        "tokenizers.\"\"\"\n",
        "def Tokenize(text):\n",
        "\t#text = clean_text(text)\n",
        "\treturn regexp_tokenize(text, pattern = '\\s+', gaps = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jllEoKJy6ot9",
        "colab_type": "text"
      },
      "source": [
        "Στις προσπαθειες μας, βρηκαμε οτι η ετοιμη συναρτηση ενω ειναι παρα πολυ γρηγορη, (πολυ πιο γρηγορη απο την δικη μας, ακομα και για τον σχετικα μικρο αριθμο δεδομενων μας), επιτρεπει λιγο λιγοτερες λεξεις (~170k vs ~173k) να \"περασουν\" ομως καταληγουμε με πολυ μεγαλο λεξιλογιο (κατι που θα φανει μετεπειτα): (παραθετουμε και την συναρτηση flatten, που αν και δεν ζητειται, ειναι χρησιμη καθως μετατρεπει μια λιστα λιστων σε μια λιστα)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNMTsZw7c9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e1cec522-a9bf-4a4e-b99a-8e591ec23640"
      },
      "source": [
        "\"\"\"Takes a list of lists and returns a list. Elements are kept in\n",
        "order.\"\"\"\n",
        "def flatten(l):\n",
        "\treturn list(itertools.chain(*l))\n",
        " \n",
        "\n",
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "text1 = []\n",
        "for line in lines:\n",
        "    text.append(Tokenize(line))\n",
        "    text1.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "text1 = flatten(text1)\n",
        "print(len(text))\n",
        "print(len(text1))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170792\n",
            "173206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmmQB8J08h6y",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 3ο\n",
        "\n",
        "\n",
        "\n",
        "> α)  Για να βρουμε το λεξιλογιο μας, φτιαχνουμε την συναρτηση unique_tokens, που παιρνει μια λιστα απο strings και αφαιρει τις πολλαπλες εμφανισεις:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSaBEEp9PgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes a list of strings (tokens) and reduces it to only unique\n",
        "elements. (gets rid of the duplicates). Returns a list of strings\n",
        "that are unique and sorted.\"\"\"\n",
        "def unique_tokens(tokens):\n",
        "\tlexicon = list(set(tokens))\n",
        "\tlexicon.sort()\n",
        "\treturn lexicon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GRjySeS9UEP",
        "colab_type": "text"
      },
      "source": [
        "> β)  Για να βρουμε το αλφαβητο του corpus, φτιαχνουμε την συναρτηση alphabet, που παιρνει μια λιστα απο strings επιστρεφει μια λιστα απο χαρακτηρες, χωρις πολλαπλες εμφανισεις:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qod5JWP4-efW",
        "colab_type": "text"
      },
      "source": [
        "Εκτελωντας τις παρακατω εντολες, μπορουμε να δουμε οτι τα λεξιλογια που δημιουργουνται απο τις διαφορετικες μεθοδους tokenization ειναι κατα πολυ διαφορετικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTPTmijl-u-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "373bb20c-96ab-4245-8311-f4feb45ded51"
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(Tokenize(line))\n",
        "text = flatten(text)\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"NLTK tokenize(): \",len(Lexicon))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"Our tokenize(): \",len(Lexicon))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK tokenize():  21672\n",
            "Our tokenize():  11249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYu1H3tI9mnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes a string and returns a list of the characters \n",
        "that were used to make the strings.\"\"\"\n",
        "def split_word(word):\n",
        "\treturn [char for char in word]\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes a list of strings as an input and returns a list of chars.\n",
        "The characters that are returned are used to make the elements of \n",
        "the input strings, but they are returned duplicate-free and sorted.\"\"\"\n",
        "def alphabet(words):\n",
        "\tchars = []\n",
        "\tfor word in words:\n",
        "\t\tchars.append(split_word(word))\n",
        "\talpbt = list(set(flatten(chars)))\n",
        "\talpbt.sort()\n",
        "\treturn alpbt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy1kN1yI9uWR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 4ο\n",
        "\n",
        "\n",
        "Για να δημιουργησουμε το αρχειο chars.syms που μας χρειαζεται για μετα, φιταχνουμε δυο συναρτησεις, μια για να κανει το (προηγουμενως δημιουργημενο) αλφαβητο indexed, δηλαδη καθε χαρακτηρας να αντιστοιχιζεται σε εναν φυσικο αριθμο, και το ε ('<epsilon>') να αντιστοιχιζεται στο 0. Ακομα, φτιαχνουμε μια συναρτηση που να παιρνει ενα τετοιο στοιχισμενο αλφαβητο και να το αποθηκευει σε ενα αρχειο της επιλογης μας (προεπιλεγμενο ονομα ειναι το \"chars.syms\"):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcfIp5xG-aSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes an alphabet (list of chars) and assigns each character \n",
        "an index. Index 0 is <epsilon> (ε). Returns a list that has n+1 \n",
        "tuples, each tuple being the character with its assigned index.\"\"\"\n",
        "def alphabet_indexing(alphabet):\n",
        "\tindexed_alphabet = []\n",
        "\tindexed_alphabet.append((\"<epsilon>\", 0))\n",
        "\ti = 1;\n",
        "\tfor letter in alphabet:\n",
        "\t\tindexed_alphabet.append((letter, str(i)))\n",
        "\t\ti+=1\n",
        "\treturn indexed_alphabet\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes two arguments. The first is an  alphabet and the \n",
        "second is the name of the file to write the output. The \n",
        "method takes the indexed alphabet and writes it to a file \n",
        "with the name given\"\"\"\n",
        "def symbols_file(alphabet, filename = \"chars.syms\"):\n",
        "\tf = open(filename, \"w+\")\n",
        "\tfor pair in alphabet:\n",
        "\t\tf.write(pair[0] + \"\\t\\t\" + str(pair[1]) + \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRI8GJmQ_Fq5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 5ο\n",
        "\n",
        "\n",
        "   α) Για to βημα 5ο, αρχικα υποθεσαμε οτι πρεπει να υλοποιησουμε εκτος των αλλων και την συναρτηση για την αποσταση Levenshtein, οποτε αν και (μετα απο ξεκαθαρισμα) δεν ειναι απαραιτητο, την παραθετουμε εδω για κριτικη, καθως και για να υπαρχει σε τυχον περιπτωση που την χρειαστουμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVaTHdqP_fOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This function computes the Levenshtein distance between two \n",
        "chars s,t. It uses the algorithm mentioned in the instructions\n",
        "of the lab, and is horribly inefficient. Takes two arguments, both\n",
        "chars and returns an integer that is the levenshtein distance of\n",
        "the two strings.\"\"\"\n",
        "def my_levenshtein(s,t):\n",
        "\t#same characters, no edit => Levenshtein cost 0\n",
        "\tif (s==t):\n",
        "\t\treturn 0\n",
        "\t#<epsilon> with some other character or any character with <epsilon>\n",
        "\t#either insertion or deletion => Levenshtein cost 1\n",
        "\telif (s=='<epsilon' and t!='<epsilon>') or (s!='<epsilon' and t=='<epsilon>'):\n",
        "\t\treturn 1\n",
        "\t#two different chars, substiturion => Levenshtein cost 1\n",
        "\telse:\n",
        "\t\treturn 1\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"This function computes the Levenshtein distance between two \n",
        "strings s,t. It uses the iterative algorithm, that creates a \n",
        "2d matrix of the characters of the strings, along with the \n",
        "values it needs to compute the sum of the weights of the insert\n",
        "/delete/substitute logic. It is a much faster algorithm than the\n",
        "recursive one. The method takes 3 arguments, string s, string t\n",
        "and a tuple of 3 integeres that are the cost of deletion, insertion\n",
        "and substitution respectively. Defaults to (1,1,1) (equal cost).\n",
        "I have taken the code for this method from:\n",
        "https://www.python-course.eu/levenshtein_distance.php \n",
        "Another method could have been the method given in\n",
        "the python package python-Levenshtein (0.12.0) that uses python to \n",
        "instruct C to compute the afforementioned array, with much faster \n",
        "execution times.\"\"\"\n",
        "def iterative_levenshtein(s, t, costs=(1, 1, 1)):\n",
        "    \"\"\" \n",
        "        iterative_levenshtein(s, t) -> ldist\n",
        "        ldist is the Levenshtein distance between the strings \n",
        "        s and t.\n",
        "        For all i and j, dist[i,j] will contain the Levenshtein \n",
        "        distance between the first i characters of s and the \n",
        "        first j characters of t\n",
        "        \n",
        "        costs: a tuple or a list with three integers (d, i, s)\n",
        "               where d defines the costs for a deletion\n",
        "                     i defines the costs for an insertion and\n",
        "                     s defines the costs for a substitution\n",
        "    \"\"\"\n",
        "    rows = len(s)+1\n",
        "    cols = len(t)+1\n",
        "    deletes, inserts, substitutes = costs\n",
        "    \n",
        "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
        "    # source prefixes can be transformed into empty strings \n",
        "    # by deletions:\n",
        "    for row in range(1, rows):\n",
        "        dist[row][0] = row * deletes\n",
        "    # target prefixes can be created from an empty source string\n",
        "    # by inserting the characters\n",
        "    for col in range(1, cols):\n",
        "        dist[0][col] = col * inserts\n",
        "        \n",
        "    for col in range(1, cols):\n",
        "        for row in range(1, rows):\n",
        "            if s[row-1] == t[col-1]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = substitutes\n",
        "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
        "                                 dist[row][col-1] + inserts,\n",
        "                                 dist[row-1][col-1] + cost) # substitution\n",
        "    \"\"\"for r in range(rows):\n",
        "        print(dist[r])\n",
        "    \"\"\" \n",
        "    return dist[row][col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbSdpft_rBN",
        "colab_type": "text"
      },
      "source": [
        "Προφανως η ασκηση δεν ανεφερε αυτο, οποτε για την δημιουργια του μετατροπεα μιας καταστασης, αρχικα δημιουργουμε τα αρχεια που χρειαζομαστε, (τα αρχεια για τα συμβολα ειναι ηδη ετοιμα, καθως ειναι ομοια με τα αρχεια του στοιχισμενου αλφαβητου που περιεχεται στο αρχειο chars.syms. Για τον σκοπο αυτο, φτιαξαμε μια μεθοδο για να τυπωνει τα περιεχομενα του fst τοσο στο stdout, οσο και στο αρχειο με ονομα \"text.txt\" που θα χρησιμοποιησουμε αργοτερα. O ακριβης αριθμος των ακμων στο fst ειναι (πληθος αλφαβητου)^2 -1, καθως θελουμε καθε γραμμα να εχει καποια ενεργεια με οποιοδηποτε αλλο γραμμα του αλφαβητου, εκτος απο το ε, που με τον εαυτο του δεν θελουμε να εχει. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8Xzx_HAmS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_fst(Alphabet, filename = \"text.txt\", print_lines = False):\n",
        "    f = open(filename, \"w+\")\n",
        "    if (print_lines):\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                else:\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "        print(\"0\")\n",
        "        f.write(\"0\")\n",
        "    else:\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "        f.write(\"0\")\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXRETrXFCcPN",
        "colab_type": "text"
      },
      "source": [
        "Για να δημιουργησουμε τα αρχεια, θα πρεπει να εκτελεσουμε τον παρακατω κωδικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSk8leWVCiwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "#print(len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "#print(len(Alphabet))\n",
        "Alphabet = alphabet_indexing(Alphabet)\n",
        "#print(Alphabet)\n",
        "symbols_file(Alphabet)\n",
        "text_fst(Alphabet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqq_r9--C4WJ",
        "colab_type": "text"
      },
      "source": [
        "Στην συνεχεια, για να δημιουργησουμε το fst, θα πρεπει να εκτελεστει το παρακτω shell script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo0hg4X5DAyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms text.txt > fst.fst\n",
        "!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait fst.fst  | dot -Tjpg >fst.jpg\n",
        "!fstinfo fst.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95snmURjDkxd",
        "colab_type": "text"
      },
      "source": [
        "Η πρωτη εντολη κανει compile τις πληροφοριες του αρχειο που δημιουργησαμε προηγουμενως για το one state fst, και παιρνει σαν input και output symbols τους χαρακτηρες του chars.syms (που ειναι και η αλφαβητα μας).\n",
        "Η δευτερη εντολη σχεδιαζει το fst που δημιουργηθηκε με τις βιβλιοθηκες dot και ghraphviz, και το αποθηκευει στο αρχειο fst.jpg. Η τριτη εντολη ειναι για να δουμε οτι ολα πηγαν καλα, δηλαδη οτι δημιουργηθηκε και εχει τα καταλληλα χαρακτηρηστικα. Επειδη ο μετατροπεας μιας καταστασης για την περιπτωση μας ειναι εξαιρετικα μεγαλος και η εικονα (jpg) δεν μας δειχνει καθαρα το τι συμβαινει, θα κανουμε μια μικροτερη αποτυπωση, απλα και μονο για λογους κατανοησης της λειτουργιας του κωδικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj4prUwFL8Ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Alphabet = [('<epsilon>', 0), ('a', '1'), ('b', '2'), ('c', '3'), ('d', '4')]\n",
        "symbols_file(Alphabet, \"chars1.syms\")\n",
        "text_fst(Alphabet, \"text1.txt\", False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXeLoMsiNG8t",
        "colab_type": "text"
      },
      "source": [
        "Τα αρχεια αυτα εχουν 25 ακμες, οποτε ειναι αρκετα πιο ευκολο να διακρινουμε τι γινεται απο την εικονα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mlvpPqwNP1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstcompile -isymbols=chars1.syms -osymbols=chars1.syms text1.txt > fst1.fst\n",
        "!fstdraw --isymbols=chars1.syms --osymbols=chars1.syms -portrait fst1.fst  | dot -Tjpg >fst1.jpg\n",
        "!fstinfo fst1.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hzCCm1NpLu",
        "colab_type": "text"
      },
      "source": [
        "Ο μετατροπεας εισοδου αν παρουμε το shortest path, αυτο σημαινει οτι δεν επιφερει καμια αλλαγη, δηλαδη η λεξη που παιρνει στην εισοδο και η λεξη που παιρνουμε στην εξοδο ειναι ιδιες."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfSj8QUkSMzc",
        "colab_type": "text"
      },
      "source": [
        "   β) Ιδανικα, αντι να εχουμε κανονες για τα βαρη, θα επρεπε αν ειχαμε πολυ περισσοτερα δεδομενα να εφαρμοζουμε πιθανοτικες μεθοδους για να προσδιορισουμε τα βαρη καλυτερα στο μοντελο μας."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6i9AWk5Teg3",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 6ο\n"
      ]
    }
  ]
}