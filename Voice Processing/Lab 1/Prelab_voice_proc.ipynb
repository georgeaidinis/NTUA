{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prelab_voice_proc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgeaidinis/NTUA/blob/master/Voice%20Processing/Lab%201/Prelab_voice_proc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM0mb3ATwibH",
        "colab_type": "text"
      },
      "source": [
        "> Αϊδίνης Γιώργος  (el16031)\n",
        "\n",
        "> Τσιλιβής Θοδωρής (el16032)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# 1η Εργαστηριακή Άσκηση - Μέρος πρώτο: Ορθογράφος"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCBjt0YXxvDZ",
        "colab_type": "text"
      },
      "source": [
        "Πριν αρχίσουμε τα βήματα για την προετοιμασία του εργαστηρίου, πρέπει να εγκαταστήσουμε κάποια εργαλεία που θα χρησιμοποιήσουμε στην εργασία αυτή. Πρώτα είναι η βιβλιοθήκη OpenFST (1.6.1), για την οποία χρησιμοποιούμε το έτοιμο bash script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hY00fgBuCMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod +x install_openfst.sh\n",
        "!./install_openfst.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcwet47Oyboe",
        "colab_type": "text"
      },
      "source": [
        "Ακόμα, θα πρέπει να εγκαταστήσουμε τις βιβλιοθήκες του NLTK (Natural Language Toolkit) καθώς και του graphviz, που μας επιτρέπει να σχεδιάζουμε τα FST που δημιουργούμε."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjbkzLreyQgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "!sudo apt install python-pydot python-pydot-ng graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Jhh4FN2Hqv",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 1ο\n",
        "\n",
        "Για το Βήμα αυτό, επιλέξαμε και τα δύο προτεινόμενα βιβλία από το Project Guternberg, για να έχουμε περισσότερα δεδομένα.\n",
        "\n",
        "Το τελικό αρχείο, ονομάζεται corpora.txt . \n",
        "\n",
        "Ιδανικά, θα θέλαμε να είχαμε όσο το δυνατόν περισσοτερα δεδομενα, καθως αυτο θα μας επετρεπε να εχουμε ευρυτερο λεξιλογιο, δηλαδη να αναγνωριζει το συστημα μας περισσοτερες λεξεις. Ακομα, θα μπορουσαμε να εχουμε στην διαθεση μας διαφορετικες γραφες των ιδιων λεξεων, που ειναι χρησιμο σε εξειδικευμενους ορθογραφους."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pINLD3af4CCM",
        "colab_type": "text"
      },
      "source": [
        "Προσδιοριζουμε την κωδικοποιηση χαρακτηρων σε UTF-8, για να αναγνωρισθουν τυχον \"περιεργοι\" χαρακτηρες.\n",
        "\n",
        "Ακομα, ειναι χρησιμο να κανουμε import τωρα ο,τι βιβλιοθηκες της python θα χρησιμοποιησουμε αργοτερα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY7KEix24A7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "import itertools\n",
        "from nltk.tokenize.regexp import regexp_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlLLoZbp3tWO",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 2ο\n",
        "\n",
        "\n",
        "\n",
        "> α)  Γραφουμε μια συναρτηση που την ονομαζουμε identity_preprocess: (αρκετα απλο)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYDfatHm4up3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Reads a string and returns it. Returns string.\"\"\"\n",
        "def identity_preprocess(stringaki):\n",
        "\treturn stringaki\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JWABj-q5Dn0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> β) Γραφουμε μια συναρτηση που να δεχεται δυο ορισματα, το ονομα αρχειου, και μια συναρτηση της επιλογης μας και να διαβαζει το αρχειο γραμμη - γραμμη καλωντας την συναρτηση που εχουμε δωσει σαν ορισμα σε καθε γραμμη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYUlckQY5esE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes 2 arguments, one being the filepath and the second\n",
        "being the name of the method that reads the string, defaulting\n",
        "to the above method, 'identity_preprocess'. Returns a list of\n",
        "strings, that are the whole text of the file.\"\"\"\n",
        "def read_from_file(filepath, myfunc = identity_preprocess):\n",
        "\tlines = []\n",
        "\twith open(filepath) as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tlines.append(myfunc(line))\n",
        "\treturn lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGCff1_15jiw",
        "colab_type": "text"
      },
      "source": [
        "> γ) Γραφουμε εδω 2 συναρτησεις, η μια ειναι η συναρτηση clean_text που \"καθαριζει\" το κειμενο που της δινουμε απο οποιον χαρακτηρα δεν ειναι γραμμα, και στην συνεχεια, την tokenize, που παιρνει ενα τετοιο καθαρο κειμενο και το χωριζει σε μερη:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fOQVa4T6Su9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes one string and returns a string that has only lowercase\n",
        "letters, that means exluding any non-alphanumeric sumbols, numbers\n",
        "punctuation.\"\"\"\n",
        "def clean_text(text):\n",
        "\t#replace new line and carriage return\n",
        "\ttext = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "\t#replace the numbers, symbols and punctuation with space\n",
        "\tpunc_list = '~`!@#$£½%^&*()_+-—“”’‘\\'=}{|:à\"?>æ<âç,œ./è;é][}' + '1234567890'\n",
        "\tt = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
        "\ttext = text.translate(t)\n",
        "\t#make everything lowercase\n",
        "\ttext = text.lower()\n",
        "\treturn text\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes a text and splits it between whitespaces or \\n. Returns\n",
        "a list of strings\"\"\"\n",
        "def tokenize(text):\n",
        "\ttext = clean_text(text)\n",
        "\ttext = text.split()\n",
        "\treturn text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wiwjnso6Zbp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> δ) μια καλη συναρτηση που βρηκαμε οτι εχει καλα αποτελεσματα ειναι η εξης:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_IOTEIT6kVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"Uses the imported library from nltk to return a list of strings\n",
        "created by using regular expressions. It is one of the fastest \n",
        "tokenizers.\"\"\"\n",
        "def Tokenize(text):\n",
        "\t#text = clean_text(text)\n",
        "\treturn regexp_tokenize(text, pattern = '\\s+', gaps = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jllEoKJy6ot9",
        "colab_type": "text"
      },
      "source": [
        "Στις προσπαθειες μας, βρηκαμε οτι η ετοιμη συναρτηση ενω ειναι παρα πολυ γρηγορη, (πολυ πιο γρηγορη απο την δικη μας, ακομα και για τον σχετικα μικρο αριθμο δεδομενων μας), επιτρεπει λιγο λιγοτερες λεξεις (~170k vs ~173k) να \"περασουν\" ομως καταληγουμε με πολυ μεγαλο λεξιλογιο (κατι που θα φανει μετεπειτα): (παραθετουμε και την συναρτηση flatten, που αν και δεν ζητειται, ειναι χρησιμη καθως μετατρεπει μια λιστα λιστων σε μια λιστα)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNMTsZw7c9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes a list of lists and returns a list. Elements are kept in\n",
        "order.\"\"\"\n",
        "def flatten(l):\n",
        "\treturn list(itertools.chain(*l))\n",
        " \n",
        "\n",
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "text1 = []\n",
        "for line in lines:\n",
        "    text.append(Tokenize(line))\n",
        "    text1.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "text1 = flatten(text1)\n",
        "print(len(text))\n",
        "print(len(text1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmmQB8J08h6y",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 3ο\n",
        "\n",
        "\n",
        "\n",
        "> α)  Για να βρουμε το λεξιλογιο μας, φτιαχνουμε την συναρτηση unique_tokens, που παιρνει μια λιστα απο strings και αφαιρει τις πολλαπλες εμφανισεις:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSaBEEp9PgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes a list of strings (tokens) and reduces it to only unique\n",
        "elements. (gets rid of the duplicates). Returns a list of strings\n",
        "that are unique and sorted.\"\"\"\n",
        "def unique_tokens(tokens):\n",
        "\tlexicon = list(set(tokens))\n",
        "\tlexicon.sort()\n",
        "\treturn lexicon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GRjySeS9UEP",
        "colab_type": "text"
      },
      "source": [
        "> β)  Για να βρουμε το αλφαβητο του corpus, φτιαχνουμε την συναρτηση alphabet, που παιρνει μια λιστα απο strings επιστρεφει μια λιστα απο χαρακτηρες, χωρις πολλαπλες εμφανισεις:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qod5JWP4-efW",
        "colab_type": "text"
      },
      "source": [
        "Εκτελωντας τις παρακατω εντολες, μπορουμε να δουμε οτι τα λεξιλογια που δημιουργουνται απο τις διαφορετικες μεθοδους tokenization ειναι κατα πολυ διαφορετικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTPTmijl-u-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(Tokenize(line))\n",
        "text = flatten(text)\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"NLTK tokenize(): \",len(Lexicon))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"Our tokenize(): \",len(Lexicon))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYu1H3tI9mnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes a string and returns a list of the characters \n",
        "that were used to make the strings.\"\"\"\n",
        "def split_word(word):\n",
        "\treturn [char for char in word]\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes a list of strings as an input and returns a list of chars.\n",
        "The characters that are returned are used to make the elements of \n",
        "the input strings, but they are returned duplicate-free and sorted.\"\"\"\n",
        "def alphabet(words):\n",
        "\tchars = []\n",
        "\tfor word in words:\n",
        "\t\tchars.append(split_word(word))\n",
        "\talpbt = list(set(flatten(chars)))\n",
        "\talpbt.sort()\n",
        "\treturn alpbt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy1kN1yI9uWR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 4ο\n",
        "\n",
        "\n",
        "Για να δημιουργησουμε το αρχειο chars.syms που μας χρειαζεται για μετα, φιταχνουμε δυο συναρτησεις, μια για να κανει το (προηγουμενως δημιουργημενο) αλφαβητο indexed, δηλαδη καθε χαρακτηρας να αντιστοιχιζεται σε εναν φυσικο αριθμο, και το ε ('<epsilon>') να αντιστοιχιζεται στο 0. Ακομα, φτιαχνουμε μια συναρτηση που να παιρνει ενα τετοιο στοιχισμενο αλφαβητο και να το αποθηκευει σε ενα αρχειο της επιλογης μας (προεπιλεγμενο ονομα ειναι το \"chars.syms\"):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcfIp5xG-aSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Takes an alphabet (list of chars) and assigns each character \n",
        "an index. Index 0 is <epsilon> (ε). Returns a list that has n+1 \n",
        "tuples, each tuple being the character with its assigned index.\"\"\"\n",
        "def alphabet_indexing(alphabet):\n",
        "\tindexed_alphabet = []\n",
        "\tindexed_alphabet.append((\"<epsilon>\", 0))\n",
        "\ti = 1;\n",
        "\tfor letter in alphabet:\n",
        "\t\tindexed_alphabet.append((letter, str(i)))\n",
        "\t\ti+=1\n",
        "\treturn indexed_alphabet\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Takes two arguments. The first is an  alphabet and the \n",
        "second is the name of the file to write the output. The \n",
        "method takes the indexed alphabet and writes it to a file \n",
        "with the name given\"\"\"\n",
        "def symbols_file(alphabet, filename = \"chars.syms\"):\n",
        "\tf = open(filename, \"w+\")\n",
        "\tfor pair in alphabet:\n",
        "\t\tf.write(pair[0] + \"\\t\\t\" + str(pair[1]) + \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRI8GJmQ_Fq5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 5ο\n",
        "\n",
        "\n",
        "   α) Για to βημα 5ο, αρχικα υποθεσαμε οτι πρεπει να υλοποιησουμε εκτος των αλλων και την συναρτηση για την αποσταση Levenshtein, οποτε αν και (μετα απο ξεκαθαρισμα) δεν ειναι απαραιτητο, την παραθετουμε εδω για κριτικη, καθως και για να υπαρχει σε τυχον περιπτωση που την χρειαστουμε:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVaTHdqP_fOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This function computes the Levenshtein distance between two \n",
        "chars s,t. It uses the algorithm mentioned in the instructions\n",
        "of the lab, and is horribly inefficient. Takes two arguments, both\n",
        "chars and returns an integer that is the levenshtein distance of\n",
        "the two strings.\"\"\"\n",
        "def my_levenshtein(s,t):\n",
        "\t#same characters, no edit => Levenshtein cost 0\n",
        "\tif (s==t):\n",
        "\t\treturn 0\n",
        "\t#<epsilon> with some other character or any character with <epsilon>\n",
        "\t#either insertion or deletion => Levenshtein cost 1\n",
        "\telif (s=='<epsilon' and t!='<epsilon>') or (s!='<epsilon' and t=='<epsilon>'):\n",
        "\t\treturn 1\n",
        "\t#two different chars, substiturion => Levenshtein cost 1\n",
        "\telse:\n",
        "\t\treturn 1\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"This function computes the Levenshtein distance between two \n",
        "strings s,t. It uses the iterative algorithm, that creates a \n",
        "2d matrix of the characters of the strings, along with the \n",
        "values it needs to compute the sum of the weights of the insert\n",
        "/delete/substitute logic. It is a much faster algorithm than the\n",
        "recursive one. The method takes 3 arguments, string s, string t\n",
        "and a tuple of 3 integeres that are the cost of deletion, insertion\n",
        "and substitution respectively. Defaults to (1,1,1) (equal cost).\n",
        "I have taken the code for this method from:\n",
        "https://www.python-course.eu/levenshtein_distance.php \n",
        "Another method could have been the method given in\n",
        "the python package python-Levenshtein (0.12.0) that uses python to \n",
        "instruct C to compute the afforementioned array, with much faster \n",
        "execution times.\"\"\"\n",
        "def iterative_levenshtein(s, t, costs=(1, 1, 1)):\n",
        "    \"\"\" \n",
        "        iterative_levenshtein(s, t) -> ldist\n",
        "        ldist is the Levenshtein distance between the strings \n",
        "        s and t.\n",
        "        For all i and j, dist[i,j] will contain the Levenshtein \n",
        "        distance between the first i characters of s and the \n",
        "        first j characters of t\n",
        "        \n",
        "        costs: a tuple or a list with three integers (d, i, s)\n",
        "               where d defines the costs for a deletion\n",
        "                     i defines the costs for an insertion and\n",
        "                     s defines the costs for a substitution\n",
        "    \"\"\"\n",
        "    rows = len(s)+1\n",
        "    cols = len(t)+1\n",
        "    deletes, inserts, substitutes = costs\n",
        "    \n",
        "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
        "    # source prefixes can be transformed into empty strings \n",
        "    # by deletions:\n",
        "    for row in range(1, rows):\n",
        "        dist[row][0] = row * deletes\n",
        "    # target prefixes can be created from an empty source string\n",
        "    # by inserting the characters\n",
        "    for col in range(1, cols):\n",
        "        dist[0][col] = col * inserts\n",
        "        \n",
        "    for col in range(1, cols):\n",
        "        for row in range(1, rows):\n",
        "            if s[row-1] == t[col-1]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = substitutes\n",
        "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
        "                                 dist[row][col-1] + inserts,\n",
        "                                 dist[row-1][col-1] + cost) # substitution\n",
        "    \"\"\"for r in range(rows):\n",
        "        print(dist[r])\n",
        "    \"\"\" \n",
        "    return dist[row][col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUbSdpft_rBN",
        "colab_type": "text"
      },
      "source": [
        "Προφανως η ασκηση δεν ανεφερε αυτο, οποτε για την δημιουργια του μετατροπεα μιας καταστασης, αρχικα δημιουργουμε τα αρχεια που χρειαζομαστε, (τα αρχεια για τα συμβολα ειναι ηδη ετοιμα, καθως ειναι ομοια με τα αρχεια του στοιχισμενου αλφαβητου που περιεχεται στο αρχειο chars.syms. Για τον σκοπο αυτο, φτιαξαμε μια μεθοδο για να τυπωνει τα περιεχομενα του fst τοσο στο stdout, οσο και στο αρχειο με ονομα \"text.txt\" που θα χρησιμοποιησουμε αργοτερα. O ακριβης αριθμος των ακμων στο fst ειναι (πληθος αλφαβητου)^2 -1, καθως θελουμε καθε γραμμα να εχει καποια ενεργεια με οποιοδηποτε αλλο γραμμα του αλφαβητου, εκτος απο το ε, που με τον εαυτο του δεν θελουμε να εχει. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8Xzx_HAmS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_fst(Alphabet, filename = \"text.txt\", print_lines = False):\n",
        "    f = open(filename, \"w+\")\n",
        "    if (print_lines):\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                else:\n",
        "                    print(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\")\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "        print(\"0\")\n",
        "        f.write(\"0\")\n",
        "    else:\n",
        "        for pair in Alphabet:\n",
        "            for pair1 in Alphabet:\n",
        "                if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                    continue \n",
        "                elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "                elif (pair[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                elif (pair1[0]=='<epsilon>'):\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "        f.write(\"0\")\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXRETrXFCcPN",
        "colab_type": "text"
      },
      "source": [
        "Για να δημιουργησουμε τα αρχεια, θα πρεπει να εκτελεσουμε τον παρακατω κωδικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSk8leWVCiwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "#print(len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "#print(len(Alphabet))\n",
        "Alphabet = alphabet_indexing(Alphabet)\n",
        "#print(Alphabet)\n",
        "symbols_file(Alphabet,)\n",
        "text_fst(Alphabet, \"onestate.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqq_r9--C4WJ",
        "colab_type": "text"
      },
      "source": [
        "Στην συνεχεια, για να δημιουργησουμε το fst, θα πρεπει να εκτελεστει το παρακτω shell script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo0hg4X5DAyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "812cf66e-dcf9-4ac5-bc61-0b6232ac8689"
      },
      "source": [
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms onestate.txt > onestate.fst\n",
        "!fstinfo onestate.fst"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       1\n",
            "# of arcs                                         728\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               26\n",
            "# of output epsilons                              26\n",
            "input label multiplicity                          26.9643\n",
            "output label multiplicity                         26.9643\n",
            "# of accessible states                            1\n",
            "# of coaccessible states                          1\n",
            "# of connected states                             1\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     1\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            y\n",
            "cyclic at initial state                           y\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95snmURjDkxd",
        "colab_type": "text"
      },
      "source": [
        "Η πρωτη εντολη κανει compile τις πληροφοριες του αρχειο που δημιουργησαμε προηγουμενως για το one state fst, και παιρνει σαν input και output symbols τους χαρακτηρες του chars.syms (που ειναι και η αλφαβητα μας).\n",
        "Η δευτερη εντολη σχεδιαζει το fst που δημιουργηθηκε με τις βιβλιοθηκες dot και ghraphviz, και το αποθηκευει στο αρχειο fst.jpg. Η τριτη εντολη ειναι για να δουμε οτι ολα πηγαν καλα, δηλαδη οτι δημιουργηθηκε και εχει τα καταλληλα χαρακτηρηστικα. Επειδη ο μετατροπεας μιας καταστασης για την περιπτωση μας ειναι εξαιρετικα μεγαλος και η εικονα (jpg) δεν μας δειχνει καθαρα το τι συμβαινει, θα κανουμε μια μικροτερη αποτυπωση, απλα και μονο για λογους κατανοησης της λειτουργιας του κωδικα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj4prUwFL8Ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Alphabet = [('<epsilon>', 0), ('a', '1'), ('b', '2'), ('c', '3'), ('d', '4')]\n",
        "symbols_file(Alphabet, \"chars1.syms\")\n",
        "text_fst(Alphabet, \"text1.txt\", False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXeLoMsiNG8t",
        "colab_type": "text"
      },
      "source": [
        "Τα αρχεια αυτα εχουν 25 ακμες, οποτε ειναι αρκετα πιο ευκολο να διακρινουμε τι γινεται απο την εικονα:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mlvpPqwNP1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstcompile -isymbols=chars1.syms -osymbols=chars1.syms text1.txt > fst1.fst\n",
        "!fstdraw --isymbols=chars1.syms --osymbols=chars1.syms -portrait fst1.fst  | dot -Tjpg >fst1.jpg\n",
        "!fstinfo fst1.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hzCCm1NpLu",
        "colab_type": "text"
      },
      "source": [
        "Ο μετατροπεας εισοδου αν παρουμε το shortest path, αυτο σημαινει οτι δεν επιφερει καμια αλλαγη, δηλαδη η λεξη που παιρνει στην εισοδο και η λεξη που παιρνουμε στην εξοδο ειναι ιδιες."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfSj8QUkSMzc",
        "colab_type": "text"
      },
      "source": [
        "   β) Ιδανικα, αντι να εχουμε κανονες για τα βαρη, θα επρεπε αν ειχαμε πολυ περισσοτερα δεδομενα να εφαρμοζουμε πιθανοτικες μεθοδους για να προσδιορισουμε τα βαρη καλυτερα στο μοντελο μας."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6i9AWk5Teg3",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 6ο\n",
        "   α) Για να υλοποιησουμε τον αποδοχεα λεξεων του λεξικου, ακολουθουμε την λογικη που ειχαμε και πριν για το αλφαβητο, μονο που αυτην την φορα ο αποδοχεας ειναι παρα (μα παρα) πολυ μεγαλος (τουλαχιστον για τον υπολογιστη μας). Παρολα αυτα, η μεθοδος sixth δημιουργει ενα αρχειο με το προσδιορισμενο ονομα, που περιεχει τις ακμες για καθε λεξη, με μηδενικο βαρος."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XArzNCcdelqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sixth(Lexicon, Alphabet, filename = \"6a.txt\", print_lines = False):\n",
        "    for i in range(0, len(Alphabet)):\n",
        "        temp = Alphabet[i]\n",
        "        Alphabet[i] = [temp, i]\n",
        "    f = open(filename, \"w+\")\n",
        "    i = 2\n",
        "    if (not print_lines):\n",
        "        for word in Lexicon:\n",
        "            for j in range(0, len(word)):\n",
        "                if j==0:\n",
        "                    f.write(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                    i += 1\n",
        "                elif (j== len(word) -1):\n",
        "                    f.write( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    i +=1\n",
        "                else:\n",
        "                    f.write( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    i+=1\n",
        "        f.write(\"1\")\n",
        "    else:\n",
        "        for word in Lexicon:\n",
        "            for j in range(0, len(word)):\n",
        "                if j==0:\n",
        "                    f.write(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    print(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                    i += 1\n",
        "                elif (j== len(word) -1):\n",
        "                    f.write( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    print( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                    i +=1\n",
        "                else:\n",
        "                    f.write( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    print( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                    i+=1\n",
        "        f.write(\"1\")\n",
        "        print(\"1\")\n",
        "    f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr6xfnX6BdiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "#print(len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "sixth(Lexicon, Alphabet, \"acceptor.txt\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em0PndE8Zdnp",
        "colab_type": "text"
      },
      "source": [
        " Αφου φτιαξουμε το αρχειο, οπως πριν καλουμε απο το cli τις ιδιες εντολες με τον μετατροπεα μιας καταστασης (με αλλαγμενα τα ονοματα προφανως. Συστηνουμε (τουλαχιστον εμεις ετσι μπορουσαμε να δουμε τα αποτελεσματα) να εκτελεστει το απο κατω κελι αντι για το απο πανω, για και στην συνεχεια να δημιουργηθει το fst, γιατι μειωνει σημαντικα ( απο 11249 λεξεις σε μολις 10 ) το λεξιλογιο, οποτε οχι μονο ειναι γρηγορη η δημιουργια του fst, αλλα και μπορει να διακριθει ο ιδιος ο αποδοχεας."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LktRu0bDGGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_from_file(\"corpora.txt\")\n",
        "#print(len(lines))\n",
        "text = []\n",
        "for line in lines:\n",
        "    text.append(tokenize(line))\n",
        "text = flatten(text)\n",
        "#print(len(text))\n",
        "Lexicon = unique_tokens(text)\n",
        "print(\"Αρχικο μεγεθος λεξικου: \",len(Lexicon))\n",
        "Alphabet = alphabet(Lexicon)\n",
        "Lexicon1 = Lexicon[10:20]\n",
        "print(\"Μειωμενο μεγεθος λεξικου: \",len(Lexicon1))\n",
        "sixth(Lexicon1, Alphabet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LwAJwLRCt5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstcompile -isymbols=chars.syms -osymbols=chars.syms acceptor.txt > acceptor.fst\n",
        "!fstinfo acceptor.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGcbyqVNa0_B",
        "colab_type": "text"
      },
      "source": [
        "   β) Για το μερος β, αρκει να τρεξουμε τις παρακατω εντολες (τις εχουμε βαλει ξεχωριστα για να παρατηρησουμε τι κανει η καθε μια)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP9n8AZKbEnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstdeterminize acceptor.fst  > acceptor_determinized.fst\n",
        "!fstminimize acceptor_determinized.fst > acceptor_minimized.fst\n",
        "!fstinfo acceptor_minimized.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blOuFPvafApW",
        "colab_type": "text"
      },
      "source": [
        "Η συναρτηση fstrmepsilon αφαιρει τις μεταβασεις που εχουν ε στο fst μας, και προφανως δεν ειχε καν νοημα να τις εφαρμοσουμε, αφου δεν ειχαμε χρησιμοποιησει καν ε στην κατασκευη του.\n",
        "Η συναρτηση fstdeterminize αφαιρει τον μη ντετερμινισμο, και οπως φαινεται απο τις εικονες των διαγραμματων που παραγονται, αν και δεν εχει ελαχιστο αριθμο καταστασεων, ειναι αφενος σαφως μικροτερο διαγραμμα απο το αρχικο και αφετερου σαφως πιο κατανοητο.\n",
        "Τελος, η συναρτηση fstminimize δημιουργει το διαγραμμα με τον ελαχιστο αριθμο καταστασεων, οπως φαινεται και απο την φωτογραφια fst_minimized.jpg που εχει 24 καταστασεις (ενω η determinized 27, ενω η αρχικη 55!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr1vRGrEhgNi",
        "colab_type": "text"
      },
      "source": [
        "*Για τα Βηματα 7,8,9, δεν ειχαμε αρκετο χρονο, καθως πολλες αποριες μας λυθηκαν την ημερα του εργαστηριου (25/11). Παρολα αυτα, οπως συζητηθηκε και με τον καθηγητη, θα ειναι υλοποιημενα στην υποβολη της ολοκληρωμενης αναφορας για το εργαστηριο 1.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuzupFrrggeN",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 7ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyfwnLBpNfz-",
        "colab_type": "text"
      },
      "source": [
        "   α) Φτιαχνουμε το Levenshtein Transducer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24sy_jxd_Nyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstcompose onestate.fst acceptor_minimized.fst > Leven.fst\n",
        "!fstinfo Leven.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XlilBJXZnTb",
        "colab_type": "text"
      },
      "source": [
        "Αναλογα με τα βαρη του καθε ειδους edit, γινεται αναλογο prioritization στα output words. Για παραδειγμα, για ισοβαρη edits, ο min distance transducer μπορει να επιλεξει και λεξεις μεγαλυτερου μηκους, που εχουν πολλα κοινα γραμματα με την αρχικη, με τελικο βαρος ισο με αλλες λεξεις μικροτερου μηκους, που ομως εχουν περισσοτερα substitutions γραμματων. Απο την αλλη, αν ειχαμε μεγαλυτερο βαρος στο removal η στο insertion, τοτε θα ειχαμε περισσοτερα αποτελεσματα ιδιου μηκους με την λεξη εισοδου, αλλα με περισσοτερα substitutions. Συμπερασματικα, καταληγουμε οτι τα βαρη επηρεαζουν την μορφη των λεξεων εξοδου. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNp5vgd4NwuJ",
        "colab_type": "text"
      },
      "source": [
        " β) Φτιαχνουμε τα απαραιτητα αρχεια για την δημιουργια ενος acceptor για την λεξη \"cit\":\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9OgWWx8OXmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Lexicon_cit = [\"cit\"]\n",
        "sixth(Lexicon_cit, Alphabet, \"cit.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlBWXN-QOy-C",
        "colab_type": "text"
      },
      "source": [
        "Φτιαχνουμε τον αποδοχεα για την λεξη \"cit\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlVgsc7oHyw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! fstcompile -isymbols=chars.syms -osymbols=chars.syms cit.txt > cit.fst\n",
        "! fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait cit.fst  | dot -Tjpg >cit.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncB7sxhrOOnE",
        "colab_type": "text"
      },
      "source": [
        "Κανουμε compose το FST για την αποσταση Levenshtein με τον αποδοχεα της λεξης \"cit\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfhSAwZfOPFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! fstcompose cit.fst Leven.fst  > min_distance.fst\n",
        "! fstinfo min_distance.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To5bzmAKPDU7",
        "colab_type": "text"
      },
      "source": [
        "Καλουμε την fstshortestdistance για να βρουμε την ελαχιστη edit αποσταση (αποσταση Levenshtein)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT6XXmZJSNnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!fstshortestpath min_distance.fst | fstrmepsilon > out.fst\n",
        "!fstdraw --isymbols=chars.syms --osymbols=chars.syms -portrait out.fst  | dot -Tjpg >out.jpg\n",
        "!fstinfo out.fst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdaYjRTDZWu8",
        "colab_type": "text"
      },
      "source": [
        "Αν τρεξουμε τον παραπανω κωδικα, βλεπουμε οτι η προβλεψη του min edit spell checker ειναι η λεξη \"wit\" που ουσιαστικα εχει μονο ενα substitution ενος γραμματος. Αν βαλουμε το argument --nshorterst = Ν, οπου Ν ο αριθμος των προβλεψεων που θελουμε να δουμε, βλεπουμε οτι υπαρχουν πολλες επιλογες με το ιδιο αριθμο edits, οπως η λεξη \"sit\" και αλλες. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_kxWWizgiHR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 8ο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm1GLoTXgjci",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Βήμα 9ο"
      ]
    }
  ]
}